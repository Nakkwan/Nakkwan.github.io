---
layout: default
title: Survey on Visual Transformer
nav_order: "2023.12.21"
parent: VIT
grand_parent: Computer Vision
permalink: /docs/computer_vision/vit/survey_on_vit_2023_12_20
math: katex
---

# A Survey on Visual Transformer
{: .no_toc}
[A Survey on Visual Transformer](https://arxiv.org/abs/2012.12556)

Table of Contents
{: .text-delta}
1. TOC
{:toc}

TransformerëŠ” self-attentionì„ ê¸°ë°˜ìœ¼ë¡œ í•¨

inductive biasê°€ ì ê²Œ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ê´€ì‹¬ì„ ë°›ê³  ìˆìŒ

ì¥ë‹¨ì ê³¼ backbone network, high/mid-level vision, low-level vision, and video processingì—ì„œì˜ transformerë¥¼ ë¶„ì„

## **Introduction**
ê¸°ë³¸ì´ ë˜ëŠ” NN infraê°€ ì¡´ì¬í•¨ <br>
$$\rightarrow$$ MLP + FC, shift-invariantì˜ CNN, sequentialì˜ RNN <br>
TransformerëŠ” self-attentionì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ìƒˆë¡œìš´ NN ì„

TransformerëŠ” NLPì—ì„œ ì²˜ìŒ ì‹œì‘ë¨ <br>
Machine translationì˜ vanilla transformer ì´í›„, BERTëŠ” Bidirectional Encoderë¥¼ ì ìš©í•˜ì—¬ labelì—†ëŠ” pretrainedë¥¼ ì œì•ˆí–ˆê³ , GPT-3ëŠ” ëŒ€ê·œëª¨ LLMì„ pretrainí•¨

CVì—ì„œë„ transformerê°€ CNNì˜ ëŒ€ì•ˆìœ¼ë¡œ ë– ì˜¤ë¦„ <br>
$$\rightarrow$$ Pixel ì˜ˆì¸¡ì€ ê³„ì‚°ì´ ë„ˆë¬´ ë³µì¡í•˜ê¸° ë•Œë¬¸ì— VITì—ì„œ Image patchì— transformer ì ìš©

ì´ì™¸ì—ë„ object detection, semantic segmentation, image processing, video understanding ë“±ì— í™œìš©ë¨

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig1.jpg" width="60%" alt="Figure 1"></center>

Taskì˜ ë¶„ë¥˜ëŠ” Detectionê³¼ ê°™ì€ ê²ƒì€ high-level, ë³µì›, denoiseëŠ” low-levelë¡œ ì·¨ê¸‰

ì „ì²´ì ì¸ ê²ƒì€ ì•„ë˜ì˜ í‘œì™€ ê°™ìŒ

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig2.jpg" width="60%" alt="Figure 2"></center>

# **Formulation of Transformer**

TransformerëŠ” transformer blockì´ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë˜ëŠ” encoderì™€ decoderë¡œ êµ¬ì„±ë¨ <br>
**Encoder**: Inputì— ëŒ€í•œ encoding <br>
**Decoder**: encodingì˜ incorporated contextual informationë¥¼ ì‚¬ìš©í•˜ì—¬, sequentialí•˜ê²Œ output ìƒì„±  <br>
**Transformer block:** MHA(multi-head attention layer), FFNN(feed-forward neural network), shortcut connection, layer normalizationìœ¼ë¡œ êµ¬ì„±ë¨

## Self-Attention

Inputìœ¼ë¡œëŠ” 3ê°œì˜ Vector $$q, k, v$$ê°€ ë“¤ì–´ê° <br>
$$\rightarrow$$ $$d_q,d_k,d_v,d_{dim}\in\mathbb{R}^{512}$$ <br>
ì´í›„ matmulì„ í†µí•´ $$Q,K,V$$ë¡œ ìœ ë„ë¨

Attention ê³„ì‚° ê³¼ì •ì€

1. Score ê³„ì‚°: $$S=Q\cdot K^T$$
2. Gradient stabilityë¥¼ ìœ„í•œ Normalize: $$S_n=S/\sqrt{d_k}$$
3. Scoreë¥¼ probabilitiesë¡œ ë³€í™˜(Softmax): $$P=\mathrm{Softmax}(S_n)$$
4. weighted $$V$$ matrixë¥¼ ì–»ìŒ: $$Z=V\cdot P$$
    

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig3.jpg" width="60%" alt="Figure 3"></center>

ìµœì¢…ì ìœ¼ë¡œ 

$$
\mathrm{Attention}(Q,K,V)=\mathrm{Softmax}(\frac{Q\cdot K^T}{\sqrt{d_k}})\cdot V
$$

Encoderì—ì„œ $$q,k,v$$ê°€ ë‹¤ ê°™ì§€ë§Œ, decoderì—ì„œ $$k,v$$ëŠ” encoderì˜ output, $$q$$ëŠ” ì´ì „ layerì—ì„œ ìœ ë„ë¨

> valueê°€ decoderì—ì„œ ì˜¤ì§€ ì•Šì€ ì´ìœ ëŠ” decoderì˜ next layerì˜ stateê°€ input layerì™€ encoderê°„ì˜ ìœ ì‚¬ì„±ìœ¼ë¡œ weightedëœ encoderë¼ëŠ” ê²ƒ <br>
Encoderë¥¼ í†µí•´ aggregationëœ inputë“¤ì„ decoderì˜ inputì„ ì°¸ì¡°í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê° <br>
decoderì˜ queryëŠ” ë§ ê·¸ëŒ€ë¡œ ìš”ì²­ <br>
Encoderì˜ keyê°€ ì‘ë‹µë˜ì–´, attention map ê³„ì‚° <br>
aggregated inputì¸ valueë¥¼ ì´ë¥¼ í†µí•´ weighted í•´ë‚˜ê°
> 
- **ì¦‰, input sentenceì— ëŒ€í•´, decoderì˜ input wordë¥¼ referenceë¡œ ì‚¼ì•„, outputì„ ì§„í–‰** <br>
- **ì²« sequenceëŠ” ë¬´ì¡°ê±´ \<sos\>ì„ (ë‚˜ë¨¸ì§„ masking ë˜ì–´ìˆìŒ)** <br>
- **ì´í›„, outputë“¤ì„ reference ì‚¼ì•„, ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— valueê°€ encoderë¡œë¶€í„° ì˜´**

> ìœ„ì— ë§í•œ ê²ƒì€ residualì´ ì—†ì„ ë•Œì¸ë°, residualì´ ì¶”ê°€ë˜ì—ˆìœ¼ë‹ˆ, ì–´ë–»ê²Œ ë™ì‘? <br>
$$\rightarrow$$ Inputì´ outputì— ê·¸ëŒ€ë¡œ ì „ë‹¬ë˜ê³ , encoderë¥¼ refí•˜ëŠ” ê²ƒì€ ì”ì°¨ì¸ë°â€¦?
> 

ìœ„ì˜ attention processëŠ” word position ì •ë³´ë¥¼ captureí•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•¨

**Positional Embedding** <br>
d_model ì°¨ì›ì˜ position embeddingì´ í•„ìš”

$$
\mathrm{PE}(pos,2i)=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\\mathrm{PE}(pos,2i+1)=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

### **Multi-Head Attention**

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig4.jpg" width="60%" alt="Figure 4"></center>

Reference wordì— ëŒ€í•´ input sentenceì˜ ì—¬ëŸ¬ wordì— attentionì„ ì§„í–‰í•  ìˆ˜ ìˆìŒ <br>
$$\rightarrow$$ ë™ì‹œì— ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì´ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 

Multi-headì—ì„œ input vectorë“¤ì„ ì„œë¡œ ë‹¤ë¥¸ subspaceì— projection <br>
head ìˆ˜ $$h$$ì— ëŒ€í•´ input vectorì˜ $$d_{model}$$ì´ ë‚˜ë‰¨ <br>
ê°ê°ì— ëŒ€í•´ attentionì„ ì§„í–‰ í›„ concatenateë˜ê³ , $$W\in\mathbb{R}^{d_{model}\times d_{model}}$$ë¥¼ í†µí•´ projection ë¨  

$$
\mathrm{MultiHead}(Q',K',V')=\mathrm{Concat}(\mathrm{head}_1,\cdots,\mathrm{head}_â„)ğ–^0,\\\mathrm{where}\;  \mathrm{head}_i=\mathrm{Attention}(Q_i,K_i,V_i).
$$

## Other Key Concepts in Transformer
### **Feed-Forward Network**
ë‘ ê°œì˜ Linear layerì™€ nonlinear activation GELU ($$d_h=2048$$)

$$
FFN(X)=W_2\sigma(W_1X),
$$

### **Residual Connection in the Encoder and Decoder**
ê° attention blockê³¼ FFNì—ëŠ” residualì´ ì‹¤í–‰ë˜ê³ , normalizeë¨ <br>
BNëŠ” featureê°’ì´ ê¸‰ê²©íˆ ë³€í•˜ê¸° ë•Œë¬¸ì—, layer normì„ ì”€

$$
\mathrm{LayerNorm}(X+\mathrm{Attention}(X)),\\\mathrm{LayerNorm}(X+\mathrm{FFN}(X))
$$

<details markdown="block">
<summary> [Pre_LN](https://arxiv.org/abs/2002.04745) (pre-layer normalization)ë„ ë§ì´ ì“°ì„</summary>
$$\rightarrow$$ Residual ë‚´ë¶€ì—ì„œ, Attentionì´ë‚˜ FFN ì´ì „ì— normalization ìˆ˜í–‰ <br>

$$
X+\mathrm{Attention}(\mathrm{LayerNorm}(X)),\\
X+\mathrm{FFN}(\mathrm{LayerNorm}(X))
$$

Preì˜ ê²½ìš° norm í›„ ë”í•˜ê¸° ë•Œë¬¸ì—, activationì˜ í¬ê¸°ê°€ í¬ê³ , PostëŠ” ë”í•œ í›„ normì´ê¸° ë•Œë¬¸ì— scalingë˜ì–´ ìˆìŒ <br>
ë”°ë¼ì„œ, gradientì˜ upper boundë¥¼ ë³´ì•˜ì„ ë•Œ, Preì˜ scaleì´ ë” ì‘ìŒ

$$\rightarrow$$ normì„ í•˜ë©´, activationì´ ì¼ì • ë²”ìœ„ ì•ˆì— ë“¤ì–´ì˜¤ê¸° ë•Œë¬¸ì—, activation funcì—ì„œ gradientê°€ í¼ <br>
$$\rightarrow$$ PreëŠ” gradientê°€ layerì˜ ì¸µìˆ˜ì— ë°˜ë¹„ë¡€ <br>
    $$\rightarrow$$ Skip-connection ë•Œë¬¸ì—, layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡, ê°’ì€ ìŒ“ì„ (ì»¤ì§) <br>
    $$\rightarrow$$ ë”°ë¼ì„œ, layer normì—ì„œ inputì˜ í¬ê¸°ëŠ” í¬ê³ , scalingì€ ì‘ì•„ì§ <br>
        $$\rightarrow$$ outputì€ ì‘ì€ ê°’ì„ ë‚´ì•¼í•˜ëŠ” ìª½ìœ¼ë¡œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— scalingì€ ì‘ì•„ì§ (skip-connect) <br>
    $$\rightarrow$$ layer normì—ì„œ scalingì˜ gradientëŠ” inputì— ì˜í–¥ì„ ë°›ê³ , inputì˜ gradientëŠ” scaleì˜ ì˜í–¥ì„ ë°›ëŠ”ë°, output ìª½ì˜ ê°’ì´ í¬ê¸° ë•Œë¬¸ì— scaleì€ ë” ì‘ìŒ <br>
    $$\rightarrow$$ ë”°ë¼ì„œ, input ìª½ì˜ gradientëŠ” ë” ì‘ì•„ì§ <br>
    $$\rightarrow$$ ~~layerê°€ ìŒ“ì¼ìˆ˜ë¡, activationì´ ì»¤ì§€ê¸° ë•Œë¬¸ì—, ê°’ì´ act_funcì—ì„œ ëìœ¼ë¡œ ê°~~ <br>
    $$\rightarrow$$ ì›ë˜, output ìª½ì— gradientê°€ ì»¸ìœ¼ë¯€ë¡œ, ë°˜ë¹„ë¡€ë¥¼ í†µí•´ ì¼ì •í•˜ê²Œ ë§ì¶°ì§ <br>
    $$\rightarrow$$ ë”°ë¼ì„œ, preëŠ” lrì„ warmupì—†ì´ í¬ê²Œ ì¤˜ë„ í•™ìŠµì´ ë˜ê³ , ìˆ˜ë ´ ì†ë„ë„ ë” ë¹ ë¦„ <br>
        $$\rightarrow$$ warmupì€ ì´ˆê¸° gradientê°€ ë„ˆë¬´ í¬ë©´, í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê¸° ë•Œë¬¸ì—, lrì„ ì²œì²œíˆ ëŠ˜ë¦¬ëŠ” ê²ƒ <br>
$$\rightarrow$$ Postì˜ ê²½ìš°, output layer ìª½ì˜ gradientê°€ í¬ê¸° ë•Œë¬¸ì— í›ˆë ¨ì´ ë¶ˆì•ˆì •í•˜ê³ , warm upì´ í•„ìš” <br>
$$\rightarrow$$ representation collapseê°€ ì¼ì–´ë‚  ìˆ˜ ìˆê³ , postëŠ” gradient vanishingì˜ ê°€ëŠ¥ì„±ì´ ìˆìŒ
</details>

### **Final Layer in the Decoder**

Vectorë¥¼ ë‹¤ì‹œ wordë¡œ ë°”ê¾¸ê¸° ìœ„í•´ linear + softmax <br>
Linear: $$d_{word}$$ë¡œ logitsë¥¼ projection

ì´í›„ ëŒ€ë¶€ë¶„ì˜ CV TransformerëŠ” encoderì˜ í˜•íƒœë¥¼ ì·¨í•¨ <br>
$$\rightarrow$$ Encoderê°€ feature extractorë¡œ ìƒê°ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ <br>
$$\rightarrow$$ global featureë¥¼ captureí•  ìˆ˜ ìˆìŒ <br>
$$\rightarrow$$ ë³‘ë ¬ ê³„ì‚°ì´ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì„

# **Vision Transformer**
CVì—ì„œ transformerì˜ ì‚¬ìš©ì„ Review

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig5.jpg" width="60%" alt="Figure 5"></center>

## **Backbone for Representation Learning**

ì´ë¯¸ì§€ì˜ ê²½ìš° text ë³´ë‹¤, dimì´ í¬ê³ , noiseê°€ ë§ê³ , ì¤‘ë³µ pixelì´ ë§ìŒ <br>
TransformerëŠ” CNNê³¼ ë¹„ìŠ·í•˜ê²Œ, backboneìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ <br>
ì´ëŸ° ê²°ê³¼ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ í‘œì— ìš”ì•½ë¨

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig6.jpg" width="60%" alt="Figure 6"></center>

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig7.jpg" width="60%" alt="Figure 7"></center>
(a) Acc v.s. FLOPs.

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig8.jpg" width="60%" alt="Figure 8"></center>
(b) Acc v.s. throughput.

### Pure Transformer
**VIT.** <br>
Classification taskì— ëŒ€í•´, image patchë¥¼ ë°”ë¡œ transformerì— ì ìš© <br>
Transformerì˜ frameworkë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì”€

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig9.jpg" width="60%" alt="Figure 9"></center>
Make Patch: $$X\in\mathbb{R}^{h\times w\times c} \rightarrow X_p\in\mathbb{R}^{n\times(p^2\cdot c)},\;\; p: \mathrm{patch \;resolution}$$

ë”°ë¼ì„œ, patchì˜ ê°œìˆ˜ $$n=hw/p^2$$

TransformerëŠ” ëª¨ë“  layerì—ì„œ ì¼ì •í•œ width(dim)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— patch vectorë¥¼ trainable linear projectionë¥¼ í†µí•´ embedding spaceë¡œ mapping <br>
ì¶”ê°€ì ìœ¼ë¡œ Positional embeddingê³¼ learnable class token (image í‘œí˜„ í•™ìŠµ) ì‚¬ìš©

class ê°œìˆ˜ì— ë”°ë¼, classification head attach <br>
$$\rightarrow$$ ì¼ë°˜ì ìœ¼ë¡œ, encoderë¥¼ large datasetìœ¼ë¡œ pretrained í›„, headì— transfer learning

Vision TransformerëŠ” large datasetì—ì„œ ê°•ì ì´ ìˆìŒ <br>
inductive biasê°€ ì—†ê¸° ë•Œë¬¸ì—, ì‘ì€ datasetì—ì„œëŠ” ì•½í•œ ëª¨ìŠµì„ ë³´ì„ <br>
Pretrainedì„ í•œ í›„ì—ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

DeiT(Data-efficient image transformer)ì—ì„œëŠ” convolution-free transformerë¥¼ ë³´ì„ <br>
CNN teacher, distillation token ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì„ <br>
- distillation tokenì€ CNN teacherì˜ labelì„ ì˜ˆì¸¡í•¨ <br>
ì¶”ê°€ì ìœ¼ë¡œ, Strong data augmentë¥¼ í†µí•´ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig10.jpg" width="60%" alt="Figure 10"></center>

**Variants of ViT.**

ì´í›„, locality, self-attentionì˜ ì„±ëŠ¥ ë“±ì„ ë†’ì´ë ¤ëŠ” ì—°êµ¬ê°€ ë§ì´ ì§„í–‰ë¨

$$\rightarrow$$ VITëŠ” long-rangeëŠ” ì˜ captureí–ˆì§€ë§Œ, localì€ ê°„ë‹¨í•œ linearë¡œ modelingë˜ì–´ ì•½í–ˆìŒ <br>
$$\rightarrow$$ ë˜í•œ overlapì—†ì´, sliding í•˜ëŠ” hard split ë°©ë²•ìœ¼ë¡œ tokenì„ ì–»ê¸° ë•Œë¬¸ì— image local structure(ex. edges and lines)ë¥¼ modelingí•  ìˆ˜ ì—†ê²Œ ë¨

local ì„±ëŠ¥ ì¦ê°€ <br>
- TNT[[29](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib29)]ëŠ” patchë¥¼ ë˜ subpatchë¡œ ë‚˜ëˆ , transformer-in-transformer êµ¬ì¡°ë¥¼ ì”€ <br>
- TwinsÂ [[62](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib62)]ê³¼ CAT[[63](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib63)]ì€ layer-by-layerë¡œ local, global attentionì„ ë²ˆê°ˆì•„ ìˆ˜í–‰ <br>
- **Swin Transformer**sÂ [[60](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib60),Â [64](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib64)]ëŠ” window ë‚´ì—ì„œ local attentionì„ ìˆ˜í–‰í•˜ê³ , windowë¥¼ shiftingí•´ê°€ë©° globalì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ìŒ <br>
- Shuffle TransformerÂ [[65](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib65),Â [66](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib66)]ëŠ” shifting ëŒ€ì‹  shuffleì„ ìˆ˜í–‰ <br>
- RegionViTÂ [[61](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib61)]ì—ì„  global, local tokenì„ ë”°ë¡œ ìƒì„±í•˜ì—¬, localì€ cross-attentionì„ í†µí•´ globalì˜ ì •ë³´ë¥¼ ì–»ìŒ <br>
- **T2T**[[67](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib67)]ì€ local feature aggregationì„ í†µí•´ local informationë¥¼ boosting <br>
    - tokensì˜ ê¸¸ì´ë¥¼ progressiveí•˜ê²Œ ì¤„ì„
    - layer-wise **Token-to-Token module: **ì´ë¯¸ì§€ì˜ local structure informationì„ ë‹´ìŒ
        - Re-structurization
            - ì´ì „ transformer layerì˜ tokensì´ $$T$$ì¼ ë•Œ,
            
            $$
            T'=MLP(MSA(T)),\\I=Reshape(T'),\;\;T'\in\mathbb{R}^{l\times c},I\in\mathbb{R}^{h\times w\times c}
            $$
            
        - Soft Split(SS)
            - Re-structurizationì—ì„œ ì–»ì€ $$I$$ë¥¼ ë‹¤ì‹œ tokenizeí•˜ëŠ” ê³³
            - tokenì„ ë§Œë“¤ ë•Œ information lossê°€ ìƒê¸¸ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—Â patchë¥¼ overlapí•˜ë©´ì„œ split
            - ê° patchëŠ” surrounding patchesì™€ correlationì„ ê°€ì§€ê²Œ ë¨
                - CNNì˜ locality inductive biasì™€ ë¹„ìŠ·í•œ priorë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
            - split patchesë¥¼Â í•˜ë‚˜ì˜ tokenìœ¼ë¡œ concat
                - local informationì´Â surrounding pixelsê³¼ patchesë¡œë¶€í„° aggregate
                
                $$
                T_o=\mathbb{R}^{l_o\times ck^2},\\ \mathrm{where\;} l_o=[\frac{h+2p-k}{k-s}+1]\times [\frac{w+2p-k}{k-s}+1],\\ \mathrm{patch\;size}=k, \mathrm{overlap\;size}=s, \mathrm{padding}=p
                $$
                
    - ë”°ë¼ì„œ, T2T moduleì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ tokensì˜ ê¸¸ì´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê³  ì´ë¯¸ì§€ì˜ spatial structureë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŒ
        - $$k > s$$ì´ê¸° ë•Œë¬¸ì—, h,wëŠ” ì ì  ì¤„ì–´ë“¦
        - ìµœì¢…ì ìœ¼ë¡œ,
        
        $$
        T'_i=MLP(MSA(T_i))\\I_i=Reshape(T'_i)\\T_{i+1}=SS(I_i), i=1,\dots,(n-1)
        $$
        
    - T2T-ViT backbone: **T2T moduleë¡œë¶€í„° tokensì˜ global attention relation
        - feature richnessì™€ connectivityë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ DenseNetì˜ dense connection
        - channel dimensionê³¼ head numberë¥¼ ë°”ê¾¸ê¸° ìœ„í•œ Wide-ResNets ë˜ëŠ” ResNeXt
        - Squeeze-an-Excitation Networks
        - GhostNet
        
        <center><img src="/assets/images/cv/vit/survey/survey-vit_fig11.jpg" width="60%" alt="Figure 11"></center>
        

self-attention ì„±ëŠ¥ ì¦ê°€

- DeepViT[[68](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib68)]:Â Cross-head communication ì œì•ˆ (re-generate the attention map)
- KVT[[69](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib69)]: Top-k attention ë§Œ ê³„ì‚°í•˜ëŠ” k-NN attention ì œì•ˆ
- XCiT[[71](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib71)]:Â self-attentionì„ channel-wiseë¡œ ìˆ˜í–‰í•˜ì—¬, high-resì— íš¨ê³¼ì 

Architectureì˜ ê²½ìš°, pyramidí˜• (Swin, HVT, PiT, etcâ€¦), 2-wayí˜• (UNet êµ¬ì¡°), NAS ë“±ì´ í™œìš©ë˜ê³  ìˆìŒ

### Transformer with Convolution

Vision Transformerì— localityë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ CNNì„ ë³‘í•©

- **BoTNet**[[100](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib100)]: ResNetì˜ ë§ˆì§€ë§‰ 3ê°œ blockì—ì„œ convë¥¼ self-attentionìœ¼ë¡œ ëŒ€ì²´, latencyì˜ overheadë¥¼ ìµœì†Œí™”í•˜ê³  baselineì„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤4

ë˜í•œ, transformerëŠ” optimizer, hyper-parameter, schedule of trainingì— ë¯¼ê°

ê·¸ ì´ìœ ê°€ ViTì˜ ì´ˆê¸° patchfiyì—ì„œ ì‚¬ìš©í•˜ëŠ” $$s$$ stride, kernel sizeì˜ convë¼ê³  ì§€ì  

$$\rightarrow$$ stride:2, kernel size: 3*3ì¸ conv stemì„ ì‚¬ìš©í•˜ì—¬ ì™„í™”ì‹œí‚´ ([Early Convolution](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib102), [CMT](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib94))

$$\rightarrow$$ single transformer blockì™€ ë¹„ìŠ·í•œ complexityë¥¼ ê°–ëŠ” 5 layer ì´í•˜ì˜ CNN

### Self-supervised Representation Learning

**Generative Based Approach.**

- iGPT[[14](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib14)]: low resolutionìœ¼ë¡œ reshape í›„, flatten. ì´í›„, bertì™€ ê°™ì´ í•™ìŠµí•˜ì—¬ pretrain
    - ê° pixelì€ sequentialë¡œ ì·¨ê¸‰ë˜ì–´, train
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig12.jpg" width="60%" alt="Figure 12"></center>
    

**Contrastive Learning Based Approach.**

í˜„ì¬ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” Self-supervised method

- [MoCo v3](https://arxiv.org/abs/2104.02057)ì—ì„œ Transformerì— contrastive ì‚¬ìš©
    - instabilityê°€ ViT pretrainì˜ ë‚®ì€ ì •í™•ë„ì˜ ë¬¸ì œë¡œ ë³´ì„
    - í•œ ì´ë¯¸ì§€ì—ì„œ 2ê°œì˜ cropì„ ì–»ìŒ (vector $$q,k$$ë¡œ encodeë¨)
    - ìœ„ì˜ ë‘ latent vectorë¥¼ contrastive loss ìµœì†Œí™”í•˜ëŠ” ê²ƒìœ¼ë¡œ formulate
    
    $$
    L_q=-\log\frac{\exp{(q\cdot k^+/\tau)}}{\exp(q\cdot k^+/\tau)+\sum_{k^-}\exp(q\cdot k^-/\tau)}
    $$
    
    - ViTì—ì„œ patch projection layerë¥¼ í›ˆë ¨í•˜ì§€ ì•Šê³ , randomí•˜ê²Œ í•˜ë©´, ì•ˆì •ì„±ì´ ì¢‹ì•„ì§€ì§€ë§Œ ì™„ì „í•œ í•´ê²°ì±…ì€ ì•„ë‹˜

### Discussions

MHSA, MLP, shortcut, LN, PE, network êµ¬ì„±ì€ ëª¨ë‘ vision transformerì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•¨

## High/Mid-level Vision

object detectionÂ [[16](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib16),Â [17](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib17),Â [113](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib113),Â [114](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib114),Â [115](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib115)], lane detectionÂ [[116](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib116)], segmentationÂ [[33](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib33),Â [25](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib25),Â [18](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib18)], pose estimationÂ [[34](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib34),Â [35](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib35),Â [36](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib36),Â [117](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib117)]ì— ëŒ€í•´ review

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig13.jpg" width="60%" alt="Figure 13"></center>

### Generic Object Detection

CNNì´ basicì´ì§€ë§Œ, transformerë„ ê°ê´‘ë°›ê³  ìˆìŒ

ODì—ì„œ transformerëŠ” í¬ê²Œ 2ê°€ì§€ë¡œ ë‚˜ë‰¨

1. Transformer-based set prediction methods
2. Transformer-based backbone methods

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig14.jpg" width="60%" alt="Figure 14"></center> 

**Transformer-based Set Prediction for Detection**.

**DETR**[[16](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib16)]ì—ì„œ Object detectionì„ intuitive set prediction problemìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ NMSê°™ì€ post-processingë¥¼ ì œê±°í•¨

- CNN backboneìœ¼ë¡œ imageì˜ featureë¥¼ ì¶”ì¶œ
- flattened featureì—ì„œ position infoë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´, flattened featureì— PE ì‚¬ìš©
- Encoder ì´í›„, object(imageì— ìˆëŠ”)ì˜ PEê°€ ê°™ì´ decoderì˜ inputìœ¼ë¡œ ë“¤ì–´ê°
    - PEì˜ Nì˜ ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ì— ìˆëŠ” ê°ì²´ ìˆ˜ë³´ë‹¤ í¼
- FFNì€ classì™€ bounding box coordë¥¼ predictí•˜ëŠ” ê²ƒì— ì‚¬ìš©ë¨
    - Nê°œì˜ objectê°€ ë³‘ë ¬ë¡œ ì˜ˆì¸¡ë¨
    - bipartite matchingì„ ì‚¬ìš©í•˜ì—¬ GTì— ëŒ€í•´ predict í• ë‹¹
    - Hungarian lossë¡œ ëª¨ë“  matchingì— ëŒ€í•œ loss ê³„ì‚°
    
    $$
    L_{Hungarian}(y,\hat{y})=\sum^N_{i=1}[-\log \hat{p}_{\hat{\sigma}(i)}(c_i)+\mathbb{I}_{\{c_i\neq\varnothing\}}L_{box}(b_i,\hat{b}_{\hat{\sigma}}(i))],
    $$
    
- End-to-End object detection framework ì œì•ˆ
- ê¸´ trainingê³¼ ì‘ì€ objectì— ëŒ€í•œ ë¬¸ì œê°€ ìˆìŒ

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig15.jpg" width="60%" alt="Figure 15"></center>

Deformable DETR[[17](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib17)]ì—ì„œ ìœ„ì˜ ë¬¸ì œë¥¼ ë³´ì™„

- image feature mapì˜ spatial locationë¥¼ ë³´ë˜ ê¸°ì¡´ DETRì—ì„œ reference point ì£¼ë³€ì˜ ì‘ì€ key position setì— ì§‘ì¤‘
- compute costë¥¼ ì¤„ì´ê³ , ë¹ ë¥´ê²Œ ìˆ˜ë ´í•¨
- ë˜í•œ pyramidí˜•ì‹ìœ¼ë¡œ ë³€í˜•í•˜ê¸° ì‰¬ì›€

[[120](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib120)]: Decoderì˜ cross-attentionì—ì„œ ë°œìƒí•˜ëŠ” ëŠë¦° ìˆ˜ë ´ ê°œì„  (Encoder Only Pyramid)

[[123](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib123)]: SMCA(Spatially Modulated Co-Attention)

[[121](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib121)]: ACT(Adaptive Clustering Transformer) ê³„ì‚° ë¹„ìš©ì„ ì¤„ì„

**Transformer-based Backbone for Detection**.

ViTì™€ ë¹„ìŠ·í•˜ê²Œ transformer ìì²´ë¥¼ backboneìœ¼ë¡œ í™œìš©

Patch ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ê°€ splitë˜ì–´ transformerì˜ inputìœ¼ë¡œ ë“¤ì–´ê°

í˜¹ì€ general transformer ì´í›„, traditional backboneìœ¼ë¡œ transfarí•  ìˆ˜ ìˆìŒ

**Pre-training for Transformer-based Object Detection.**

**UP-DETR**[[32](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib32)]: unsupervised pretext task(query patch detection)

DETReg[129](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib129)]: Selective Searchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ì˜ì˜ objectë¥¼ preditioní•˜ë„ë¡ pretrain

YOLOS[[126](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib126)]: ViTì—ì„œ class token ëŒ€ì‹  detection tokenì„ ì‚¬ìš©

### Segmentation

**Panoptic Segmentation.**

DETRì„ í™•ì¥í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

Max-DeepLab[[25](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib25)]: Mask Transformer ì‚¬ìš©

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig16.jpg" width="60%" alt="Figure 16"></center>

Max-DeepLab

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig17.jpg" width="60%" alt="Figure 17"></center>

Mask Transformer

**Instance Segmentation.**

**VisTR**[[33](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib33)]: Videoì—ì„œ instance segë¥¼ í•˜ê¸° ìœ„í•´, 3D CNNìœ¼ë¡œ image ë“¤ì˜ featureë¥¼ ì–»ì€ í›„, Transformer

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig18.jpg" width="60%" alt="Figure 18"></center>

**Semantic Segmentation.**

- **[SETR](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf)**[[18](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib18)]: ViTì˜ encoderì— Decoderë¥¼ design (2ê°œë¥¼ designí•¨)
    - Encoderì˜ output ZëŠ” $$\frac{HW}{256}\times C$$ë¡œ reshape ë˜ê³ ,
    - SETR-PUP
        - x2 ì”©, 4ë²ˆ upsampling í•˜ì—¬, $$(H, W)$$ resolution output ì–»ìŒ
    - SETR-MLA
        - M streamì— ëŒ€í•´ aggregateí•´ê°€ë©° outputì„ ì–»ìŒ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig19.jpg" width="60%" alt="Figure 19"></center>
    
- SegFormer[[135](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib135)]: multi layerì—ì„œ featureë¥¼ ë½‘ê³ , MLPë¥¼ ì ìš©í•˜ì—¬ decoderì™€ Transformerë¥¼ í†µí•©í•˜ì—¬ ê°„ë‹¨í•˜ê³  ì¢‹ì€ framework ì œì•ˆ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig20.jpg" width="60%" alt="Figure 120"></center>
    

**Medical Image Segmentation.**

- **Swin-UNet**[[30](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib30)]: Transformer-based U-shaped Encoder-Decoder architecture
    - tokenized image patcheë¥¼ ì‚¬ìš©í•˜ê³ ,
    - local-global semantic featureë¥¼ ìœ„í•œ skip-connection ì‚¬ìš©

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig21.jpg" width="60%" alt="Figure 21"></center>

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig22.jpg" width="60%" alt="Figure 22"></center>

- Medical Transformer[[136](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib136)]: self-attentionì— control mechanism ì ìš©
    - Gated Axial-Attention model ì œì•ˆ
        
        <center><img src="/assets/images/cv/vit/survey/survey-vit_fig23.jpg" width="60%" alt="Figure 23"></center>
        

### Pose Estimation

RGBDì¸ input imageì—ì„œ joint coordinate ë˜ëŠ” mesh verticeë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œ

**Hand Pose Estimation.**

- PointNet[[138](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib138)]: Inputìœ¼ë¡œ Point cloudê°€ ë“¤ì–´ê°ˆ ë•Œì— ëŒ€í•œ Classification, Segmentation ëª¨ë¸
    
    ê¸°ì¡´ point cloudëŠ” 3D voxelë¡œ ë°”ê¾¸ëŠ” ê²ƒê³¼ ê°™ì´, discrete dataë¡œ ë³€í™˜ í›„ NN modelì— input
    
    $$\rightarrow$$ í•˜ì§€ë§Œ quantization artifactê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
    
    $$\rightarrow$$ Point cloudëŠ” meshì™€ ê°™ì´ ì´ì›ƒ ì ë“¤ì„ ê³ ë ¤í•˜ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì— ì‰¬ì›€
    
    $$\rightarrow$$ ì ë“¤ì˜ ì¡°í•©ì´ë¯€ë¡œ permutation invariantí•˜ê³  rigid motionì—ë„ invariant
    
    $$\rightarrow$$ ì ì˜ ìˆœì„œì— ë”°ë¼ ë‹¬ë¼ì§€ì§€ ì•Šê³ , ì „ì²´ë¥¼ íšŒì „, ì´ë™í•´ë„ 3D í˜•ì²´ê°€ ë‹¬ë¼ì§€ì§€ ì•ŠìŒ
    
    $$\rightarrow$$ í•˜ì§€ë§Œ local pointê°„ì—ëŠ” ì—°ê´€ì„±ì´ ìˆì„ ìˆ˜ ìˆìŒ
    
    - Point CloudëŠ” x,y,z corrdì™€ feature channel (color, normal) ë“±ì˜ ì •ë³´ê°€ ìˆìŒ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig24.jpg" width="60%" alt="Figure 24"></center>
    
    - input permutatonì— invariantí•˜ë ¤ë©´ 3ê°€ì§€ì˜ ë°©ë²•ì´ ìˆìŒ
        1. inputì„ canonicalÂ í•˜ê²Œ ì •ë ¬
            1. data perturbationì— stableí•˜ì§€ ì•ŠìŒ
        2. inputì„ seqì²˜ëŸ¼ ë‹¤ë£¸. ë‹¨, ëª¨ë“  permutationì— ëŒ€í•´ augment
            1. Point ë“¤ì— ìˆœì„œê°€ ìƒê²¨ë²„ë¦¼
        3. **symmetric fucntion**ì„ ì‚¬ìš©í•¨. ex) +ì™€ *ëŠ” symmetric binary function
            1. MLPì™€ max pooling ì‚¬ìš©
    - MLPëŠ” input nodeì— ëŒ€í•´ weightë¥¼ sharing
    - Point ê°„ ì •ë³´ëŠ” max poolingì—ì„œ ì–»ìŒ
    - Segmentationì€ global ì •ë³´ë„ í•„ìš”í•˜ê¸° ë•Œë¬¸ì—, ê° pointì— appendix
    - T-Netì˜ ê²½ìš° permutation invariantë¥¼ ìœ„í•´ orthogonal regularization ì¶”ê°€
    
    $$
    L_{reg}=\lVert I-AA^T \rVert^2_F
    $$
    
- PointNet++[[139](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib139)]: PointNetì—ì„œ ë” ê³„ì¸µì ìœ¼ë¡œ featureë¥¼ ë½‘ìŒ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig25.jpg" width="60%" alt="Figure 25"></center>
    
- Hand-transformer[[34](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib34)]: PointNetìœ¼ë¡œ featureë¥¼ ë½‘ì€ í›„, MHSAë¡œ embeddingì„ ë½‘
    - Reference Extractorë¥¼ ë½‘ì€ í›„, positional encodingìœ¼ë¡œ decoderì— input
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig26.jpg" width="60%" alt="Figure 26"></center>
    
- HOT-Net[[35](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib35)]: ResNetìœ¼ë¡œ 2D hand-object poseë¥¼ ì¶”ì • í›„, transformerì˜ inputìœ¼ë¡œ 3D hand-object pose predict
- Handsformer[[140](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib140)]: ì†ì— ëŒ€í•œ color imageê°€ inputìœ¼ë¡œ ë“¤ì–´ê°€ë©´ 2D location setê³¼ Spatial PEê°€ MHSAë¡œ ë“¤ì–´ê°
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig27.jpg" width="60%" alt="Figure 27"></center>
    

**Human Pose Estimation.**

- METRO[[36](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib36)]: 2D ì´ë¯¸ì§€ì—ì„œ ì‚¬ëŒ pose ì¶”ì •
    - CNNìœ¼ë¡œ feature ì¶”ì¶œ í›„, template human meshì— concateí•˜ì—¬ PE
    - ì´í›„, multi-layer transformer encoder
    - Human poseì—ì„œ non-local relationshipì— ëŒ€í•œ í•™ìŠµì„ ìœ„í•´ ì¼ë¶€ maskingí•˜ë©° ì§„í–‰
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig28.jpg" width="60%" alt="Figure 28"></center>
    
- Transpose[[117](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib117)]: ê° poseì˜ keypointsê°€ ì–´ë–¤ spatial pointë¥¼ referí–ˆëŠ”ì§€, heatmap ì¶”ì •
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig29.jpg" width="60%" alt="Figure 29"></center>
    
- TokenPose[[141](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib141)]: Pose Estimì— Keypoint tokenì„ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²• ì œì‹œ
    - ì´ë¯¸ì§€ë¥¼ CNNì„ í†µí•´ vectorë¥¼ ë½‘ê³ , transformerì— PEì™€ ê°™ì´ inputìœ¼ë¡œ ë“¤ì–´ê°
    - ê° ê´€ì ˆì— ëŒ€í•´ keypoint vectorë¥¼ Transformerì— inputìœ¼ë¡œ ê°™ì´ ë„£ê³ , outputì„ ë½‘ìŒ
    - ì´ë¯¸ì§€ì™€ ê´€ì ˆì— ëŒ€í•œ contraintë¥¼ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ìˆìŒ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig30.jpg" width="60%" alt="Figure 30"></center>
    
- Test-Time Personalization[[144](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib144)]: Human labeling ì—†ì´, test-timeì—ì„œ pose personalize
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig31.jpg" width="60%" alt="Figure 31"></center>
    

### Other Tasks

**Pedestrian Detection.**

PED[[145](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib145)]: DETR, Deformable DETRì€ ì¡°ë°€í•œ pedestrianì— ì—´í™”ê°€ ìˆìœ¼ë¯€ë¡œ, dense queryì™€ ê°•í•œ attentionì„ ê°€ì§€ëŠ” DQRF(Rectified Attention field) decoder ì œì•ˆ

**Lane Detection.**

LSTR[116]: Transformerë¥¼ í†µí•´, global context í•™ìŠµ

**Scene Graph.**

Graph R-CNN[149]: self-attentionì„ í†µí•´ ì¸ì ‘ ë…¸ë“œì˜ ìƒí™© ì •ë³´ë¥¼ ê·¸ë˜í”„ì— í†µí•©

Texema[151]: T5(Text-to-Text Transfer Transformer)ë¥¼ ì‚¬ìš©í•˜ì—¬, text inputì—ì„œ structured graphë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ í™œìš©í•˜ì—¬ relational reasoning module ê°œì„ 

**Tracking.**

- TMT[153]: Template-based discriminative trackerì—ì„œ Transformer encoder-decoder
- TransT[155]: Template-based discriminative trackerì—ì„œ Transformer encoder-decoder
- TransTrack[156]: online joint-detection-and-tracking
    - Query, Key ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ê¸°ì¡´ ê°œì²´ë¥¼ ì¶”ì 
    - learned object querie setì„ pipelineì— ë„ì…í•˜ì—¬ ìƒˆë¡œìš´ objectë¥¼ ê²€ìƒ‰

**Re-Identification.**

- TransReID[[157](https://arxiv.org/abs/2102.04378)]: Imageì˜ idë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” detail featureë¥¼ ë³´ì¡´í•˜ë©´ì„œ re-identification
    - VIT êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨
    - JPM (jigsaw patch module): shift ë° shuffleì„ í†µí•´ patch embeddingì„ ì¬ë°°ì—´
    - SIE (side information embeddings): ì¹´ë©”ë¼ì˜ view, variationsì— ëŒ€í•œ í¸í–¥ ì™„í™”
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig32.jpg" width="60%" alt="Figure 32"></center>
    
- Re-ID[[158](https://arxiv.org/abs/2104.01745)]: Videoì— ëŒ€í•œ Re-ID
    - spatial, temporal featureë¥¼ refine í›„ cross view transformerë¡œ multi-view aggregate
- Spatiotemporal Transformer[[159](https://arxiv.org/abs/2103.16469)]: Videoì— ëŒ€í•œ Re-ID
    - spatial, temporal featureë¥¼ refine í›„ cross view transformerë¡œ multi-view aggregate

**Point Cloud Learning.**

- Point Transformer[[162](https://arxiv.org/abs/2012.09164)]: Point Transformerë¼ëŠ” ìƒˆë¡œìš´ transformer framework ì œì•ˆ
    - self-attention layerê°€ point setì˜ permutationì— invariant
    - 3D point cloudì˜ segmentì— ê°•í•¨
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig33.jpg" width="60%" alt="Figure 33"></center>
    
- PCT[[161](https://arxiv.org/abs/2012.09688)]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig34.jpg" width="60%" alt="Figure 34"></center>
    

## Low-level Vision

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig35.jpg" width="60%" alt="Figure 35"></center>

### Image Generation

- TransGAN[[38](https://arxiv.org/abs/2102.07074)]: ë‹¨ê³„ë³„ë¡œ feature map resolutionë¥¼ ì ì§„ì ìœ¼ë¡œ ë†’ì—¬, memory-friendly generator
    - DiscriminatorëŠ” ë‹¤ì–‘í•œ inputì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨
    - ì„±ëŠ¥ê³¼ ì•ˆì •ì„± ì¦ê°€ë¥¼ ìœ„í•´ grid self-attention, data augmentation, relative position encoding, modified normalization ë„ì…
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig36.jpg" width="60%" alt="Figure 36"></center>
    
- ViTGAN[[163](https://arxiv.org/abs/2107.04589)]: INR ë° Self-modulated layernormê³¼ Euclidean distance ì‚¬ìš©
    - Discì˜ Lipschitznessë¥¼ ìœ„í•´ self-attentionì— Euclidean distance ë„ì…
    
    $$
    \mathrm{Attention}_h(X)=\mathrm{softmax}(\frac{d(XW_q,XW_k)}{\sqrt{d_h}})XW_v,\;\;\mathrm{where\;} W_q=W_k
    $$
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig37.jpg" width="60%" alt="Figure 37"></center>

    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig38.jpg" width="60%" alt="Figure 38"></center>
    
    
- VQGAN[[37](https://arxiv.org/abs/2012.09841)]: ì§ì ‘ì ìœ¼ë¡œ tranformerì—ì„œ high resë¥¼ ë½‘ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, VQVAEì˜ latentì—ì„œ pixelCNNì„ ì´ìš©í•˜ì—¬, autoregressiveë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë˜ ë°©ì‹ì— transformer ë„ì…
- DALL-E[41]

### Image Processing

ì¼ë°˜ì ìœ¼ë¡œ, pixelì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ patch ë‹¨ìœ„ë¡œ transformerì— ì‚¬ìš©í•¨

- **TTSR**[[39](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf)]: Reference-base SR (Refer imageì—ì„œ low-resolutionìœ¼ë¡œ relevant texture transfer)
    - Low-res inputê³¼ low-res referì—ì„œ ê°ê° $$q,k$$ë¥¼ ì–»ì–´, ì—°ê´€ì„± $$r_{i,j}$$ ê³„ì‚°
        
        $$
        r_{i,j}=\left\langle \frac{q_i}{\lVert q_i\rVert},\frac{k_i}{\lVert k_i\rVert} \right\rangle
        $$
        
    - ì—°ê´€ì„±ì´ ë†’ì€ feature $$h_i = \arg\max_j r_{i,j}$$ ë§Œ ë½‘ì•„, $$V$$ patchì— hard-attention
    - ì´ë¡œ ì¸í•´, high-resì˜ refer imageì˜ featureê°€ low-res imageì— transferë¨

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig39.jpg" width="60%" alt="Figure 39"></center>

- **IPT**[[19](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf)]: Large datasetìœ¼ë¡œ pre-trainëœ Transformer í™œìš© (Multi image processing task ê°€ëŠ¥)
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig40.jpg" width="60%" alt="Figure 40"></center>
    
- SceneFormer[[165](https://ar5iv.labs.arxiv.org/html/2012.12556#bib.bib165)]: Indoor ì¥ë©´ ë³€í™˜

## Video Processing

frame synthesis, action recognition, video retrieval, etcâ€¦

### High-level Video Processing

**Video Action Recognition.**

Videoì—ì„œ human actionì„ identifying, localizing

Videoì—ì„œ targetì´ ì•„ë‹Œ ë‹¤ë¥¸ ì‚¬ëŒì´ë‚˜ ì‚¬ë¬¼ì€ criticalí•˜ê²Œ ì‘ìš©í•  ìˆ˜ ìˆìŒ

- **Action Transformer**[[172](https://arxiv.org/abs/1812.02707)]: I3Dë¥¼ ì´ìš©í•˜ì—¬, actionì— ëŒ€í•œ featureë¥¼ ë½‘ê³ , intermidiate featureì˜ ê°ì²´ì— ëŒ€í•œ ROI poolë¥¼ query, featureë¥¼ K, Vë¡œ í•˜ì—¬ ROIì˜ ê°ì²´ì— ëŒ€í•œ classification
- **Temporal Transformer Network**[[175](https://arxiv.org/abs/1906.05947)]: Class ë‚´ varianceë¥¼ ì¤„ì´ê³ , class ê°„ varianceë¥¼ ë„“í˜
- **Actor-transformer**[[178](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gavrilyuk_Actor-Transformers_for_Group_Activity_Recognition_CVPR_2020_paper.pdf)]: Groupì—ì„œì˜ í–‰ë™ì¸ì‹. ê° inputì˜ object ë“¤ì„ ì…ë ¥ìœ¼ë¡œ, embedding í›„ Transformer. Outputì€ ì˜ˆì¸¡ëœ í–‰ë™ (=classification)

**Video Retrieval.**

Videoì—ì„œ ë¹„ìŠ·í•œ ë¶€ë¶„ì„ ì°¾ëŠ” ê²ƒì´ ê´€ê±´ì„

- **Temporal context aggregation**[[179](https://openaccess.thecvf.com/content/WACV2021/papers/Shao_Temporal_Context_Aggregation_for_Video_Retrieval_With_Contrastive_Learning_WACV_2021_paper.pdf)]: ê° frameì—ì„œ featureë¥¼ ë½‘ì€ í›„, temporal transformer. ì´í›„, contrastive learningìœ¼ë¡œ featureì— ëŒ€í•´ pairì˜ pos, negë¥¼ í•™ìŠµ
- **Multi-modal Transformer for Video Retrieval**[[180](https://arxiv.org/abs/2007.10639)]

**Video Object Detection.**

ì˜ìƒì—ì„œ ê°ì²´ íƒì§€

ì¼ë°˜ì ìœ¼ë¡œ ì „ì—­ì—ì„œ objectë¥¼ detectí•˜ê³ , localì—ì„œ classification

- **MEGA**[181]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig41.jpg" width="60%" alt="Figure 41"></center>
    

**Multi-task Learning.**

Videoì—ëŠ” targetê³¼ ê´€ê³„ì—†ëŠ” frameì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ìˆìŒ

- **Video Multitask Transformer Network**[[183](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CoView/Seong_Video_Multitask_Transformer_Network_ICCVW_2019_paper.pdf)]

### Low-level Video Processing

**Frame/Video Synthesis.**

Frameì— ëŒ€í•œ extrapolation

- **ConvTransformer**[[171](https://arxiv.org/abs/2011.10185)]: feature embedding, position encoding, encoder, query decoder, synthesis feed forward network
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig42.jpg" width="60%" alt="Figure 42"></center>
    
    - Decoderì˜ Query set $$\mathcal{Q}$$ëŠ” $$t-0.5$$ì˜ frameì„ ë§Œë“¤ ë•Œ, $$\mathcal{J}$$(feature embeddingí•œ ê°’)ê°€ $$t-1$$ì¼ ë•Œì™€ $$t$$ì¼ ë•Œì˜ average
    - SSFNì€ UNet í˜•ì‹ì˜ networkê°€ ê³„ë‹¨ì‹ìœ¼ë¡œ êµ¬ì„±ë¨

**Video Inpainting.**

- **Learning Joint Spatial-Temporal Transformations for Video Inpainting**[[28](https://arxiv.org/abs/2007.10247)]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig43.jpg" width="60%" alt="Figure 43"></center>
    

## Multi-Modal Tasks

Video2Text, Image2Text, Audio2Text, etcâ€¦

Transformer-based Modelì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ í†µí•©í•˜ê¸°ì— íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì³ë¥¼ ê°€ì§€ê³ , ë°©ëŒ€í•œ ë°ì´í„°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” capabilityë¥¼ ê°€ì§€ê³  ìˆì–´, í™•ì¥ì„±ì´ ì¢‹ìŒ

### **CLIP**[[41](https://arxiv.org/abs/2103.00020)]

Text-Encoderì™€ Image-Encoderì— ëŒ€í•´ ë‘ latentê°€ pairí•œì§€, contrastive learningìœ¼ë¡œ í•™ìŠµ

- Text encoder: Standard Transformer with Masked self-attention
- Image encoder: ResNet ë˜ëŠ” Vision Transformer

Nê°œì˜ image-text pairê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $$N$$ê°œì˜ positive pairì— ëŒ€í•´ì„œëŠ” cosine similarityê°€ í¬ê²Œ, $$N^2-N$$ê°œì˜ negative pairì— ëŒ€í•´ì„œëŠ” cosine similarityê°€ ì‘ê²Œ í•™ìŠµ

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig44.jpg" width="60%" alt="Figure 44"></center>

- **Unified Transformer**[[189](https://arxiv.org/abs/2105.13290)]: Multi-Modal Multi-Task Model
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig45.jpg" width="60%" alt="Figure 45"></center>
    
- **VideoBERT**[[185](https://arxiv.org/abs/1904.01766)]: CNNìœ¼ë¡œ video feature ì¶”ì¶œ í›„, Transformerë¡œ caption í•™ìŠµ
    - Bertì— inputë§Œ CNNì˜ feature tokenìœ¼ë¡œ ë°”ë€ ëŠë‚Œ

**VQA**(Visual Question Answering)

- **VisualBERT**[[186](https://arxiv.org/abs/1908.03557)]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig46.jpg" width="60%" alt="Figure 46"></center>
    

**VCR**(Visual Commonsense Reasoning)

- **VL-BERT**[[187](https://arxiv.org/abs/1908.08530)]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig47.jpg" width="60%" alt="Figure 47"></center>
    

**SQA**(Speech Question Answering)

- **SpeechBERT**[[188](https://arxiv.org/abs/1910.11559)]
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig48.jpg" width="60%" alt="Figure 48"></center>
    

## Efficient Transformer

Transformerì—ì„œ ë†’ì€ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ëŠ” ë²”ìš©ì„±ì„ ì €í•˜ì‹œí‚´

íš¨ìœ¨ì„±ì„ ìœ„í•´ compressing, accelerating transformer model ì†Œê°œ

- Pruning
- Low-rank decomposition
- Knowledge distillation
- Network quantization
- Compact architecture design

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig49.jpg" width="60%" alt="Figure 49"></center>

### Pruning and Decomposition

- **Michel**[[45](https://arxiv.org/abs/1905.10650)]: Attentionì—ì„œ taskì™€ layerì— ë”°ë¼, í•„ìš”í•œ headì˜ ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    - ì¤‘ë³µëœ relationì„ ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸
    - ë”°ë¼ì„œ, ê° headê°€ ìµœì¢… outputì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ estim í›„ pruning
    - Headë¥¼ í•˜ë‚˜ì”© ì œê±°í•´ê°€ë©°, BLEU scoreë¥¼ ì¸¡ì •
- **BERT LTH**[[190](https://arxiv.org/abs/2005.00561)]: Bertì˜ LTHë¥¼ general redundancy, task-specific redundancyì—ì„œ ë¶„ì„
    - BERTì—ì„œë„ LTì— í•´ë‹¹í•˜ëŠ” good sub-networkê°€ ì¡´ì¬í•˜ê³ , attentionê³¼ FFN layerì—ì„œ ëª¨ë‘ ë†’ì€ compressionì´ ê°€ëŠ¥
- **Patch Slimming**[[192](https://arxiv.org/abs/2106.02852)]: Layerì—ì„œ íš¨ê³¼ê°€ ì—†ëŠ” patchë“¤ì„ ì‚­ì œí•´ë‚˜ê°€ë©° ê³„ì‚°
    - attentionì˜ ê²½ìš°, patchê°„ì˜ ì—°ê´€ì„±ì„ í•™ìŠµ
    - ì—°ê´€ì„±ì´ ì—†ëŠ” patchê°€ ìˆì„ ìˆ˜ ìˆìŒ
    - output layerë¶€í„° ì´ì „ layerì—ì„œì˜ ì—°ê´€ì„± ì˜í–¥ì„ ë³´ê³ , patchë¥¼ ì„ íƒ
- **Vision Transformer Pruning**[[193](https://arxiv.org/abs/2104.08500)]:
    1. Sparsity Regularization ($$L_1$$)
    2. Pruning dimension of Linear Projection
    3. Fine-tuning
- **LayerDrop**[[204](https://arxiv.org/abs/1909.11556)]: í›ˆë ¨ ì¤‘ì—” regularizationì´ ê°€ëŠ¥í•˜ê³ , inferenceì—ì„  pruneì´ ë¨
    - ì¦‰, training ì‹œì—, layerë¥¼ random delete í•˜ë©° í›ˆë ¨í•˜ê¸° ë•Œë¬¸ì— modelì´ pruningì— robust í•¨
    - Inference ì‹œì—, ì›í•˜ëŠ” subnetworkë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig50.jpg" width="60%" alt="Figure 50"></center>
    
- **Structured pruning**[[206](https://arxiv.org/abs/1910.04732)]: Low-rankë¡œ parameterizingí•˜ë©° í›ˆë ¨ì„ ì§„í–‰í•˜ê³ , rankê°€ ë‚®ì€ componentë“¤ì€ ì‚­ì œí•´ê°€ë©° pruning
- **[LLM-Pruner](https://arxiv.org/abs/2305.11627):** gradient informationì„ ê¸°ë°˜ìœ¼ë¡œ non-critical coupled structureë¥¼ prune

### Knowledge Distillation

Large teacher network $$\rightarrow$$ shallow architectureë¥¼ ê°€ì§„ student networkë¡œ distillation

- **XtremeDistil**[[210](https://arxiv.org/abs/2004.05686)]: Bert $$\rightarrow$$ small modelë¡œ distillation
- **Minilm**[[211](https://arxiv.org/abs/2002.10957)]: self-attention layersì˜ outputì„ studentê°€ mimic
    - teacherì™€ studentì—ì„œ $$Q\cdot K, V$$ì— ëŒ€í•´ scaled dot-productë¥¼ ì´ìš©í•˜ì—¬ distillation
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig51.jpg" width="60%" alt="Figure 51"></center>)
    

- **Jia**[[213](https://arxiv.org/abs/2107.01378)]: patch ë‹¨ì—ì„œ manifold distillationì„ í†µí•´ fine-grained distillation ë‹¬ì„±
    
    $$
    \mathcal{M}(\psi(F_S))=\psi(F_S)\psi(F_S)^T, \mathrm{\;\;where\;} \psi = \mathrm{reshape} \\
    \mathcal{L}_{mf}=\lVert\mathcal{M}(\psi(F_S)) - \mathcal{M}(\psi(F_T))\rVert^2_F
    $$
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig52.jpg" width="60%" alt="Figure 52"></center>
    

### Quantization

ê¸°ì¡´ ì–‘ìí™” ë°©ë²•ë“¤

1. [Searching for low-bit weights in quantized neural networks](https://arxiv.org/abs/2009.08695)
2. [Profit: A novel training method for sub-4-bit mobilenet models](https://arxiv.org/abs/2008.04693)
3. [Riptide: Fast end-to-end binarized neural networks](https://cowanmeg.github.io/docs/riptide-mlsys-2020.pdf)
4. [Proxquant: Quantized neural networks via proximal operators](https://arxiv.org/abs/1810.00861)

- **Compressing transformers with pruning and quantization**[[222](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A)]
- **Post-Training Quantization for Vision Transformer**[[224](https://arxiv.org/abs/2106.14156)]

### Compact Architecture Design

Distillation ì´ì™¸ì— ì§ì ‘ì ìœ¼ë¡œ compactí•œ modelì„ ì„¤ê³„

- **Lite Transformer**[[225](https://arxiv.org/abs/2004.11886)]: LSRA(Long-Short Range Attention)
    - Attentionì˜ ê° head ë§ˆë‹¤ long, shortì— ëŒ€í•œ relation modeling
- **hamburger**[[226](https://arxiv.org/abs/2109.04553)]: self-attention layer ëŒ€ì‹  matrix decomposition ì‚¬ìš©
    - token ê°„ì˜ ì¢…ì†ì„±ì„ decompositionì„ í†µí•´ ë” ëª…í™•íˆ ì•Œ ìˆ˜ ìˆê³ , compact í•¨
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig53.jpg" width="60%" alt="Figure 53"></center>
    
- NAS ë°©ë²• ì‚¬ìš©: [[ViTAS](https://arxiv.org/abs/2106.13700)], [[227](https://arxiv.org/abs/1910.14488)], [[228](https://arxiv.org/abs/1901.11117)], [[Bossnas](https://arxiv.org/abs/2103.12424)]

ì‹œê°„ ë³µì¡ë„ë¥¼ $$O(N)$$ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•

- **Transformers are RNN**[[230](https://arxiv.org/abs/2006.16236)]: Linearized Attention
    - Softmaxë¡œ ê° tokenì˜ relationì„ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— $$O(N^2)$$ì„
    - Softmax ì „ì— $$Q, K$$ì˜ similarity functionì„ ë°”ê¿€ ìˆ˜ ìˆìŒ
    
    $$
    V'_i=\frac{\sum^N_{j=1}\phi(Q_i)\phi(K_i)^TV_j}{\sum^N_{j=1}\phi(Q_i)\phi(K_i)^T} \rightarrow
    V'_i=\frac{\phi(Q_i)\sum^N_{j=1}\phi(K_i)^TV_j}{\phi(Q_i)\sum^N_{j=1}\phi(K_i)^T}
    $$
    
    - Sum ë¶€ë¶„ì€ Queryì™€ ê´€ê³„ê°€ ì—†ê¸° ë•Œë¬¸ì— ì‹œê°„ ë³µì¡ë„ $$O(N)$$ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥
    - ê³µê°„ ë³µì¡ë„ ë˜í•œ ë¶„ëª¨ì˜ ê°’ì„ ë¯¸ë¦¬ ì €ì¥í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— $$O(N)$$ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
    - Softmaxì™€ dot-productë¥¼ ëŒ€ì²´í•  í•¨ìˆ˜ëŠ” similarity function ê°’ì´ ì–‘ìˆ˜ê°€ ë˜ì–´ì•¼ í•¨
    - ì´ì— ëŒ€í•œ feature representation $$\phi$$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •ë¨
        
        $$
        \phi(x)=\mathrm{elu}(x)+1=
        \begin{cases}
        x+1, & x>0 \\
        \exp(x), & x\le0
        \end{cases}
        $$
        
    - Causal Maskingë„ triangle matrixì¼ í•„ìš”ê°€ ì—†ê³ , sumì˜ ë²”ìœ„ë¥¼ $$N$$ì—ì„œ $$i$$ë¡œ ë°”ê¾¸ë©´ ë¨
- **Big Bird**[[232](https://arxiv.org/abs/2007.14062)]: ê° tokenì„ graphì˜ vertexë¡œ ê°„ì£¼í•˜ì—¬ inner-productë¥¼ edgeë¡œ ì •ì˜
    - graph ì´ë¡ ì˜ sparse graphë¥¼ í†µí•´ transformerì˜ dense graphë¥¼ ê·¼ì‚¬í•˜ì—¬ $$O(N)$$ë‹¬ì„±
    
    <center><img src="/assets/images/cv/vit/survey/survey-vit_fig54.jpg" width="60%" alt="Figure 54"></center>
    

### ì „ì²´ Efficient ìˆœì„œ

<center><img src="/assets/images/cv/vit/survey/survey-vit_fig55.jpg" width="60%" alt="Figure 55"></center>

# CONCLUSIONS AND DISCUSSIONS

TransformerëŠ” CNNì— ë¹„í•´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ê³¼ potentialì„ ê°€ì§€ê³  ìˆìŒ

TransformerëŠ” ê´‘ë²”ìœ„í•œ Vision Taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŒ

## Challenges

Transformerì˜ Robustnessì™€ generalizationì€ challenge point ì„

CNNì— ë¹„í•˜ì—¬, transformerëŠ” inductive biasê°€ ë‚®ê¸° ë•Œë¬¸ì— ë§ì€ datasetì´ í•„ìš”í•¨

ì¦‰, datasetì˜ qualityê°€ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹¨

CNN ê°™ì´ locality iductive biasê°€ ì—†ëŠ” Transformerê°€ ì˜ ë™ì‘í•˜ëŠ” ì´ìœ ëŠ” ë³´í†µ ì§ê´€ì ìœ¼ë¡œ ë¶„ì„í•¨

1. Large scale Datasetì´ inductive biasë¥¼ ëŠ¥ê°€í•¨
2. PEë¥¼ í†µí•´, ì´ë¯¸ì§€ì—ì„œ ì¤‘ìš”í•œ position ì •ë³´ë¥¼ ìœ ì§€í•¨
3. Over-parameterization ë„ í•˜ë‚˜ì˜ ì´ìœ ê°€ ë  ìˆ˜ ìˆìŒ
    1. ì¼ë°˜ì ìœ¼ë¡œ 256x256 resolutionì˜ ê²½ìš° inputì˜ ê²½ìš°ì˜ ìˆ˜ê°€ 5ì²œë§Œê°œ ì •ë„ì„

TransformerëŠ” ê³„ì‚°ëŸ‰ì´ ë†’ìŒ

ex) ViTëŠ” inferenceì— 180ì–µ FLOPs. í•˜ì§€ë§Œ, GhostNet ë“±ì€ 6ì–µ FLOPs

ê·¸ë¦¬ê³  NLPì— ë§ê²Œ ë””ìì¸ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ê°œì„ ì´ í•„ìš”í•¨

## Future Prospects

1. Low Cost, High performanceì˜ Transformer
    1. Real-worldì— application ê°€ëŠ¥ì„±ì„ ë†’ì—¬ì¤Œ
2. NLP modelë“¤ê³¼ ë¹„ìŠ·í•˜ê²Œ multiple taskì— ëŒ€í•´ ë™ì‘í•˜ëŠ” model ê°œë°œ

# Appendix

## Self-attention for Computer Vision

Self-attentionì´ Transformerì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•¨

CNNì˜ low-resolution, long-rangeë¥¼ ë³´ëŠ” output ë‹¨ì˜ layerì™€ ë¹„ìŠ·í•œ ì—­í• ë¡œ ê°„ì£¼ ê°€ëŠ¥

ë”°ë¼ì„œ, self-attentionì— ëŒ€í•´ ì¡°ì‚¬

### Image Classification.

Classificationì—ì„œ trainable attentionì€ 2ê°œë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆìŒ

1. Hard attention
    1. local ë˜ëŠ” ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ attention
2. Soft attention
    1. SENetê³¼ ê°™ì´, channel-wise convë¥¼ í†µí•´, featureë“¤ì„ reweight

### Semantic Segmentation.

### Object Detection.

### Other Vision Tasks.