---
layout: default
title: StyleTalk
nav_order: "2023.12.10"
parent: 3D
grand_parent: Computer Vision
permalink: /docs/computer_vision/3d/styletalk_2023_12_10
---

# **StyleTalk**
{: .no_toc}
[StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles](https://arxiv.org/abs/2301.01081)

Table of contents
{: .text-delta }
1. TOC
{:toc}

## **Abstract**
ì‚¬ëŒë§ˆë‹¤ ë§í•˜ëŠ” ìŠ¤íƒ€ì¼ê³¼ í™”ë²•ì´ ë‹¤ë¥¸ë°, ê¸°ì¡´ì˜ one-shot talking head generator ë“¤ì€ ë‹¤ì–‘í•œ í™”ë²• ìŠ¤íƒ€ì¼ì— ëŒ€í•œ ìƒì„±ì´ ì–´ë ¤ì› ìŒ

ì„ì˜ì˜ reference talking vedeoì—ì„œ ë§í•˜ê¸° ìŠ¤íƒ€ì¼ì„ ì–»ì€ ë‹¤ìŒ reference ìŠ¤íƒ€ì¼ê³¼ ë‹¤ë¥¸ ì˜¤ë””ì˜¤ë¡œ ë§í•˜ê¸° ìœ„í•œ one-shot portrait <br>
style referenceì˜ ì–¼êµ´ ë™ì‘ íŒ¨í„´ ì¶”ì¶œ í›„ portraitì— style codeë¡œ ì¸ì½”ë”© (style encoder) <br>
ì´í›„, voiceì™€ style codeì—ì„œ stylized facial animationì„ í•©ì„±í•˜ëŠ” style-controllable decoder (style-aware adaptation ì‚¬ìš©) <br>
style ì ìš©ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê¸° ìœ„í•´ adjust the weight <br>

<center><img src="/assets/images/cv/3d/styletalk_fig1.jpg" width="950%" alt="Figure 1"></center>

1. style videoë¡œë¶€í„° [3DMM](#3dmm) parameter ì¶”ì¶œ í›„, style code $$s$$ë¥¼ ì–»ê¸° ìœ„í•´ encoder $$E_s$$ì— feeding
2. Audio Encoder $$E_a$$ë¥¼ í†µê³¼í•˜ì—¬ audioì— ëŒ€í•œ feature ì¶”ì¶œ
3. ë‘ featureê°€ Style-controllable Dynamic decoderì— ì…ë ¥, stylized expression parameterë¥¼ ì–»ìŒ
4. Image renderer $$E_r$$ì— stylized expression parameter $$\hat{\delta}$$ì™€ identity reference image $$I_r$$ì„ feedingí•˜ì—¬ output video ìƒì„±

ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ (b)ëŠ” style-controllable dynamic decoderì˜ ì„¸ë¶€ ì„¤ëª…

## **Introduction**
Talking headì˜ ê²½ìš°, virtual human, ë”ë¹™ ë“±ì—ì„œ í™œìš©ë¨ <br>
ì…ìˆ , ë¨¸ë¦¬ í¬ì¦ˆ, ë™ì˜ìƒì—ì„œ ë§ì€ ë°œì „ì´ ìˆì—ˆì§€ë§Œ, ë‹¤ì–‘í•œ í™”ë²•ì— ëŒ€í•œ ì˜ìƒ ìƒì„±ì€ í•˜ì§€ ëª»í•¨ <br>
$$\rightarrow$$ one shotì— ëŒ€í•˜ì—¬ (ê°™ì€ ì‚¬ëŒì´ë”ë¼ë„ ìƒí™©ì— ë”°ë¼ì„œ ë§í•  ë•Œ ìŠ¤íƒ€ì¼ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ) <br>
ë”°ë¼ì„œ, one shot reference ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ style reference imageì™€ ìŒì„±ì„ ê¸°ë°˜ìœ¼ë¡œ, talking headë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°©ë²• ì œì•ˆ

ì´ì „ ë°©ë²•ë“¤ì€ ë‹¨ìˆœíˆ ê°ì • classë§Œ ê°€ì§€ê³  ìƒì„±í•˜ê±°ë‚˜ ë‹¨ì¼ í”„ë ˆì„ì— ëŒ€í•˜ì—¬ styleì„ ì „ë‹¬ë°›ìŒ <br>
ë”°ë¼ì„œ ë§í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ ë™ì‘ íŒ¨í„´ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ styleì„ ì „ì†¡í•  ìˆ˜ ìˆëŠ” ë°©ë²• ì œì•ˆ <br>
1. style ì˜ìƒê³¼ ì˜¤ë””ì˜¤ë¡œë¶€í„° ì¸ì½”ë”©ëœ featureë¥¼ ì–»ìŒ
    1. style ì˜ìƒì˜ [3DMM](#3dmm) (3D Morphable Model)ì˜ sequenceë¡œë¶€í„° style latent code $$s$$ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•˜ì—¬ [self-attention pooling layer](https://arxiv.org/abs/2008.01077)ê°€ ìˆëŠ” transformer ê¸°ë°˜ style encoderë¥¼ ì´ìš©
    2. triplet constraintì„ ì‚¬ìš©í•˜ì—¬ unseen style clipì— ëŒ€í•´ì„œë„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨
    3. í•™ìŠµëœ style code $$s$$ê°€ semantically meaningful spaceì— ìˆìŒì„ í™•ì¸
2. style-controllable dynamic decoderì— inputìœ¼ë¡œ ë„£ì–´, 3D ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ì–»ìŒ
    1. style codeë¥¼ í†µí•´ ì¡°ê±´ë¶€ ì¼ëŒ€ì¼ mappingìœ¼ë¡œ ë°”ê¾¸ì—ˆì§€ë§Œ, í‘œì •ì˜ ë³€í™”ê°€ í° ê²½ìš°ì—ëŠ” ì—¬ì „íˆ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ë¦½ì‹±í¬ ë° ì‹œê°ì  ì•„í‹°íŒ©íŠ¸ë¥¼ ë³¼ ìˆ˜ ìˆìŒ
    2. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ style-controllable dynamic decoder ì œì•ˆ
    3. multi-head attention ë’¤ì˜ feed-forward layerê°€ [style ì œì–´ì— ì¤‘ìš”í•¨](https://arxiv.org/abs/2011.03803)
    4. ë”°ë¼ì„œ, style code $$s$$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FFì˜ kernel weightë¥¼ adaptively ìƒì„±
        1. $$s$$ë¥¼ FFì— íƒœì›Œì„œ attention ê³„ì‚°
        2. ë”°ë¼ì„œ, one-shot settingì˜ ì¼ëŒ€ë‹¤ mappingì—ì„œ ì¼ëŒ€ì¼ mappingìœ¼ë¡œ ì „í™˜
            1. ë” íš¨ê³¼ì ìœ¼ë¡œ ë¦½ì‹±í¬ë¥¼ ìƒì„±í•˜ê³  ì„¤ë“ë ¥ìˆëŠ” ì–¼êµ´ ìƒì„±
3. Image rendererë¥¼ ì´ìš©í•˜ì—¬ referenceì™€ 3D ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ rendering

{: .highlight-title }
> Contribution:
> 
> - í•˜ë‚˜ì˜ ëŒ€ìƒ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ë§í•˜ëŠ” ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” one-shot ì œì–´ ê°€ëŠ¥í•œ talking head í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆ
> - universal style extractorë¥¼ ì œì•ˆí•˜ì—¬ ë‹¤ì–‘í•œ unseen clipì— ëŒ€í•´ ì˜ ë™ì‘í•˜ë„ë¡ í•¨
> - style-controllable dynamic decoderë¥¼ ì´ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ stylizeëœ ì–¼êµ´ê³¼ ì…ìˆ  ìƒì„±

## **Proposed Method**
3ê°œì˜ inputì„ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ style-controllable talking face generate framework ì œì•ˆ <br>
1. Reference image $$I^r$$
2. Audio Clip $$A$$: speech contentë¥¼ ì œê³µí•˜ëŠ” $$T$$ ê¸¸ì´ì˜ ì˜¤ë””ì˜¤
3. Style Clip $$V$$: style reference talking video $$V = I^s_{1:N}$$ (ê¸¸ì´ $$N$$)

Outputì€ photo-realistic taking video $$Y(=\hat{I}_{1:T})$$ <br>
$$\rightarrow$$ referenceì˜ ì–¼êµ´ì„ ê°€ì§„, style clip í‘œì •ì˜ ë¹„ë””ì˜¤

ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ 4 ë¶€ë¶„ìœ¼ë¡œ ì´ë¤„ì§ <br>
1. **Audio encoder $$E_a$$**: ìŒì†Œ ë‹¨ìœ„ì˜ label $$a_{1:T}$$ë¡œë¶€í„° ë¶„ì ˆëœ featureì˜ sequence $$a'_{1:T}$$ë¥¼ ì¶”ì¶œ
2. **Style encoder $$E_s$$**: ì–¼êµ´ì˜ ë™ì‘ íŒ¨í„´ì„ compactí•œ style code $$s$$ë¡œ ì¶”ì¶œ
3. **Style-controllable dynamic decoder $$E_d$$**: audio featureì™€ style code $$s$$ë¡œë¶€í„° stylized 3DMM expression parameter $$\hat{\delta}_{1:T}$$ ìƒì„±
4. Image renderer $$E_r$$: reference imageì™€ expression parameterë¡œë¶€í„° ì‚¬ì‹¤ì ì¸ talking face ìƒì„±

í›ˆë ¨ì˜ ê²½ìš° $$\{I^r, a_{t-w,t+w},V\}$$ë¥¼ inputìœ¼ë¡œ í•˜ì—¬ [Wang](https://arxiv.org/abs/2112.02749)ì˜ ë°©ë²•ì„ ì‚¬ìš©í•¨ <br>
$$\rightarrow$$ $$w$$ëŠ” window lengthì´ê³ , 5ë¡œ ì„¤ì •ë¨

<center><img src="/assets/images/cv/3d/styletalk_fig1.jpg" width="95%" alt="Figure 1"></center>

### **Audio Encoder**
ìŒì†Œ ë‹¨ìœ„ì˜ label $$a_{1:T}$$ë¡œë¶€í„° ë¶„ì ˆëœ featureì˜ sequence $$a'_{1:T}$$ë¥¼ ì¶”ì¶œ

í•˜ì§€ë§Œ ì˜¤ë””ì˜¤ëŠ” talking-styleì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê°ì •ì´ë‚˜ ê°•ë„ê°™ì€ ì •ë³´ì™€ ë¬´ê´€í•œ ì •ë³´ë¥¼ í¬í•¨

ì´ëŸ° ì •ë³´ë¥¼ ì—†ì• ê¸° ìœ„í•˜ì—¬ [**MFCC**](#mfcc) ê°™ì€ acoustics featureê°€ ì•„ë‹Œ phoneme labelì„ ì‚¬ìš© <br>
$$\rightarrow$$ ASRë¡œ ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„, phindex.jsonì— mapping

phoneme label $$a_{t-w,t+w}$$ëŠ” word embeddingì„ í†µí•˜ì—¬ ë°”ë€Œê³ , transformer encoderë¡œ ë“¤ì–´ê°€, audio featureë¥¼ ì–»ìŒ $$a'_{t-w,t+w},\; a'_t\in \mathbb{R}^{256}$$

phonemeì€ ìŒì„± ì¸ì‹ íˆ´ì—ì„œ ì¶”ì¶œë¨

### **Style Encoder**
ì–¼êµ´ì˜ ë™ì‘ íŒ¨í„´ì„ compactí•œ style code $$s$$ë¡œ ì¶”ì¶œ

speaking styleì€ ì–¼êµ´ í‘œì • íŒ¨í„´ì´ê¸° ë•Œë¬¸ì— ì–¼êµ´ì˜ ëª¨ì–‘, í…ìŠ¤ì³, ì¡°ëª…ê³¼ëŠ” ë¬´ê´€í•¨

ì´ëŸ° ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì—†ì• ê¸° ìœ„í•´, [**3DMM**](#3dmm) ì‚¬ìš©í•˜ì—¬ style video clipì„ sequantialí•œ expression parameter $$(\delta_{1:T}\in\mathbb{R}^{N\times64})$$ë¡œ ë³€í™˜
        
ì¶”ê°€ì ìœ¼ë¡œ ì´ë¯¸ì§€ì˜ ì •ì ì¸ í‘œí˜„ë¿ë§Œ ì•„ë‹ˆë¼, ë™ì‘ íŒ¨í„´ì„ modelingí•˜ëŠ” style encoder ì„¤ê³„ <br>
Transformer encoderë¡œ êµ¬ì„±ë˜ì–´ ìˆê³ , sequential 3DMMì„ input tokenìœ¼ë¡œ ì‚¬ìš© <br>
encoderëŠ” ê° tokenì— ëŒ€í•œ style vector $$s'_{1:N}$$ë¥¼ ì¶œë ¥ <br>
$$\rightarrow$$ ëª‡ frameì„ ì…ë ¥ìœ¼ë¡œ ë„£ì–´, self-attentionìœ¼ë¡œ ì§‘ê³„í•˜ì—¬ ì¶œë ¥ <br>
$$\rightarrow$$ ì¶”ê°€ì ìœ¼ë¡œ, FFì— token levelì—ì„œ weightë¥¼ ê³„ì‚°í•˜ëŠ” ì¶”ê°€ì ì¸ attention-based mechanism ì‚¬ìš© <br>
$$\rightarrow$$ ì—¬ê¸°ì„œ ê° tokenì€ ë¹„ë””ì˜¤ì—ì„œ ê° frameì´ styleì— ê¸°ì—¬í•˜ëŠ” weight ì„ <br>
$$\rightarrow$$ ê° weightë¥¼ ë‹¤ ë”í•´ì„œ ìµœì¢… style code $$s\in\mathbb{R}^{d_s}$$ë¥¼ ì–»ìŒ

$$
ğ’”=\mathrm{softmax}â¡(W_sH)H^T,\; 
W_s\in\mathbb{R}^{1\times d_s},\;H=[s_1,\cdots,s_N]\in\mathrm{R}^{d_s\times N}
$$

### **Style-Controllable Dynamic Decoder**
**Style Decoder**
ì•ì„œ, articulation representation $$a'_{t-w,t+w}$$ì™€ style code $$s$$ë¥¼ inputìœ¼ë¡œ ë°›ì•„, vanilla transformer decoderë¥¼ ì ìš©í–ˆì—ˆìŒ

style code $$s$$ë¥¼ $$2w+1$$ë²ˆ ë°˜ë³µí•˜ì—¬, positional encodingì„ ì¶”ê°€í•œ í›„ style tokenë“¤ì„ ì–»ìŒ <br>
$$\rightarrow$$ voiceì˜ latentì™€ ê°™ì€ ì°¨ì›ì´ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë°˜ë³µí•˜ê³ , style codeëŠ” query, articulation representationëŠ” key, valueë¡œ ë“¤ì–´ê°

**Dynamic Style Decoder**
í•˜ì§€ë§Œ ìœ„ì˜ ë‹¨ìˆœí•œ decoderëŠ” í‘œì •ì˜ ì›€ì§ì„ì´ í´ ë•Œ, ì…ìˆ ê³¼ í‘œì •ì— artifactê°€ ìƒê¹€ <br>
ì•ì„  ë‹¤ë¥¸ ì—°êµ¬ì—ì„œ static kernel weightê°€ ë‹¤ì–‘í•œ styleì„ modelingí•  ìˆ˜ ì—†ë‹¤ê³  ê°€ì •í–ˆìŒ <br>
ë”°ë¼ì„œ, style codeì— ë”°ë¼ adaptiveí•˜ê²Œ style-aware adaptive transformer ì„¤ê³„

ë˜í•œ [Wang](https://arxiv.org/abs/2011.03803)ì—ì„œ FFê°€ transformer decoderì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ë°í˜”ê¸° ë•Œë¬¸ì— FFì— ì ìš© <br>
Style-aware adaptive FF layerë¥¼ ì ìš©í•˜ëŠ”ë°, 8ê°œì˜ parallelí•œ layer set weight$$(\tilde{W}_k, \tilde{b}_k)$$ ì‚¬ìš© <br>
$$\rightarrow$$ Parallel weightëŠ” ê°ê° ë‹¤ë¥¸ talking styleì„ ëšœë ·í•˜ê²Œ modelingí•˜ê²Œ í•™ìŠµë  ê²ƒì´ë¼ ê¸°ëŒ€ë¨ <br>
$$\rightarrow$$ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´, softmaxë¥¼ ì ìš©í•œ í›„, weight sum

$$
\tilde{W}(s)=\sum^K_{k=1}\pi_k(s)\tilde{W}_k,\;\;\tilde{b}(s)=\sum^K_{k=1}\pi_k(s)\tilde{b}_k,\\\mathrm{s.t.}\;0\leq\pi_k(s)\leq1,\;\;\sum^K_{k=1}\pi_k(s=1),
$$

ë”°ë¼ì„œ, ìµœì¢…ì ì¸ FF layerëŠ”

$$
y=g(\tilde{W}^T(s)x+\tilde{b}(s)),\;\;g=\mathrm{act\_func}
$$

### **Disentanglement of Upper and Lower faces**
Upper face (ëˆˆ, ëˆˆì¹)ê³¼ lower face (ì…ìˆ )ì´ ë‹¤ë¥¸ ì›€ì§ì„ íŒ¨í„´ì´ ìˆë‹¤ëŠ” ê²ƒì´ ê´€ì°°ë¨ <br>
$$\rightarrow$$ ë³„ë„ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•˜ì˜€ìŒ

ë”°ë¼ì„œ, expression paramì„ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ , upper face decoderì™€ lower face decoder, ë‘ ê°œë¡œ parallelí•˜ê²Œ style-controllable dynamic decodersë¥¼ êµ¬ì„± <br>
$$\rightarrow$$ 64ê°œ ì¤‘, 13ê°œê°€ mouthì— í•´ë‹¹í•˜ì—¬ lower, ë‚˜ë¨¸ì§€ê°€ upper <br>
$$\rightarrow$$ 13ê°œì¸ ì´ìœ ëŠ” [supp.](#supplementary) <br>
ìµœì¢…ì ìœ¼ë¡ , concatenatedë˜ì–´ outputë¨

## **Objective Function Design**
outputì€ í•œ frameì„ ë½‘ì•„ë‚´ê¸° ë•Œë¬¸ì—, sequenceì— ëŒ€í•œ batchë¥¼ í•™ìŠµí•¨ <br>
í•œë²ˆì— 64 frameì„ ë½‘ìŒ <br>

64 frameì— 3ê°œì˜ Discriminatorë¥¼ ì ìš© <br>
1. $$D_{tem}$$: temporal discriminator <br>
2. $$D_{sync}$$: vertex-based lip-sync discriminator <br>
3. $$D_{style}$$: style discriminator

ë˜í•œ semantically meaningful style spaceë¥¼ ì–»ê¸° ìœ„í•´, triplet constraintë¥¼ ì‚¬ìš©

### **Lip-sync Discriminator** 
ì…ìˆ ì€ ë§í•˜ëŠ” ì™€ì¤‘ì— ë§ì´ ì›€ì§ì„. ë”°ë¼ì„œ syncê°€ ì¤‘ìš”í•¨ <br>
ë”°ë¼ì„œ $$D_{sync}$$ ì„¤ê³„ <br>
- audioì˜ windowë¥¼ random sampling
- videoì˜ windowë¥¼ random sampling
- ë‘ windowê°€ sync, async ëœ ê²ƒì— ëŒ€í•´ disc ì ìš©

3DMMì—ì„œ mouth-relatedëŠ” ì–¼êµ´ ì›€ì§ì„ë„ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, ì…ìˆ ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•´ PCA baseë¡œ expression parameterë¥¼ face meshë¡œ ë°”ê¿”ì„œ ì… verticeë¥¼ ì„ íƒ

SyncNetì— mesh vertex coordinateì™€ phonemeë¥¼ ê°ê° inputìœ¼ë¡œ ë„£ìŒ

mouthì™€ phonemeì˜ windowì— ëŒ€í•œ emb$$(e_m,e_a)$$ë¥¼ ë½‘ê¸° ìœ„í•´ pretrained [PointNet](https://arxiv.org/abs/1612.00593)ì„ ì‚¬ìš©

ë‘ embì— ëŒ€í•´ cosine similarityë¥¼ ì ìš©

$$
P_{sync}=\frac{e_m\cdot e_a}{\max(\lVert e_m\rVert_2\cdot\lVert e_a\rVert_2,\epsilon)},
$$

{: .highlight-title}
> $$
> L_{sync}=\frac{1}{L}\sum^L_{i=1}-\log(P^i_{sync})
> $$

### **Style Discriminator** 
ì „ì²´ styleì— ëŒ€í•œ loss <br>
sequential 3DMM expression parameters $$\delta_{1:L}$$ë¥¼ inputìœ¼ë¡œ frozen PatchGAN (CE loss)

{: .highlight-title}
> $$
> L_{style}=-\log(P^s_i),\;\; \mathrm{s=speaking\;style}
> $$

### **Temporal Discriminator**
í˜„ì‹¤ì ì´ì§€ ì•Šì€ ê²ƒì„ êµ¬ë³„ <br>
sequential 3DMM expression parameters $$\delta_{1:L}$$ë¥¼ inputìœ¼ë¡œ PatchGAN (GAN hinge loss)

### **Triplet Constraint**
ìœ ì‚¬í•œ style codeëŠ” ë¹„ìŠ·í•œ spaceì— ëª¨ì—¬ìˆì–´ì•¼ í•¨ (ì§ê´€ì ìœ¼ë¡œ) <br>
ë”°ë¼ì„œ, style code $$s$$ì— triplet contraint <br>
$$c$$ì˜ styleì„ ê°€ì§„ clipì´ ìˆì„ ë•Œ, ë‹¤ë¥¸ styleì„ ê°€ì§„ clip ë‘ê°œë¥¼ samplingí•˜ì—¬ loss ì ìš©

{: .highlight-title}
> $$
> L_{trip}=\max\{\lVert s_c-s^p_c\rVert_2-\lVert s_c-s^n_c\rVert_2+\gamma,0\},\;\; \gamma=5 (\mathrm{margin\; parameter})
> $$

### **Total Loss**
Facial expressionì˜ reconstructionì˜ ê²½ìš° L1ê³¼ SSIM ì ìš©

{: .highlight-title}
> $$
> L_{trip}=\max\{\lVert s_c-s^p_c\rVert_2-\lVert s_c-s^n_c\rVert_2+\gamma,0\},\;\; \gamma=5 (\mathrm{margin\; parameter})
> $$

{: .highlight-title}
> ìµœì¢… loss:
> 
> $$
> L=\lambda_{rec}L_{rec}+\lambda_{trip}L_{trip}+\lambda_{sync}L_{sync}+\lambda_{tem}L_{tem}+\lambda_{style}L_{style},\\
> \lambda_{rec}=88,\;\lambda_{trip}=1,\;\lambda_{sync}=1,\;\lambda_{tem}=1,\;\lambda_{style}=1
> $$

## **Experiments**
### **Dataset**
{: .no_toc }
ì—¬ëŸ¬ ë§í•˜ê¸° ìŠ¤íƒ€ì¼ì´ í¬í•¨ëœ

1. [MEAD](https://wywu.github.io/projects/MEAD/MEAD.html): 60ëª…ì˜ í™”ìê°€ 8ê°€ì§€ ê°ì •ì˜ ì„¸ ê°€ì§€ ê°•ë„ ìˆ˜ì¤€ìœ¼ë¡œ ë§í•˜ëŠ” ì–¼êµ´ ì½”í¼ìŠ¤
    1. ë™ì¼í•œ ê°•ë„ ìˆ˜ì¤€ê³¼ ë™ì¼í•œ ê°ì •ì¼ ë•Œ, ë™ì¼í•œ speaking styleì´ë¼ê³  ê°€ì •
2. [HDTF](https://github.com/MRzzm/HDTF): In-the-wild audio-visual dataset
    1. í•œ í™”ìì˜ ë¹„ë””ì˜¤ í´ë¦½ì´ ë™ì¼í•œ speaking styleì´ë¼ê³  ê°€ì •

Training setì—ì„œ 1104ê°œì˜ speaking styleì„ ì–»ìŒ <br>
$$\rightarrow$$ 256Ã—256ë¡œ crop, resizedë˜ê³  30FPSë¡œ sampling

### **Implementation Details**
{: .no_toc }
Pytorch, Adam <br>
$$E_r$$ì€ VoxCeleb, MEAD, HDTFë¥¼ ì¡°í•©í•˜ì—¬ í›ˆë ¨ë¨ <br>
$$D_{sync}, D_{style}$$ëŠ” HDTF, MEADë¡œ 12ì‹œê°„ (RTX 3090 GPU 4ê°œ, lr: 0.0001) <br>
ì•ì„œ í›ˆë ¨ëœ ê²ƒë“¤ì€ fronzen <br>
ì´í›„, $$E_a, E_s, E_d, D_{tem}$$ì€ HDTF, MEADë¡œ 4ì‹œê°„ (RTX 3090 GPU 2ê°œ, lr: 0.0001) <br>

### **Quantitative Evaluation**
{: .no_toc }
ì…ìˆ : SyncNetì˜ confidence score $$\mathrm{Sync_{conf}}$$ì™€ M-LMD(Landmark Distance on the Mouth) ì‚¬ìš© <br>
í‘œì •: F-LMD(Landmark Distance on the whole face) ì‚¬ìš© <br>
ë¹„ë””ì˜¤ í’ˆì§ˆ: SSIM, CPBD(Cumulative Probability of Blur Detection)

<center><img src="/assets/images/cv/3d/styletalk_fig4.jpg" width="90%" alt="Figure 4"></center>
$$\rightarrow$$ unseenì— ëŒ€í•´ ìˆ˜í–‰

### **Qualitative Evaluation**
{: .no_toc }
EAMM, GC-AVTë§Œ ìŠ¤íƒ€ì¼ control ê°€ëŠ¥í•˜ê³  ê½¤ë‚˜ ì •í™•í•˜ì§€ë§Œ upper faceë§Œ control ê°€ëŠ¥í–ˆìŒ <br>
ë˜í•œ ë°°ê²½ë„ ë¶€ì¡±í•˜ê³ , ìŠ¤íƒ€ì¼ì˜ ì¼ì¹˜ì„±ì´ ë¶€ì¡±í•¨ <br>
Wav2Lip, AVCT, PC-AVS, GC-AVTëŠ” ì…ìˆ  ìƒì„±ì„ ì˜í•˜ì§€ë§Œ, í•˜ë‚˜ì˜ í™”ë²•ë§Œ ê°€ëŠ¥ <br>
ì œì•ˆëœ ë°©ë²•ì€ ì •í™•í•œ lip-sync, identity ë³´ì¡´ ë° ì¢‹ì€ ë°°ê²½ ìƒì„±ì„ í•  ìˆ˜ ìˆìŒ

<center><img src="/assets/images/cv/3d/styletalk_fig5.jpg" width="90%" alt="Figure 5"></center>

### **Ablation Study**
{: .no_toc }
MEAD datasetì—ì„œ 6ê°œì˜ ë³€í˜•

1. K=4, 16 $$\rightarrow$$ 8ì¼ ë•Œê°€ ì¢‹ì•˜ìŒ
2. $$D_{style}$$, triplet loss $$\rightarrow$$ landmarkì— íš¨ê³¼ì 
3. $$D_{sync}$$ ì—†ì• ê¸° $$\rightarrow$$ ì…ìˆ ì— íš¨ê³¼ì 

<center><img src="/assets/images/cv/3d/styletalk_fig6.jpg" width="50%" alt="Figure 6"></center>
<center><img src="/assets/images/cv/3d/styletalk_fig7.jpg" width="75%" alt="Figure 7"></center>

### **Style Space Inspection**
{: .no_toc }
**Style Space Visualization** <br>
t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ style code $$s$$ë¥¼ 2Dì— project

{: .note-title}
> **t-SNEëŠ”?**
> 
> ë¶ˆí•„ìš”í•œ featureë¥¼ ì—†ì• , vectorê°„ ê´€ê³„ì„± íŒŒì•… <br>
> ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œì‹œì¼œ ì‹œê°í™” <br>
> ë†’ì€ ì°¨ì› ê³µê°„ì—ì„œ ë¹„ìŠ·í•œ ë°ì´í„° êµ¬ì¡°ëŠ” ë‚®ì€ ì°¨ì› ê³µê°„ì—ì„œ ê°€ê¹ê²Œ ëŒ€ì‘
> 
> 1. ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ ìŒì— ëŒ€í•˜ì—¬ ìœ ì‚¬ë„ ì¸¡ì • <br>
> 2. low-dimì— ë¬´ì‘ìœ„ ë°°ì¹˜í•˜ì—¬ ìœ ì‚¬ë„ ì¸¡ì • <br>
> 3. 2ì˜ ìœ ì‚¬ë„ê°€ 1ê³¼ ë¹„ìŠ·í•´ì§€ë„ë¡ projection ê°±ì‹  <br>
> 4. ìˆ˜ë ´í•  ë•Œê¹Œì§€ 3 ë°˜ë³µ
> 
> ì¦‰, KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ (ë¹„ì„ í˜•ì ) <br>
> PCAëŠ” ê³µë¶„ì‚°ì—ì„œ eigen vector ê³„ì‚°, ì¤‘ìš”í•œ principal componentë¥¼ ë½‘ìŒ

$$\rightarrow$$ speaker 4ëª…ì—ì„œ ì¶”ì¶œ
<center><img src="/assets/images/cv/3d/styletalk_fig8.jpg" width="80%" alt="Figure 8"></center>
- (a) speaker ë‚´ë¶€ì˜ í™”ë²•ì´ ê°™ì€ ê°ì •ì˜ ë‹¤ë¥¸ speakerë³´ë‹¤ ê°€ê¹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
- (b) ê°™ì€ ê°ì •ì´ ì˜ clusteringë¨
    - ì¶”ê°€ì ìœ¼ë¡œ ìœ ì‚¬í•œ ê°ì •ì´ ë¹„ìŠ·í•˜ê²Œ clusterë˜ê¸°ë„ í•¨ (angry, disgusted)
- ë”°ë¼ì„œ, semantically learn í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ

**Style Manipulation** <br>
ìœ„ì˜ semantically meaningful style code ë•ë¶„ì— manipulate ê°€ëŠ¥
<center><img src="/assets/images/cv/3d/styletalk_fig9.jpg" width="90%" alt="Figure 9"></center>

- Interpolate in Style Code $$s$$

## **Conlcusion**
- ë‹¤ì–‘í•œ styleì„ ê°€ì§„ one-shot ì˜¤ë””ì˜¤ ê¸°ë°˜ talking-headë¥¼ ìƒì„± frameworkì¸ StyleTalk ì œì•ˆ
- Style ref videoì—ì„œ speaking style ì¶”ì¶œ í›„ ëŒ€ìƒì— ì£¼ì…
- ì‹œê³µê°„ì ìœ¼ë¡œ styleì„ capture
- ì •í™•í•œ lip-syncì™€ ë” ë‚˜ì€ identity ë³´ì¡´ ì„±ëŠ¥
- Condition speaking styleê³¼ ê°™ì€ ì‚¬ì‹¤ì ì¸ talking-head video ìƒì„±

## **Limitation**
1. ê·¹ë‹¨ì ì¸ head ë°©í–¥ì´ë‚˜ ì¸¡ë©´ referê°€ ìˆëŠ” style videoì—ì„œ í•©ë¦¬ì ì¸ speaking styleì„ ì–»ì§€ ëª»í•¨
2. ë¦¬ë“¬ ê°™ì€ ì¡°ìŒê³¼ ë¬´ê´€í•œ ì •ë³´ë¥¼ ì™„ì „íˆ ì œê±°í–ˆê¸° ë•Œë¬¸ì— output videoì˜ í‘œì •ì˜ ë¦¬ë“¬ì´ audioì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ
3. í–¥í›„ ì‘ì—…ì—ì„œëŠ” ì´ëŸ¬í•œ ì •ë³´ë¥¼ ì˜¤ë””ì˜¤ì—ì„œ ë¶„ë¦¬í•˜ê³  ì´ ì •ë³´ë¥¼ í”„ë ˆì„ ì‘ì—…ì— ì£¼ì… ì˜ˆì •

## **Supplementary**
### **MFCC**
{: .note-title}
> MFCCë€?
> 
> **Mel Spectrum**(ë©œ ìŠ¤í™íŠ¸ëŸ¼)ì—ì„œÂ **Cepstral**(ì¼‘ìŠ¤íŠ¸ëŸ´)Â ë¶„ì„ì„ í†µí•´ ì¶”ì¶œëœ ê°’ <br>
> ì†Œë¦¬ì˜ ê³ ìœ í•œÂ **íŠ¹ì§•**ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜ <br>
> í™”ì ë¶„ë¥˜, ìŒì•… ì¥ë¥´ ë¶„ë¥˜ ë“±ì— ì“°ì¼ ìˆ˜ ìˆìŒ
> 
> **MFCCì˜ ì¶”ì¶œ ê³¼ì •**
> <center><img src="/assets/images/cv/3d/styletalk_fig3.jpg" width="70%" alt="Figure 3"></center>
> 
> 1. **ì˜¤ë””ì˜¤ ì‹ í˜¸**ë¥¼ í”„ë ˆì„ë³„(ë³´í†µ 20ms - 40ms)ë¡œ ë‚˜ëˆ„ì–´Â **FFT**ë¥¼ ì ìš©í•´Â **Spectrum**ì„ êµ¬í•¨ <br>
> 2. **Spectrum**ì—Â **Mel Filter Bank**ë¥¼ ì ìš©í•´Â **Mel Spectrum**ì„ êµ¬í•¨ <br>
> 3. **Mel Spectrum**ì—Â **Cepstral**Â ë¶„ì„ì„ ì ìš©í•´Â **MFCC**ë¥¼ êµ¬í•¨ <br>
> 
> ìŠ¤í™íŠ¸ëŸ¼ì˜ ê°œë…ìœ¼ë¡œëŠ” ë‹¤ìŒì´ ìˆìŒ
>
> {: .note-title}
> > Spectrum (ìŠ¤í™íŠ¸ëŸ¼)
> >   
> > Time domain (ì‹œê°„ x ì¶•, ì‹ í˜¸ì˜ ì§„í­ y ì¶•)ì—ì„œ FFTë¥¼ ì´ìš©í•˜ì—¬ frequency domainìœ¼ë¡œ ë³€í™˜ <br>
> > DFTëŠ” discrete signalì— ëŒ€í•´ ë‚´ì  sumì„ í•´ê°€ëŠ” ê²ƒ
> > 
> > $$
> > DFT: X_k=\sum^{N-1}_{n=0}x_ne^{-\frac{2\pi i}{N}kn},\;\;\;\; k=0,\cdots,N-1\\
> > IDFT: x_n=\frac{1}{N}\sum^{N-1}_{k=0}X_ke^{\frac{2\pi i}{N}kn},\;\;\;\; n=0,\cdots,N-1
> > $$
>
> {: .note-title}
> > Cepstrum (ì¼‘ìŠ¤íŠ¸ëŸ¼)
> >     
> > Spectrumì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•
> > 
> > ìŒì„±ì´ë‚˜ ì•…ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í™”ìŒ(**ë°°ìŒ**)ì„ ê°€ì§€ê³  ìˆìŒ <br>
> > ë°°ìŒ êµ¬ì¡°ì˜Â ì°¨ì´ê°€Â ìŒìƒ‰ì˜ ì°¨ì´ë¥¼ ë§Œë“¬ <br>
> > ë”°ë¼ì„œ, **Spectrum**ì—ì„œÂ **ë°°ìŒ**Â êµ¬ì¡°ë¥¼ ìœ ì¶”í•´ë‚¼ ìˆ˜ ìˆë‹¤ë©´ ì†Œë¦¬ì˜ ê³ ìœ Â íŠ¹ì§•ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŒ
> > 
> > **Cepstral Analysis**
> > 
> > ì£¼íŒŒìˆ˜ì—ì„œ peakì ë“¤ì„ **í¬ë¨¼íŠ¸**(Formants)ë¼ê³  í•¨ <br>
> > $$\rightarrow$$ ë°°ìŒ(harmonics)ê³¼ ë§Œë‚˜ ì†Œë¦¬ë¥¼ í’ì„±í•˜ê²Œ í˜¹ì€ ì„ ëª…í•˜ê²Œ ë§Œë“œëŠ”Â í•„í„°Â ì—­í• ì„ í•¨ <br>
> > **í¬ë¨¼íŠ¸ë“¤ì„ ì—°ê²°í•œ ê³¡ì„ (Spectral Envelope)**ê³¼Â **Spectrum**ì„Â **ë¶„ë¦¬**í•´ë‚´ë©´ ë¨
> > 
> > <center><img src="/assets/images/cv/3d/styletalk_fig2.jpg" width="70%" alt="Figure 2"></center>
>
> {: .note-title}
> > Mel Spectrum (ë©œ ìŠ¤í™íŠ¸ëŸ¼)
> >     
> > ì‚¬ëŒì˜ ì²­ê°ê¸°ê´€ì€ ì €ì£¼íŒŒìˆ˜ì— ë” ë¯¼ê°í•¨
> > ë”°ë¼ì„œ **ì‹¤ì œ ì‚¬ëŒì´ ì¸ì‹í•˜ëŠ” ì£¼íŒŒìˆ˜**ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ì—¬ í‘œí˜„í•œ ê²ƒì´ **Mel Scale**
> >     
> > **Mel Spectrum**ì€ **Mel Scale**ì— ê¸°ë°˜í•œÂ **Filter Bank**ë¥¼Â **Spectrum**ì— ì ìš©í•˜ì—¬ ë„ì¶œí•´ë‚¸ ê²ƒ


### **3DMM**
[3DMM](https://www.face-rec.org/algorithms/3D_Morph/morphmod2.pdf)ì—ì„œ face shape $$S$$ëŠ” affine modelì„ í†µí•˜ì—¬

$$
S=S(\delta,\phi)=\bar{S}+B_{exp}\delta+B_{id}\phi
$$

$$\bar{S}$$ëŠ” í‰ê·  ì–¼êµ´ ëª¨ì–‘, $$B_{id},B_{exp}$$ëŠ” PCA ê¸°ë°˜ identity, expression <br>
$$\delta\in\mathbb{R}^{64},\phi\in\mathbb{R}^{80}$$ëŠ” ê°ê°, íŠ¹ì • 3D faceì— ëŒ€í•œ coefficient vectors

{: .note-title}
> 3DMM
> 
> 3D Morphable Model <br>
> - faceì˜ íŠ¹ì§•ì„ PCAì—ì„œ eigenvalueë¥¼ í†µí•´, í‰ê· ì ì¸ ê²ƒë¶€í„° ì–»ë“¯ì´ 3D shapeì— weight sumì„ í•˜ë“¯ì´ íŠ¹ì§•ì ì¸ shapeë¥¼ ì–»ìŒ <br>
> - shapeëŠ” íŠ¹ì§•ì  vortexì˜ 3ì°¨ì›ì˜ xyz ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ì§ˆ ìˆ˜ ìˆìŒ <br>
> - textureì˜ ê²½ìš° ìœ„ì˜ vortex ìœ„ì¹˜ì—ì„œì˜ rgb colorë¡œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ
> 
> ë…¼ë¬¸ì—ì„ , 257 ì°¨ì›ì˜ coefficient ì¤‘, 80~144ë¥¼ expressionìœ¼ë¡œ ê°€ì§„ 3DMMì„ ì‚¬ìš©
> 
> 1. **í˜•íƒœ ëª¨ë¸ (Shape Model)**:  <br>
>     ì–¼êµ´ì˜ ê¸°í•˜í•™ì  ëª¨ì–‘ì„ ì„¤ëª… <br>
>     ì–¼êµ´ì˜ ì£¼ìš” íŠ¹ì§•ì„ í‘œí˜„í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³ , ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì„ í™œìš©
>     
> 2. **ì§ˆê° ëª¨ë¸ (Texture Model)**:  <br>
>     ì–¼êµ´ì˜ í”¼ë¶€ í‘œë©´ì˜ ì§ˆê°ì„ ì„¤ëª… <br>
>     ì–¼êµ´ì˜ ìƒ‰ìƒ, ì£¼ê·¼ê¹¨, ì£¼ë¦„ ë“±ì„ í¬í•¨ <br>
>     ì£¼ë¡œ í…ìŠ¤ì²˜ ë§µ(texture map) ë˜ëŠ” í…ìŠ¤ì²˜ ê³µê°„(texture space) ê´€ë ¨ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„
>     
> 3. **ì¡°ëª… ëª¨ë¸ (Lighting Model):** <br>
>     ì¡°ëª… ì¡°ê±´ì— ë”°ë¥¸ ì–¼êµ´ì˜ ì™¸ê´€ì„ ëª¨ë¸ë§ <br>
>     ì–¼êµ´ì— ë¹›ì´ ì–´ë–»ê²Œ ë°˜ì‚¬ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ê³ , ì´ë¥¼ í†µí•´ ì–¼êµ´ì˜ ì…ì²´ê°ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
>     
> 4. **í‘œì • ëª¨ë¸ (Expression Model)**:  <br>
>     ì–¼êµ´ í‘œì •ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ í¬í•¨ <br>
>     ì–¼êµ´ì˜ ë‹¤ì–‘í•œ í‘œì •(í–‰ë³µ, í™”ë‚¨, ìŠ¬í”” ë“±)ì„ í‘œí˜„í•˜ëŠ” ë° ì‚¬ìš©

### **Input to Lip Sync Discriminator**
ìœ„ì—ì„œ identityëŠ” ì œì™¸í•˜ì—¬ ì ìš©í•œ, face meshì—ì„œ ì… ì˜ì—­ì˜ ì •ì  ì¢Œí‘œë¥¼ ì„ íƒ

$$
M=\bar{S}+B_{exp}\delta
$$

ì´í›„, PointNetì„ ì´ìš©í•˜ì—¬, mouthì—ì„œ embedding
<center><img src="/assets/images/cv/3d/styletalk_fig10.jpg" width="70%" alt="Figure 10"></center>

### **Implementation Details**
{: .no_toc }
<center><img src="/assets/images/cv/3d/styletalk_fig11.jpg" width="90%" alt="Figure 11"></center>

### **Audio Encoder**
{: .no_toc }
phonemeì€ 128ë¡œ convert í›„ 256ìœ¼ë¡œ projection (Linear layer) <br>
sequence=11

$$
a'_{t-w:t+w}\in\mathbb{R}^{11\times256}
$$

### **Style Encoder**
{: .no_toc }
64~256 (N) frameì˜ ë¹„ë””ì˜¤ë¥¼ ì–»ì–´, linearë¡œ 256 dimìœ¼ë¡œ ë³€í™˜ <br>
Audio encoderì™€ ê°™ì€ í˜•ì‹ì„ inputìœ¼ë¡œ í•˜ì—¬ style code $$s\in\mathbb{R}^{256}$$ì¶”ì¶œ

## **Code**
### **Overview**
{: .no_toc }
3DMMê³¼ phenomì€ ë‹¤ë¥¸ modelë¡œë¶€í„° ì´ë¯¸ ì¤€ë¹„ë˜ì–´ì•¼ í•¨

ê°ê°, encoderì— ë“¤ì–´ê°€ì„œ outputì„ ì–»ì€ í›„, ë‘˜ì„ decoderì— ë„£ìŒ

decoderì˜ outputì¸ expression paramì€ head poseì™€ concateë˜ì–´, rendererì— ê°™ì´ ë“¤ì–´ê°

expressionì€ frame window ë§Œí¼ stackë˜ê³ , stackëœ elemë“¤ì€ split_sizeë§Œí¼ ë¬¶ì—¬ rendererì— ë“¤ì–´ê°

$$\rightarrow$$ split_sizeë¥¼ ìµœëŒ€ frame windowë¼ê³  ìƒê°í•´ë„ ë ë“¯ (ì¦‰, (frame window, â€¦)ë¡œ inputì´ êµ¬ì„±)

rendererì—ì„œëŠ” src_imgì™€ ìœ„ì˜ expressionì´ ê°™ì´ ë“¤ì–´ê°€ì„œ image ìƒì„±


## **Related Work**
### **Audio-Driven Talking Head Generation**
{: .no_toc }
ì˜¤ë””ì˜¤ ê¸°ë°˜ talking head ë°©ë²•ì€ person-specificê³¼ person-agnostic methodë¡œ ë‚˜ë‰¨ <br>
**Person-specific**ì€ í›ˆë ¨ëœ speakerì— ëŒ€í•´ì„œë§Œ ì‘ë™í•¨ <br>
ì¼ë°˜ì ìœ¼ë¡œ 3D ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ë¨¼ì € ì œì‘í•œ ë‹¤ìŒ realistic talking videoë¥¼ í•©ì„± <br>
ëª‡ëª‡ ë°©ë²•ë“¤ì€ high-fidelity talking headë¥¼ ìœ„í•´ Nerfë¥¼ ì‚¬ìš©  <br>
**Person-agnostic**ì€ one-shotì—ì„œ talking-head video ìƒì„±ì„ ëª©í‘œë¡œ í•¨ <br>
ì´ˆê¸°ì—ëŠ” speechì— ë§ëŠ” lip ìƒì„±ì— ì´ˆì ì„ ë§ì¶¤ <br>
ì´í›„ ì–¼êµ´ í‘œì •ê³¼ ë¨¸ë¦¬ í¬ì¦ˆë¥¼ ê³ ë ¤í•œ ë°©ë²•ì´ ë‚˜ì˜´ <br>
í•˜ì§€ë§Œ í‘œí˜„ë ¥ì´ í’ë¶€í•œ stylized talking-head videoë¥¼ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŒ 

### **Stylized Talking Head Generation**
{: .no_toc }
talking-headì— ì–¼êµ´ í‘œì •ì„ ê³ ë ¤í•œ ë°©ë²•ë“¤ <br>
Ji(2021)ëŠ” audioì—ì„œ contentì™€ ê°ì •ì„ extractí•˜ì—¬ ì˜ˆì¸¡í•œ landmarkë¡œ ìƒì„±ë˜ë„ë¡ guide <br>
$$\rightarrow$$ í•˜ì§€ë§Œ ì˜¤ë””ì˜¤ë§Œ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì— ì •í™•ì„±ì´ ëª¨í˜¸í•˜ê³  ì ìš© ê°€ëŠ¥ì„±ì„ ì œí•œ <br>
Wang(2020)ê³¼ Sinha(2022)ëŠ” emotion labelì„ inputìœ¼ë¡œ ì‚¬ìš© <br>
Ji(2022)ì™€ Liang(2022)ì€ frame ë³„ë¡œ í™”ìì˜ í‘œì •ì„ inputìœ¼ë¡œ ì „ë‹¬ <br>
ì¦‰, ì´ì „ ë°©ë²•ë“¤ì€ ì–¼êµ´ í‘œì •ì˜ temporal, spatial featureë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•¨


## **Reference**
1. [StyleTalk Github](https://github.com/FuxiVirtualHuman/styletalk)
