---
layout: default
title: HIPPO
nav_order: "2024.02.11"
parent: Mamba
grand_parent: Paper Review
permalink: /docs/paper_review/mamba/hippo_2024_02_11
math: katex
---

# **HIPPO**
{: .no_toc}
[HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf)

Table of contents
{: .text-delta }
1. TOC
{:toc}

## **OverView** <br>
- ê¸°ì¡´ RNN ëª¨ë¸ë“¤ì€ N-dimensionì— historyë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ Gate êµ¬ì¡°, attentionê³¼ ê°™ì€ ì§ê´€ì  ì„¤ê³„ë¥¼ í•´ì™”ìŒ
- HiPPOëŠ” ì‹œê°„ì— ë”°ë¼ ì €ì¥í•´ì•¼ í•˜ëŠ” memoryë¥¼ ğ‘“(ğ‘¡)ë¡œ ê°€ì •í•˜ê³  ì´ë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
- $$f(t)$$ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ OPì™€ ì´ì— ëŒ€í•œ coefficient $$c_n$$ë¥¼ êµ¬í•˜ê³  N-dimensionì´ ë†’ì•„ì§ˆìˆ˜ë¡ ê·¼ì‚¬ë¥¼ ì˜í•  ìˆ˜ ìˆìŒ
- OPì˜ ì í™”ì‹ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¨ (ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ë˜ì–´ ìˆìŒ)
- ì‹œê°„ ğ‘¡ì— ë”°ë¼ ğ‘ğ‘›ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê³¼ì œì¸ë°, self-similar relationìœ¼ë¡œ $$\frac{t}{dt}c_n(t)$$ì´ ë‚˜íƒ€ë‚¨
- ë”°ë¼ì„œ, compute efficiencyí•˜ê³  time scale robustí•˜ê²Œ $$c_n$$ì„ êµ¬í•  ìˆ˜ ìˆìŒ
- ODE Discretizationì„ í†µí•´ continuousí•œ $$c_n(t)$$ë„  discrete-time linear recurrenceë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
- ì—¬ëŸ¬ OPë“¤ì„ HiPPO Frameworkë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ì˜ LMU, Gate RNN ë“±ì€ ì´ëŸ° HiPPO Framework ì¤‘ í•˜ë‚˜ë¼ëŠ” ê²ƒì„ ë³´ì„
- ì¦‰, Gate RNN(Input Gate, Forgetting Gate)ê³¼ê°™ì´ memory êµ¬ì¡°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì„¤ê³„í•˜ë˜ ê²ƒì„ HiPPOëŠ” Matrix $$A, B$$ì™€ ê°™ì€í¼ìœ¼ë¡œ ì œì‹œí•¨
- HiPPO-LegSì™€ ê°™ì€ Modelì„ ì œì‹œí•˜ë©° Sota ì„±ëŠ¥ì„ ë‹¬ì„±í•¨
  1. Time-invariant
  2. Compute Efficiency
  3. Gradient, Approximation Error Bounded

## **Intorudction**
Language modeling, speech recognition, video processingì™€ ê°™ì€ sequential dataì˜ modeling ë° í•™ìŠµì€ long-termì— ëŒ€í•œ ì´ì „ stepì˜ memoryê°€ ê°€ì¥ ì¤‘ìš”í•œ ì¸¡ë©´ì´ë‹¤. (Online updateì— ëŒ€í•´ bounded storageë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ history representationì„ í•™ìŠµ) <br>

RNNê³¼ ê°™ì€ memory unitì„ í†µí•´ ì „ì²´ historyì— ëŒ€í•œ informationì„ ì €ì¥í•˜ëŠ” ë°©ë²•ì€ vanishing gradient ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. LSTMê³¼ GRUê°™ì€ Gate ê¸°ë°˜ ë°©ì‹ì´ë‚˜ LMU, Fourier Recurrent Unitê³¼ ê°™ì€ higher-order frequency ê¸°ë°˜ ë°©ë²•ë„ unified understanding of memoryëŠ” ë„ì „ì ì¸ ê³¼ì œë‹¤.
ë˜í•œ ê¸°ì¡´ì—ëŠ” ì „ì²´ ê¸¸ì´ë‚˜ ì‹œê°„ëŒ€ì— ëŒ€í•œ ì‚¬ì „ ì œí•œì´ ìˆê³  distribution shiftê°€ ë°œìƒí•˜ëŠ” ìƒí™©ì—ì„  ë¬¸ì œê°€ ë  ìˆ˜ ìˆì—ˆë‹¤. <br>
ë”°ë¼ì„œ, ì €ìëŠ” 
1. time scaleì— ëŒ€í•œ priorì™€ lengthì— ëŒ€í•œ dependencyê°€ ì—†ê³  
2. memory mechanismì— ëŒ€í•´ ì´ë¡ ì  ì´í•´ (Gradient Boundì— ëŒ€í•œ ë¶„ì„)
3. ê¸°ì¡´ ë°©ë²•ì— ëŒ€í•´ unified method <br>

ë¥¼ ì„¤ê³„í–ˆë‹¤. 

### **Online Function Approximation**
Time-Sequence Modelingì€ online inputì— ëŒ€í•´ modeling í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— history memoryë¥¼ ì‹œê°„ ê´€ë ¨ basis functionì¸ $$f(t)$$ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. <br>
ì´ëŠ” basis functionì„ ê¸°ì¤€ìœ¼ë¡œ optimal coefficientë¥¼ í†µí•´ ê·¼ì‚¬í•  ìˆ˜ ìˆìœ¼ë©°, ì €ìëŠ” ê³¼ê±°ì˜ ê° ì‹œì ì˜ ì¤‘ìš”ì„±ì„ ì§€ì •í•˜ëŠ” **ì¸¡ë„(measure)**ë¥¼ ë°˜ì˜í•˜ì—¬ ê° timelineì— ëŒ€í•´ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•œë‹¤. <br>
Orthogonal Polynomialì€ optimal coefficientë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , closed-formìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ ë¬¸ì œì— ì ì ˆí•œ basis functionìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. <br>

<details markdown="block">
<summary><b>OP (Orthogonal Polynomial)</b></summary>
Orthogonal polynomial sequenceëŠ” sequenceë‚´ì˜ ë‘ polynomialì´ inner product ë‚´ì—ì„œ ì„œë¡œ orthogonalí•œ ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë§ì´ ì‚¬ìš©ë˜ëŠ” orthogonal polynomialë¡œëŠ” **Hermite, Laguerre, Jacobi, Gegenbauer, Chebyshev, Legendre polynomial** ë“±ì´ ìˆë‹¤.  <br>
OPëŠ” íŠ¹ì • measure (weight) $$\mu_t$$ì— ëŒ€í•´ ë‘ polynomial $$P_m, P_n$$ì˜ ì ë¶„ì´ ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ì¼ ë•Œ ì„±ë¦½í•œë‹¤. <br>

$$
\begin{align}
\mathrm{If} \; \deg P_n = n, \left\langle P_m, P_n \right\rangle _{\mu(x)} &= \int P_m(x)P_n(x)d\mu(x), \\
\left\langle P_m, P_n \right\rangle _{\mu(x)} &= 0 \;\mathrm{for}\; m \neq n    
\end{align} 
$$

ê° basis $$P_n (n=0, 1, 2, \cdots)$$ì€ Gram-Schmidt(ì„ì˜ì˜ ë‹¤í•­ì‹ì„ ì§êµ ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜)ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.

**Recurrence relation** <br>
ëª¨ë“  normalized orthogonal polynomial($$P_n(s)$$)ì€ 3-term recurrenceê°€ ì¡´ì¬í•œë‹¤. 
Polynomials $$P_n$$ì€ $$A_n$$ì´ 0ì´ ì•„ë‹ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ recurrence relationì„ ë§Œì¡±í•œë‹¤. <br>
$$\rightarrow$$ ì¦‰, $$P_0, P_1, A_n, B_n, C_n$$ì„ ì•Œë©´ ì´í›„ ë‹¤í•­ì‹ë„ ë‹¤ êµ¬í•  ìˆ˜ ìˆìŒ

$$
\begin{align}
P_n(x)&=(A_nx+B_n)P_{n-1}(x)+C_nP_{n-2}(x) \\
&\text{where } A_n = \frac{\left\langle xP_n, P_n \right\rangle}{\left\langle P_n, P_n \right\rangle}, B_n = \frac{\left\langle P_n, P_n \right\rangle}{\left\langle P_{n-1}, P_{n-1} \right\rangle}
\end{align} 
$$

Recurrence relationì€ 
1. $$P_n(x)$$ë¥¼ $$O(n)$$ ì—°ì‚°ìœ¼ë¡œ ìƒì„±ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì 
2. ì´ì „ ë‘ ì°¨ìˆ˜ì˜ í•­ë§Œ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— memory efficient
3. Gramâ€“Schmidtì— ë¹„í•´ ìˆ˜ì¹˜ì  ëˆ„ì  ì˜¤ë¥˜ê°€ ì—†ì–´, ì•ˆì •ì 
4. Jacobi Matrix(tridiagonal matrix)ë¡œ êµ¬ì„± ê°€ëŠ¥

ì˜ ì¥ì ì´ ìˆë‹¤. <br>


> **Jacobi Matrix** <br>
> ì í™”ì‹ì˜ ê³„ìˆ˜($$A_n, B_n$$ ë“±)ë¥¼ ëŒ€ê°/ë¹„ëŒ€ê° ì„±ë¶„ìœ¼ë¡œ í•˜ëŠ” **tridiagonal matrix**ë¥¼ ì˜ë¯¸í•œë‹¤. <br>
> 
>   $$\begin{equation}J=\begin{bmatrix}\alpha_0 & \sqrt{\beta_1} & 0 & 0 & \cdots \\ \sqrt{\beta_1} & \alpha_0 & \sqrt{\beta_2} & 0 & \cdots \\ 0 & \sqrt{\beta_2} & \alpha_2 & \sqrt{\beta_3} & \cdots \\ \vdots & \vdots & \ddots & \ddots & \ddots\end{bmatrix}\end{equation}$$
> 
> Jacobi Matrixì˜ eigenvalueëŠ” $$P_n(x)$$ì˜ zerosê°€ ë˜ëŠ”ë°, ì´ëŠ” Gauss-Quadratureì˜ nodeì™€ ê°™ë‹¤. <br> 
> Gauss-Quadratureì˜ nodeëŠ” í•¨ìˆ˜ $$f(x)$$ì˜ ì ë¶„ì„ ê·¼ì‚¬í•  ë•Œ, ê¸°ì¤€ì´ ë˜ëŠ” ì ë“¤ì´ë‹¤.  <br>
> 
>   $$\int_a^b f(x)w(x)dx \eqsim \sum^n_{i=1}w_if(x_i)$$
> 
> í•¨ìˆ˜ì˜ ì ë¶„ì„ êµ¬í•˜ì§€ ì•Šê³ , nê°œ ì ì—ì„œì˜ ê³„ì‚°ì— ë”°ë¼ íš¨ìœ¨ì ì´ê³  ì •í™•í•˜ê²Œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤. <br>
> ì´ëŸ¬í•œ íŠ¹ì„± ë•ì—, HiPPOì—ì„œëŠ” ì í™”ì‹ìœ¼ë¡œë¶€í„° ì–»ì€ ê³„ìˆ˜ë¥¼ Jacobi Matrixë¡œ êµ¬ì„±í•˜ê³ , eigenvalueì™€ eigenvectorë¥¼ êµ¬í•´ Gauss-Quadratureì˜ nodeì™€ weightë¥¼ ë‹¤ì–‘í•œ ìˆ˜ì¹˜í•´ì„ ë° íš¨ìœ¨ì ì¸ í•™ìŠµì˜ baseë¡œ ì‚¬ìš©í•œë‹¤. <br>

{: .highlight-title }
> ì¦‰, Recurrence relationì„ í†µí•œ êµ¬ì¡°ê°€ HiPPOì˜ state space representationì˜ ê¸°ë°˜ì´ ëœë‹¤.

> **ëŒ€í‘œì  ì§êµë‹¤í•­ì‹ì˜ ì í™”ì‹**
> 1. Legendre $$P_n$$ <br>
>   $$\rightarrow (n+1)P_{n+1}(x)=(2n+1)xP_n(x)-nP_{n-1}(x)$$
> 2. Laguerre $$L_n$$ <br>
>   $$\rightarrow (n+1)L^{(\alpha)}_{n+1}(x)=(2n+1+\alpha-x)L^{(\alpha)}_n(x)-(n+\alpha)L^{(\alpha)}_{n-1}(x)$$
> 3. Hermite $$H_n$$ <br>
>   $$\rightarrow H_{n+1}(x)=2xH_n(x)-2nH_{n-1}(x)$$
> 4. Chebyshev $$T_n$$ <br>
>   $$\rightarrow T_{n+1}(x)=2xT_n(x)-T_{n-1}(x)$$

**Reconstruction**

$$
\begin{align}
\sum^{N-1}_{i=0} c_iP_i(x) / \lVert P_i \rVert^2_\mu, \quad \mathrm{where} \; c_i = \left\langle f, P_i \right\rangle _\mu = \int f(x)P_i(x)d\mu(x)
\end{align} 
$$


{: .note-title}
> Legendre Polynomial <br>
> 
> $$\begin{align}\int^1_{-1}P_m(x)P_n(x)dx=\frac{2}{2n+1}\delta_{mn}, \text{where } w(x)=1\end{align}$$
> 
> $$\begin{align}P_0(x)&=1 \\ P_1(x)&=x \\ P_2(x)&=\frac{1}{2}(3x^2-1) \\ P_3(x)&=\frac{1}{2}(5x^3-3x) \end{align}$$
>
> Laguerre Polynomials <br>
> 
> $$\begin{align}\int^{\inf}_{0}L_m(x)L_n(x)e^{-x}dx=\delta_{mn}, \text{where } w(x)=e^{-x}\end{align}$$
> 
> $$\begin{align}L_0(x)&=1 \\ L_1(x)&=-x+1 \\ L_2(x)&=3x^2-4x+2\end{align}$$
</details>


{: .highlight-title }
> **HiPPO(High-order Polynomial Projection Operators)**ëŠ” measureì— ë”°ë¼ $$f(t)$$ë¥¼ OPì— projectioní•˜ì—¬ vector space(optimal coefficient)ë¡œ ì••ì¶•í•˜ëŠ” frameworkë‹¤. <br>
> ODEë‚˜ linear recurrenceë¡œ memoryë¥¼ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— continuousí•˜ê³  real-timeìœ¼ë¡œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.

## **The HiPPO Framework: High-order Polynomial Projection Operators**
1. Projectionì„ í†µí•œ Online Function Approximation ë° memory representation í•™ìŠµ
2. Memory updateë¥¼ ìœ„í•œ HiPPO framework
3. LMU ë„ì¶œ ë° HiPPO Framework (HiPPO-LegS, HiPPO-LagT, etc) ì œì‹œ
4. Continuous-time ì¸ HiPPO framework descretize
5. GRU, LSTMì˜ generalize

ì¼ë°˜ì ìœ¼ë¡œ, $$f(t)\in\mathcal{R}, t\leq 0$$ì¸ ë¬¸ì œì—ì„œëŠ” $$t\leq 0$$ì¸ ëª¨ë“  $$f_{\leq t}:=f(t)\vert_{x\ge t}$$ì— ëŒ€í•´ cumulative historyì— ëŒ€í•œ ì´í•´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ predictionì´ í•„ìš”í•˜ë‹¤. <br>
HiPPOëŠ” bounded dimensionì— ëŒ€í•œ projectionì„ í†µí•´ ì ‘ê·¼í•œë‹¤. <br>

Cumulative historyì˜ approximationë¥¼ ìœ„í•´ì„œ approximationì˜ quantify í‰ê°€ì™€ subspace ê²°ì • ë°©ë²•ì´ í•„ìš”í•˜ë‹¤. <br>

QuantifyëŠ” function ê°„ì˜ ê±°ë¦¬ë¥¼ ì •ì˜í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. <br>
Hilbert space structureì—ì„œ orthogonal polynomialì€ mesarue $$\mu(x)$$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‚´ì ì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. <br>

$$
\begin{align}
    \left\langle f, g \right\rangle_\mu&=\int^b_a f(x)g(x)d\mu(x), \qquad where \quad d\mu(x)=w(x)dx \\
    \left\langle f, g \right\rangle_\mu&=\int^{\inf}_0 f(x)g(x)w(x)dx
\end{align}
$$

Normì€ ë‚´ì ì´ ì •ì˜ë˜ì—ˆì„ ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„ëœë‹¤.

$$
\begin{align}
    \parallel f\parallel_{L_2(\mu)}&=\sqrt{\left\langle f, f \right\rangle_\mu} \\
    &=\left(\int^{\inf}_0 f(x)f(x)w(x)dx^{1/2}\right) \\
    &=\left\langle f, f \right\rangle_\mu^{1/2}
\end{align}
$$

ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ HiPOì—ì„œ orthogonal polynimialì˜ ê¸¸ì´ì™€ ì§êµì„±ì„ ì •ì˜í•˜ê³ , Recurrence Relationê³¼ Reconstructionì„ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 

$$N$$-dimensionì˜ subspace $$\mathcal{G}$$ë¥¼ approximationìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ $$N$$ì€ ì••ì¶• ì •ë„ë¥¼ ì˜ë¯¸í•˜ë©°, $$N$$ê°œì˜ (basisì˜)coefficientsë¥¼ í†µí•´ projected historyë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. 

ìµœì¢…ì ìœ¼ë¡œ, ëª¨ë“  $$t$$ì—ì„œ,

$$
\begin{align}
    \parallel f_{\leq t}-g^{(t)}\parallel_{L_2(\mu^{(t)})}
\end{align}
$$

ë¥¼ minimizeí•˜ëŠ” $$g^{(t)}\in\mathcal{G}$$ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì´ë©°, ì£¼ì–´ì§„ $$\mu^{(t)}$$ì— ëŒ€í•´ ì–´ë–»ê²Œ closed-formìœ¼ë¡œ optimization problemì„ í‘¸ëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤.  <br>
$$\mu$$ëŠ” input domainì˜ importanceë¥¼, basisëŠ” approximation ì •ë„ë¥¼ ì˜ë¯¸í•œë‹¤. 

### **General HiPPO framework**
ì•ì—ì„œì™€ ê°™ì´, Nê°œì˜ coefficientë¡œ í‘œí˜„ë˜ëŠ” approximationì—ì„œ basisë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì „í†µì ì¸ ë°©ë²•ì— ë”°ë¼, $$\mu^{(t)}$$ì— ëŒ€í•´ orthogonal basisë¥¼ subspaceë¡œ í•˜ëŠ” coefficientë¥¼ $$c^{(t)}_n:=\left\langle f_{\leq}, g_n \right\rangle$$ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. <br>

ì¶”ê°€ì ìœ¼ë¡œ,  $$c^{(t)}$$ì˜ ë¯¸ë¶„ì´ $$c^{(t)}$$ì™€ ê´€ë ¨ë˜ì–´ í‘œí˜„ë  ìˆ˜ ìˆëŠ” self-similar relationì„ ë„ì¶œí•œë‹¤. ì´ë¥¼ í†µí•´, $$c(t)\in\mathbb{R}^N$$ì€ $$f(t)$$ dynamicsì— ëŒ€í•œ ODEë¡œ í’€ ìˆ˜ ìˆë‹¤.

<figure>
    <center><img src="/assets/images/papers/mamba/hippo_fig1.jpg" width="80%" alt="Figure 1"></center>
	<center><figcaption><em>[Figure 1]</em></figcaption></center>
</figure>

#####  **The HiPPO abstraction: online function approximation**
> **Definition 1.** $$\left(-\inf, t\right]$$ì—ì„œ time-varying measure $$\mu^{(t)}$$ì™€ $$N$$-dimensionì˜ subspace $$\mathcal{G}$$ ìƒì—ì„œ polynomial, ì—°ì† í•¨ìˆ˜ $$f:\mathbb{R}_{\ge 0}\rightarrow \mathbb{R}$$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, HiPPOëŠ” $$\operatorname{proj}_t$$ì™€ coefficient ì¶”ì¶œ ì—°ì‚° $$\operatorname{coef}_t$$ì„ ì •ì˜í•˜ë©°, ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤. <br>
> 1. $$\operatorname{proj}_t$$ëŠ” ì‹œê°„ tê¹Œì§€ì˜ í•¨ìˆ˜ $$f$$ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ê·¸ë¦¬ê³  $$f_{\leq t}$$ëŠ” Approximation Errorë¥¼ minimizeí•˜ë„ë¡ Polynomial subspace $$\mathcal{G}$$ë¡œ mappingëœë‹¤. <br>
> $$\text{Approximation Error: } \parallel f_{\leq t}-g^{(t)}\parallel_{L_2(\mu^{(t)})}$$
> 2. $$\operatorname{coef}_t: \mathcal{G}\rightarrow \mathbb{R}^N$$ì€ orthogonal polynomialë“¤ì˜ linear combineìœ¼ë¡œ í‘œí˜„í–ˆì„ ë•Œ, ê°€ì¥ ì˜ ê·¼ì‚¬ëœ polynomialë¡œ ê·¼ì‚¬í•œ ê·¸ ê³„ìˆ˜ë“¤ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì´ë‹¤. <br>
> $$\operatorname{coef}\circ\operatorname{proj}$$ì˜ ì¡°í•©ì„ HiPPOë¼ê³  í•œë‹¤. $$(\operatorname{hippo}(f))(t)=\operatorname{coef}_t(\operatorname{proj}_t(f))$$

ê° $$t$$ì— ëŒ€í•´, $$\operatorname{proj}_t$$ëŠ” ë‚´ì ìœ¼ë¡œ ì˜ ì •ì˜ë˜ì§€ë§Œ, ê³„ì‚°í•˜ê¸°ëŠ” ì‰½ì§€ ì•Šë‹¤. Appendix Dì—ì„œ **HiPPO**ê°€ ODE formìœ¼ë¡œ ì •ë¦¬ë˜ì–´ ë‹¤ìŒì²˜ëŸ¼ ë‚˜íƒ€ë‚œë‹¤.  

$$
\begin{align}
    \frac{d}{dt}c(t)=A(t)c(t)+B(t)f(t), \quad \text{where } A(t)\in\mathbb{R}^{N\times N}, B(t)\in\mathbb{R}^{N\times 1}
\end{align}
$$

ë”°ë¼ì„œ, HiPPOëŠ” ODEë¥¼ í’€ê±°ë‚˜, recurrentí•˜ê²Œ $$c(t)$$ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤. 

### **High Order Projection: Measure Families and HiPPO ODEs**
ì—¬ëŸ¬ measure $$\mu_t(x)$$ì— ëŒ€í•´ HiPPO Frameworkë¥¼ êµ¬ì²´í™”í•  ìˆ˜ ìˆë‹¤. <br>
Appendixì—ì„œëŠ” ë‹¤ìŒì— ì œì‹œë  2ê°œì˜ HiPPO frameworkê°€ ì´ì „ ì—°êµ¬ì¸ LMUì™€ FRUì˜ ì¼ë°˜í™” ë²„ì „ì´ë¼ëŠ” ê²ƒì„ ë³´ì¸ë‹¤. LegT(Translated Legendre)ì™€ LagT(Translated Laguerre)ëŠ” sliding window í˜•ì‹ìœ¼ë¡œ, ì´ì „ $$\theta$$ê¹Œì§€ì˜ historyë¥¼ measureì— ë”°ë¼ í• ë‹¹í•œë‹¤.
- LegT: ê· ì¼í•˜ê²Œ í• ë‹¹
- LagT: ìµœê·¼ historyì— exponentialí•˜ê²Œ ë” ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ <br>

$$
\begin{align}
    \textbf{LegT: }\mu^{(t)}(x)=\frac{1}{\theta}\mathbb{I}_{[t-\theta,t]}(x) \quad \textbf{LagT: }\mu^{(t)}(x)=\exp^{-(t-x)}\mathbb{I}_{[t-\inf,t]}(x)=
    \begin{cases}
    e^{x-t} & \text{if } x\leq t \\
    0 & \text{if } x>t
    \end{cases} \;  
\end{align}
$$

**Definition 1**ì„ ë§Œì¡±í•˜ëŠ” HiPPO operationì€ ì•„ë˜ì™€ ê°™ì€ LTI(Linear Time-Invariant) ODEë¡œ ë‚˜íƒ€ë‚œë‹¤.

$$
\begin{align}
    \textbf{LegT: }& \\
    &A_{nk}=\frac{1}{\theta}
    \begin{cases}
        (-1)^{n-k}(2n+1) & \text{if } n\leq k  \\
        2n+1 & \text{if } n\ge k ,
    \end{cases}, \quad &B_n=\frac{1}{\theta}(2n+1)(-1)^n \\
    \textbf{LagT: }& \\
    &A_{nk}=\frac{1}{\theta}
    \begin{cases}
        1 & \text{if } n\leq k  \\
        0 & \text{if } n<k
    \end{cases}, \quad &B_n=1 
\end{align}
$$

### **HiPPO recurrences: from Continuous to Discrete Time with ODE Discretization**
ì‹¤ì œ ìš°ë¦¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ëŠ” ì´ì‚°í™”ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ODE-formì˜ ìˆ˜ì‹ì„ ì´ì‚°í™”í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ìœ„ì˜ continuous HiPPO ODEë¥¼ discrete HiPPO ODEë¡œ ì´ì‚°í™”í•œë‹¤. <br>
ì´ì‚°í™” timestep $$\delta t$$ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚œë‹¤.

$$
\begin{align}
    c(t+\delta t)=c(t)+\delta t\cdot u(t,c(t),f(t))^2
\end{align}
$$


## **HiPPO-LegS: Scaled Measures for Time scale Robustness**
Sliding Window ë°©ì‹ì€ Signal Processingì—ì„œ ë§ì´ ì“°ì´ì§€ë§Œ, ì „ì²´ historyë¥¼ approximationí•˜ê¸° ìœ„í•´ì„ , scaled-windowë¥¼ í†µí•´ memoryë¥¼ í•´ì•¼í•œë‹¤. <br>
ë”°ë¼ì„œ, ì €ìëŠ” **HiPPO-LegS**(scaled Legendre measure)ë¥¼ ì œì‹œí•œë‹¤. <br>

$$
\begin{alignat}{3}
    \begin{matrix}
        \frac{t}{dt}c(t)=-\frac{1}{t}Ac(t)+\frac{1}{t}Bf(t) \\ \\
        c_{k+1}=\left(1-\frac{A}{k} \right)c_k+\frac{1}{k}Bf_k
    \end{matrix} 
    ,\quad &
    A_{nk}=
    \begin{cases}
        (2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}}, & \text{if } n>k \\
        n+1, & \text{if } n=k \\
        0, & \text{if } n<k
    \end{cases},
    \quad & 
    B_n=(2n+1)^{\frac{1}{2}}
\end{alignat}
$$

**HiPPO-LegS**ëŠ” input timescaleì— invariantí•˜ê³  ê³„ì‚°ì— íš¨ìœ¨ì ì´ë©° approximation errorê°€ boundedë˜ì–´ ìˆë‹¤.


### **Timescale robustness**
<figure>
    <center><img src="/assets/images/papers/mamba/hippo_fig2.jpg" width="80%" alt="Figure 2"></center>
	<center><figcaption><em>[Figure 2]</em></figcaption></center>
</figure>
LegSëŠ” $$[0, t]$$ì— ëŒ€í•´ uniformí•˜ê²Œ measureë¥¼ í• ë‹¹í•œë‹¤. ë”°ë¼ì„œ, ì‹œê°„ì¶•ì— ëŒ€í•´ window sizeê°€ adaptiveí•˜ê¸° ë•Œë¬¸ì— timescaleì— robustí•˜ë‹¤.

> **Proposition 3.** <br>
> For any scalar $$\alpha>0$$, if $$h(t)=f(\alpha t)$$, then $$\operatorname{hippo}(f)(\alpha t)$$. <br> 
> In other words, if $$\gamma \rightarrow \alpha t$$ is any dilation function, then $$\operatorname{hippo}(f \circ\gamma)(\alpha t)=\operatorname{hippo}(f)\circ\gamma$$.

### **Computational efficiency**
HiPPO-LegSì˜ Matrix $$A$$ëŠ” $$O(N)$$ì˜ ê³„ì‚° ë³µì¡ë„ë¥¼ ê°€ì§„ë‹¤. 

> **Proposition 4.** <br>
> Under any generalized bilinear transform discretization (cf. Appendix B.3), each step of the
 HiPPO-LegS recurrence in equation (4) can be computed in $$O(N)$$ operations.

### **Gradient flow**
RNN êµ¬ì¡°ëŠ” gradientê°€ $$W^t$$ í˜•íƒœì´ê¸° ë•Œë¬¸ì— vanishingì´ ë°œìƒí•  ìˆ˜ ìˆì§€ë§Œ, HiPPOëŠ” convolution-based trainingê³¼ orthogonal polynomialâ€“based memory ë•Œë¬¸ì— kernelì˜ í˜•íƒœë¡œ long-term memoryì— ëŒ€í•œ gradientë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŒ.

> **Proposition 5.** <br>
> For any times $$t_0 < t_1$$, the gradient norm of HiPPO-LegS operator for the output at time $$t_1$$ with respect to input at time $$t_0$$ is $$\parallel\frac{\partial c(t_1)}{\partial f(t_0)}=\Theta(1/t_1) \parallel$$


### **Approximation error bounds**
LegSì˜ Approximation errorì€ inputì´ ë¶€ë“œëŸ¬ìš¸ìˆ˜ë¡, Nì´ ë†’ì„ìˆ˜ë¡ ê°ì†Œ

> **Proposition 6.** <br>
> Let $$f : \mathbb{R}_+ \to \mathbb{R}$$ be a differentiable function, and let $$g^{(t)} = \operatorname{proj}_t(f)$$ be its projection at time $$t$$ by HiPPO-LegS with maximum polynomial degree $$N-1$$. If $$f$$ is $$L$$-Lipschitz then $$\parallel f_{\leq t} - g^{(t)}\parallel=O(\frac{tL}{\sqrt{N}})$$. If $$f$$ has order-$$k$$ bounded derivatives then $$\parallel f_{\leq t} - g^{(t)} \parallel=O(t^k N^{-k+1/2})$$.


## **Empirical Validation**


## **Conclusion**
### **Key Contribution**
- HiPPO Framework ì œì‹œ
  - Long-term sequenceì—ì„œë„ ë™ì‘
  - ìœ ì—°í•œ time-step
  - Distribution Shift ë¬¸ì œ ì™„í™”
- ì´ì „ Methodë“¤ì˜ ë™ì‘ì€ ìˆ˜í•™ì ìœ¼ë¡œ ë¶„ì„ (unified)

### **Significance & Impact**
- ì§êµ ë‹¤í•­ì‹ ê·¼ì‚¬ë¼ëŠ” ìˆ˜í•™ì  ì ‘ê·¼ì„í†µí•´ memory ë¬¸ì œ ë¶„ì„
- ìƒˆë¡œìš´ ëª¨ë¸ ê³„ë³´ì˜ ì‹œì‘
  - ì§ì ‘ì ì¸ ëª¨ë¸ êµ¬í˜„ì˜ ì–´ë ¤ì›€ì´ìˆì§€ë§Œ, ì´ë¡ ì  ê²¬ê³ í•¨ìœ¼ë¡œ ì¸í•´ í›„ì† ì—°êµ¬ì˜ ê¸°ë°˜ì´ ë¨
  - ex. S4, Mamba

## **Reference**
1. [Wikipedia: Orthogonal polynomials](https://en.wikipedia.org/wiki/Orthogonal_polynomials)
2. 