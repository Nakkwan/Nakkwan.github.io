---
title: GAN
tags:
    - Deep Learning
    - Paper
    - GAN
---

A brief note on GANs <br>
vanilla GAN, Conditional GAN, Pix2Pix, PatchGAN, StarGAN, PGGAN

<!--more-->

---

### vanilla GAN

<br>

GAN = Generative Adversarial Network <br>
GAN은 진짜 모델과 비슷한 가짜 모델을 만드는 적대적 생성 네트워크로 unsupervised training에 속한다.<br>
latent variable z로부터 가짜 데이터를 생성하는 Generator G와 G로부터 생성한 가짜 데이터와 진짜 데이터를 구별하는 Discriminator D가 존재한다.
G는 D를 속일 수 있도록 학습되고 D는 진짜 데이터를 판별할 수 있도록 학습되며 G와 D가 경쟁하여 학습된다. <br>

![GAN1_Image1](https://user-images.githubusercontent.com/48177363/140280172-dfe2449f-0e34-4ab5-88f4-b4a47b05ee67.jpg){: width="750"} <br>

Loss: $$\min_{G}\max_{D}V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log (D(x))] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]$$<br>

-   G: $$\nabla_{\theta_{g}}\frac{1}{m}\sum_{i=1}^{m}\log (1-D(G(z^{(i)})))$$ <br>
-   D: $$\nabla_{\theta_{g}}\frac{1}{m}\sum_{i=1}^{m}[\log D(x^{(i)}) + \log (1-D(G(z^{(i)})))]$$
    <br>

---

### Conditional GAN

<br>
vanilla GAN의 경우 생성되는 이미지를 조절할 수 없다. MNIST dataset을 기준으로 이미지를 생성한다고 할 때, 어떤 숫자를 생성할지 사용자가 조절하기 힘들다. 따라서 Conditional GAN에서는 auxiliary information을 추가하여 생성되는 데이터에 대한 조건을 추가한다. <br> <br>
Loss: $$\min_{G}\max_{D}V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log (D(x\mid y))] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z\mid y)))]$$<br><br>
MNIST의 경우 100-dimension의 laten vector z로부터 데이터가 생성되는데 z에 MNIST의 클래스 개수에 해당하는 10-dimension의 vector y를 concate하여 input으로 넣게 된다. 
vector y는 n_class-dimension의 one-hot-vector의 형식이 embedding된 같은 차원의 n_class-dimension vector다.

![GAN1_Image2](https://user-images.githubusercontent.com/48177363/140280245-5df6e893-4fe1-49f6-8bd3-5f2c5e09ffb0.jpg){: width="750"} <br>

---

### Pix2Pix

<br>
Pix2Pix는 CGAN에서 생성하는 이미지의 condition을 이미지로 주는 model이다. Condition 이미지 자체가 Generator의 INPUT으로 들어간다.<br>
x를 condition, y를 Ground Truth, z를 latent라 할 때, Loss는
- $$\mathcal{L}_{cGAN}(G,D)$$: $$ \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x,z}[\log 1-D(x,G(x,z))]$$ <br>
- $$\mathcal{L}_{pix}(G)$$: $$\mathbb{E}_{x,y,z}[\lVert y - D(x,z) \rVert_{1}]$$ <br>
- $$\mathcal{L}_{G}: \mathcal{L}_{cGAN}(1,D(G(z),z)) + \mathcal{L}_{pix}(G(z),y)$$ <br>
- $$\mathcal{L}_{D}: \mathcal{L}_{cGAN}(1,D(z,y)) + \mathcal{L}_{cGAN}(0,D(G(z),y))$$ <br>
로 나타난다. Pix2Pix에는 <br>
- CNN (UNet) <br>
- PatchGAN의 Discriminator<br>
가 적용되었다. <br>

###### PatchGAN

<br>
PatchGAN은 기존의 GAN이 Discriminator를 속이려는 방향으로만 학습을 진행하다보니 생성된 데이터에 blur가 발생하거나 특정 성격이 강화되는 현상을 해결하기 위한 방법을 제시한다. <br>
PatchGAN의 Discriminator에서는 기존의 FCN을 사용하던 Discriminator와 다르게 CNN을 사용하였다. 이미지의 참/거짓 판단 여부를 여러 패치에서 모두 참이 되도록 학습을 진행한다. D는 Output feature로 NxN feature map을 생성한다. 논문에서 256x256 사이즈의 input image에 대해 output feature map의 각 unit의 receptive field는 70x70 이다. 논문에서는 각 unit에서 receptive field 밖의 영역은 independent 하다고 가정했다. input size에 따라 적절한 receptive field가 다른 hyperparameter다. PatchGAN의 D는 <br>
- Parameter 수의 감소 <br>
- high frequency에 대한 성능 강화 <br>
논문에서 CrossEntorpy를 사용하여 NxN feature map에 대한 Loss를 계산했지만 D에 convnet을 사용한 경우 모두 PatchGAN으로 봐도 된다는 [issue](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39)가 있다. 따라서 NxN feature의 평균으로 Loss를 계산하는 경우와 1x1 feature를 쓰는 경우에도 PatchGAN이라고 볼 수 있다.

---

### CycleGAN

<br>
기존의 Pix2Pix는 Condition image에 대한 GT가 존재해야하기 때문에 dataset이 unpair한 경우 translation이 불가능하다. CycleGAN에서는 unpaired한 경우에 대한 translation 방법을 제시한다.<br>
CycleGAN은 기본적으로 Unpaired한 각각의 dataset에 대한 translation을 위해 2개의 Generator와 2개의 Discriminator로 구성된다. <br>
![GAN1_Image8](https://user-images.githubusercontent.com/48177363/140329623-25759d98-de33-4887-93a5-cc0e304cacbb.jpg){: width="750"} <br>
우선적으로 CycleGAN의 Loss는 <br>
- $$\mathcal{L}_{GAN}: \mathbb{E}_{y \sim p_{data}(y)}[\log D_{y}(y)] + \mathbb{E}_{x \sim p_{data}(x)}[\log D_{y}(G(x))]$$ <br>
- $$\mathcal{L}_{cyc}: \mathbb{E}_{x \sim p_{data}(x)}[\lVert F(G(x))-x \rVert_{1}] + \mathbb{E}_{y \sim p_{data}(y)}[\lVert G(F(y))-y \rVert_{1}]$$ <br>
- $$\mathcal{L}_{idt}: \mathbb{E}_{y \sim p_{data}(y)}[\lVert G(y)-y \rVert_{1}] + \mathbb{E}_{x \sim p_{data}(x)}[\lVert F(x)-x \rVert_{1}]$$ <br><br>
- $$\mathcal{L}_{G}: \mathcal{L}_{GAN}(G,D_{Y},X,Y) + \mathcal{L}_{GAN}(F,D_{X},Y,X) + \lambda\mathcal{L}_{cyc}(G,F)$$<br>
- $$\mathcal{L}_{D}: \mathcal{L}_{GAN}$$<br>
이다.<br>
일반적인 GAN과 같이 $$\mathcal{L}_{GAN}$$만 쓰게 된다면, 어떤 input을 넣든 D를 속이기 위해 같은 하나의 이미지만 뽑아내는 image collapse가 일어나도록 학습될 수 있다. 따라서 generate된 fake image가 G에 넣었을 때 다시 원래의 image가 나오도록 $$\mathcal{L}_{cyc}$$가 추가되었다. <br>
CycleGAN의 경우 generate 시 원래 이미지의 색을 보존해주는 loss가 없기 때문에 G를 통과한 image의 색이 보존이 장담될 수 없다. 따라서 G를 통과한 image가 다시 G가 될 수 있도록$$\mathcal{L}_{idt}$$를 추가해주면 색감을 보존할 수 있다.<br>

---

### StarGAN

<br>
현재까지의 GAN들은 2개 이상의 domain에 대해 다루기에 robust하다. StarGAN은 여러 도메인의 dataset에 생성 모델을 적용하기 위한 모델이다. 기존의 GAN의 경우 4개의 도메인에 대해 변환을 진행할 떼, 12개의 Generator가 필요하다. 따라서 여러 도메인에 대한 통합 모델인 StarGAN이 제안되었다. StarGAN은 하나의 generator로 여러 도메인 간의 translate를 수행한다.<br>

![GAN1_Image3](https://user-images.githubusercontent.com/48177363/140310456-bf251cae-ebbf-4216-89c7-829d61d940db.jpg){: width="750"} <br>

StarGAN은 Cycle GAN과 Condition GAN이 합쳐진 형태다. Input이 translate를 원하는 target domain label과 같이 generator에 Input으로 들어가 fake image를 만들게 된다. CycleGAN과 같이 reconstruction을 위해 fake image가 원래 image의 label을 condition으로 하여 input으로 generator로 다시 들어간다. 2개의 generator를 쓴 CycleGAN과는 다르게 fake image를 만드는데 썼던 generator에 들어가게 된다. <br>
Discriminator는 input으로 받은 이미지에 대해 fake/real을 판단하는 $$D_{src}$$와 domain을 판단하는 $$D_{cls}$$를 output으로 만들어낸다. <br>

![GAN1_Image4](https://user-images.githubusercontent.com/48177363/140313000-fe3b43be-6aa4-48b6-8bc7-c592d0740e03.jpg){: width="750"} <br>

StarGAN의 Loss로는 $$x$$는 input image, $$c, c'$$는 각각 target domain label, orign domain label이라고 할 때,<br>

-   $$\mathcal{L}_{adv}: \mathbb{E}_{x}[\log D_{src}(x)] + \mathbb{E}_{x,c}[\log (1-D_{src}(G(x,c)))]$$<br>
-   $$\mathcal{L}_{cls}^{r}: \mathbb{E}_{x,c'}[-\log D_{cls}(c'\mid x)]$$<br>
-   $$\mathcal{L}_{cls}^{f}: \mathbb{E}_{x,c}[-\log D_{cls}(c'\mid G(x,c'))]$$<br>
-   $$\mathcal{L}_{rec}: \mathbb{E}_{x,c,c'}[\lVert x-G(G(x,c),c') \rVert_{1}]$$<br><br>
-   $$\mathcal{L}_{G}: \mathcal{L}_{adv} + \lambda_{cls} \mathcal{L}_{cls}^{f} + \lambda_{rec}\mathcal{L}_{rec}$$<br>
-   $$\mathcal{L}_{D}: -\mathcal{L}_{adv} + \lambda_{cls}\mathcal{L}_{cls}^{r}$$<br>

으로 나타난다. <br>
G에 대한 훈련은 생성된 이미지에 대한 Loss인 $$\mathcal{L}_{adv}$$와, 생성된 이미지의 label에 대한 Loss인 $$\mathcal{L}_{cls}^{f}$$, 그리고 image collapse를 방지하기 위한 $$\mathcal{L}_{rec}$$으로 구성된다. <br>
D에 대해서는 이미지를 더 잘 구별하기 위한 $$-\mathcal{L}_{adv}$$와 실제 이미지에 대한 label 판단인 $$\mathcal{L}_{cls}^{r}$$로 구성된다.<br>
$$\mathcal{L}_{adv}$$의 경우 Wasserstein GAN의 Loss를 사용하면 성능이 더 향상된다는 논문의 언급이 있다.<br>

###### Network

Generator는 기본적으로 Resblock으로 이루어져있다. Downsampling을 수행하는 Conv 2개와 Residual Block 6개, Upsampling을 수행하는 Deconv 2개로, 총 18개의 convoluation layer가 사용된다.
image에 대한 target label은 image size만큼 repeat되어, channel dimension에서 concate되어 generator의 input으로 들어간다.<br>

![GAN1_Image5](https://user-images.githubusercontent.com/48177363/140325350-737e4747-b5de-4fca-aad9-1dd1f7c6517c.jpg){: width="750"} <br>

Discriminator는 기본적으로 PatchGAN(ConvNet)으로 구성되어 있다. <br>

![GAN1_Image6](https://user-images.githubusercontent.com/48177363/140326030-80bc2d0f-5b65-45f4-b6ee-b8f7e691ccef.jpg){: width="750"} <br>

###### Mask Vector m

StarGAN은 여러 attribute뿐만 아니라 서로 다른 dataset을 한꺼번에 훈련하는 경우에 대해서도 언급했다. 논문에선 CelebA와 RaFD dataset을 같이 훈련시켰는데 두 dataset의 label 정보는 다르다. CelebA에는 머리색, 나이 등에 대한 labeling이 되어있지만 RaFD dataset에는 다른 attribute에 대한 label만 존재한다. 따라서 위 두 dataset을 같이 훈련시킬 때, 다른 dataset에 대한 훈련에 영향을 끼치지 않기위해 StarGAN에서는 Mask Vector m을 제시한다. target label이 속하는 dataset에 대한 mask를 input으로 추가해, 다른 dataset에 대한 label에 대해서는 훈련이 진행되지 않도록 한다. <br>
![GAN1_Image7](https://user-images.githubusercontent.com/48177363/140328458-87454f46-aaf6-4930-8524-0e888caef46f.jpg){: width="750"} <br>

---

### PGGAN

<br>
고해상도의 이미지를 생성하는 방법은 VAE, GAN등이 있지만, VAE는 흐릿한 결과를 생성하는 경우가 있고, PixelGAN의 경우 pixel에 대한 분포를 직접 설정하기 때문에 적용 가능성이 제한적이다. 따라서 PGGAN에서는 GAN의 점진적인 학습 방식을 제시한다. 저해상도 이미지부터 고해상도 이미지까지 점진적으로 학습하여, 한번에 고해상도 이미지를 생성하는 것보다 쉬운 문제를 해결하게 된다. PGGAN의 전체적이 구조는 다음과 같다. <br>
![GAN1_Image9](https://user-images.githubusercontent.com/48177363/140339359-630773a5-ac5e-478f-922f-0ca8a1a76e82.jpg){: width="750"} <br>
G와 D가 encoder decoder와 비슷하게 반대되는 모양으로 이뤄진다. 논문에서는 4x4 patch부터 시작하여 1024x1024 patch까지 생성한다. Generator가 일정 수준 이상으로 D를 속이게 되면 G와 D layer를 추가한다. 처음 추가된 layer가 학습 시, initialize가 안되어있기 때문에 이전에 학습된 layer에도 안좋은 영향을 끼칠 수 있다. 따라서 논문에서는 smoothing fade-in이라는 방법을 제시한다. <br>
![GAN1_Image10](https://user-images.githubusercontent.com/48177363/140345154-54d868e4-a417-4eed-8a8e-1b75d9ab0fbc.jpg){: width="750"} <br>
smoothing fade-in은 새로운 G가 추가되어 학습할 때, 어느정도 학습이 진행될 때까지 이전 layer의 output에 2배를 한 결과를 합쳐, 학습을 한다. D의 경우도 input을 $$\frac{1}{2}$$한 결과를 다음 layer에 일정비율로 합쳐서 계산하여 이전 layer가 초기화가 되지않은 새 layer의 영향을 덜 받을 수 있도록 한다. <br>
PGGAN은 iteration이 저해상도에서 시작하기 때문에 다른 GAN보다 비교적 학습속도가 빠르다는 단점이 있다. <br>
![GAN1_Image11](https://user-images.githubusercontent.com/48177363/140350960-84daee7e-28a8-4d7e-a0e2-cd4f4abc247a.jpg){: width="750"} <br>

---

###### Reference

-   [GAN](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)<br>
-   [Conditional GAN](https://arxiv.org/pdf/1411.1784.pdf)<br>
-   [Pix2Pix](https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)<br>
-   [PatchGAN](https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Combining_Markov_Random_CVPR_2016_paper.pdf)<br>
-   [CycleGAN](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)<br>
-   [StarGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)<br>
-   [PGGAN](https://arxiv.org/pdf/1710.10196.pdf)<br>
-   [Why GAN trainig hard?](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
