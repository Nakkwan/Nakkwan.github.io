---
title: StyleGAN Paper Review
tags:
  - Deep Learning
  - Paper
  - GAN
---
A brief paper review of StyleGAN <br>
<!--more-->

---
### StyleGAN
[StyleGAN](https://github.com/NVlabs/stylegan)은 이미지의 스타일을 바꿔주는 생성 모델로 2018년 NVidia에서 발표했다. <br>

##### Introduction

StyleGAN은 기존 GAN(Generative Adversarial Network)를 기반으로 한 이미지 생성 모델들이 이미지 생성, 합성 과정은 block box이고, 합성 과정에서 atrribute를 조절하기 힘들고 artifact들이 나타나는 현상을 해결하기 위해 제시되었다. <br> StyleGAN은 input space(input image)를 그대로 GAN 모델에 넣는 것이 아닌 latent space로 mapping 하여 model에 input으로 넣었다. Input image의 probability density를 따르는 latent space는 input space의 entanglement를 어느정도 해결하여 image attribute 조절에 도움이 된다.<br>

##### Style-based generator

![StyleGAN_Image1](https://user-images.githubusercontent.com/48177363/143530735-8c468fc7-6c96-4547-b94e-fbfd5f9497c1.jpg){: width="450"} <br>

전통적인 GAN은 latent space $$\mathcal{Z}$$(input image)를 그래도 GAN network의 첫번째 layer의 input으로 넣는 방식이 많았다. 하지만 StyleGAN에서는 $$\mathcal{Z}$$를 이미지 스타일 생성을 위한 latent space $$\mathcal{W}$$로 mapping 시킨 후 AdaIN을 사용하여 image에 style을 입히며 기본 구조는 PGGAN의 progressive 방식을 사용한다.
<details>
  <summary>
    AdaIN
  </summary>  
  <div markdown="1">
  AdaIN은 Content의 평균, 분산을 통해 스타일을 일치시키는 방식이다. 보통 style transfar는 이미지 등의 probability distribution를 일치시킴으로서 일어난다.<br>
  <ul>
    <li>Batch Normalization </li>
    Batch Normalization은 feature map을 channel 별로 mean과 std를 정규화시키는 방식이다. Network에 input이 minibatch 단위로 들어갈 때, batch의 각 channel을 같이 normalization 시킨다.<br>
    \begin{align}
      &BN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta  ( \gamma, \beta 는 parameter 다) \\
      &\mu_{c}(x) = \frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{c}(x) = \sqrt{\frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{c}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Instance Normalization</li>
    Batch Normalization이 같은 위치의 channel이면 batch 단위로 normalization을 진행했던 것과 다르게 IN은 각 sample 마다 channel 별로 normalization을 진행한다. <br>
    \begin{align}
      &IN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta (\gamma, \beta는 parameter 다) \\
      &\mu_{nc}(x) = \frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{nc}(x) = \sqrt{\frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{nc}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Conditional Instance Normalization </li>
    Style s마다 IN를 따로 학습하여 같은 Convolution weight를 가지고 있더라도 다른 Style을 generate할 수 있다.
    \begin{align}
      &CIN(x) = \gamma^{s} (\frac{x - \mu (x)}{\sigma (x)}) + \beta^{s} (\gamma, \beta는 parameter 다) \\
    \end{align}
    <li>Interpreting IN</li>
    BN과 IN의 비교 그래프는 아래와 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143556856-595b1646-2cee-4b61-8e53-9fd75bbbc251.jpg" width="600"> <br>
    Training set을 Origin(a)과 luminance(b)로 normalize해서 training set으로 활용했을 때, BN보다 IN의 Style Loss convergence가 빠른 것을 확인할 수 있다. 하지만 Training set을 Target과는 다른 하나의 Style로 normalize하여 training을 하였을 시, BN과 IN의 Style Loss의 convergence gap이 줄어든 것을 확인할 수 있다. 따라서 IN이 training 시, Style을 normalize하는 효과를 가지고 있어, Style transfar에 더 효과적인 normalize 방식이라는 것을 확인할 수 있다. <br><br>
    <li>Adaptive Instance Normalization</li>
    따라서 Style transfar에는 BN보다 IN이 좀 더 효과적이라고 할 수 있다. Style Mixing을 위한 StyleGAN에서는 AdaIN을 사용한다. AdaIN은 content input x와 style input y를 받고 training parameter가 없다. 기존 IN의 parameter를 input으로 대체한다. 아래의 식과 같이 기존 content input의 probability distribution을 style input의 것으로 바꿔 스타일 변화를 일으킨다.
    \begin{align}
      AdaIN(x) = \sigma (y) (\frac{x - \mu (x)}{\sigma (x)}) + \mu (y) \\
    \end{align}
    </ul>
  </div>
</details>

기존의 방법처럼 latent space z를 바로 network에 넣게 되면 임의의 latent z의 distribution은 고정되어 있기 때문에 training dataset의 distribution과 맞지 않아, image attribution과 latent z가 non-linear하게 mapping 되어있기 때문에 attribute 조절에 어려움이 있게 된다. 따라서 latent z를 latent w에 mapping시켜, 유동적인 분포를 갖는 latent w를 network의 input으로 넣게 되어 latent w와 attribute가 linear하게 될 수 있도록 한다. 이러한 특징을 disentanglement라 하고 disentangle한 정도를 측정하기 위해 논문에서 PPL(perceptual path length)와 linear saperability를 사용한다. <br><br>
latent z,w는 모두 1x512를 사용하며, mapping network $$\mathcal{f}$$는 8 layer MLP다. <br>
latent w는 ($$y_{s}, y_{b}$$)로 나뉘어 각 layer에 AdaIN의 입력으로 들어간다.
\begin{align}
AdaIN(x_{i}, y) = y_{s,i}(\frac{x_{i} - \mu (x_{i})}{\sigma (x_{i})}) + y_{b,i} \\
\end{align}
feature map x는 y(style)로 normalize 된다. <br><br>
(b)는 각 layer에 넣어주는 noise input이다. noise에 대한 설명은 뒤에 stocastic variation에서 자세하게 다룬다.<br>
generator에서 layer의 첫번째에 latent를 넣는 것은 성능에 큰 의미가 없다. 따라서 generator의 input으로 고정된 4x4x512 크기의 tensor를 넣는다. 

##### Properties of the style-based generator

- Style Mixing<br>
Style Mixing은 style, attribute를 더 localize 하기 위해 사용된다. 훈련을 할 때, 설정된 퍼센티지의 이미지는 latent $$z_{1}, z_{2}$$를 latent $$w_{1}, w_{2}$$로 mapping 한 후, network의 앞부분은 $$w_{1}$$을, 뒷 부분은 $$w_{2}$$를 사용하여 image generate를 한다. 이 regularization은 network가 인접한 style이 서로 상관관계가 없다고 가정할 수 있도록 한다. (latent w에서 style하나를 변경했을 때 다른 부분까지 영향받지 않도록 한다는 것을 의미하는 것 같다. 두 latent가 들어가다보니 이미지에서 각 역할을 분리해주는 느낌...?) <br>
![StyleGAN_Image3](https://user-images.githubusercontent.com/48177363/143576232-e874dcf7-36a5-4d34-b105-9366a4655088.jpg "Style Mixing"){: width="600"} <br>
Style Mixing 그림을 보면 얕은 layer 부분의 AdaIN에 들어간 latent는 얼굴 윤곽, 피부톤, 머리색 등 이미지의 Coarse한 부분에 영향을 끼치고, 깊은 layer는 눈, 코, 입 모양 등 dense한 부분에 영ㅇ향을 끼치는 것을 확인할 수 있다. <br><br>
- Stochastic variation <br>
사람에게는 피부, 머리카락 위치 등 이미지에 확률적으로 나타난다고 간주할 수 있는 특징이 있다. 기존의 GAN은 activation에 pseudorandom numbers를 추가하여 구현하였기 때문에 반복적인 패턴이 나타나는 경향이 있었다. StyleGAN은 Generator에서 이런 stochastic variation을 구현하기 위해 각 layer에 noise를 넣는다. <br>
![StyleGAN_Image4](https://user-images.githubusercontent.com/48177363/143583877-31466559-d3bc-43ec-a02d-fa43e76d3b3a.jpg "Examples of stochastic variation"){: width="300"} <br>
위 그림에서 흰 부분은 noise에 의한 변화가 일어난 부분을 의미한다. 전체적인 실루엣 등은 유지되지만 상세한 부분이 변화된 것을 확인할 수 있다. 노이즈가 없이 generation을 수행한다면 generator는 전체적인 평균적 특징을 잡기 때문에 detail에 약간씩 번지는 느낌이 날 수 있다. <br>
![StyleGAN_Image5](https://user-images.githubusercontent.com/48177363/143584161-b2be9006-4bc0-4b32-b38c-9b0f3dd7352d.jpg "Effect of noise inputs at different layers (a) Noise is applied to all layers. (b) No noise. (c) Noise in coarse layer (d) Noise in coarse layers"){: width="300"} <br>
위와 같이 노이즈가 없으면 머리카락같이 세밀한 부분이 번지는 모습을 볼 수 있고, noise도 마찬가지로 layer의 깊이에 따라 이미지에 미치는 영향이 다름을 확인할 수 있다.

##### Disentanglement studies
Disentanglement은 linear한 subspace로 구성된 latent space로, 각 subspace는 variation을 control하는 것을 목표로 한다. 하지만 latent z는 sampling probability가 training dataset에 해당 subspace의 밀도와 일치하기 때문에 training dataset에 독립적일 수 없고, linear하게 되는 것을 방해한다. 하지만 $$\mathcal{f}$$로 mapping된 latent w는 각 subspace가 linear하게 되도록 trainable하기 때문에 attribute의 variation이 더 linear해진다. 이 disentanglement를 정량화하기 위해 PPL과 linear saperability가 쓰인다. <br>
- PPL (Perceptual Path Length) <br>
latent vector에서의 interpolation이 이미지 생성에서 non-linear하다는 것을 entangle하다고 한다. 따라서 PPL은 두 latent vector를 interpolation하는 경로를 일정하게 나누어 만들어진 이미지들의 image length를 더해서 계산한다.

$\mathcal{l}_{z} = \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(slerp(z_{1},z_{2};t)), G(slerp(z_{1},z_{2};t + \epsilon)))]$ <br>
$\mathcal{l}_{w} = \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(lerp(f(z_{1}),f(z_{2});t)), G(lerp(f(z_{1}),f(z_{2});t + \epsilon)))]$ <br>




- Linear Saperability <br>




