---
title: StyleGAN Paper Review
tags:
  - Deep Learning
  - Paper
  - GAN
---
A brief paper review of StyleGAN <br>
<!--more-->

---
### StyleGAN
[StyleGAN](https://github.com/NVlabs/stylegan)은 이미지의 스타일을 바꿔주는 생성 모델로 2018년 NVidia에서 발표했다. <br>

#### Introduction

StyleGAN은 기존 GAN(Generative Adversarial Network)를 기반으로 한 이미지 생성 모델들이 이미지 생성, 합성 과정은 block box이고, 합성 과정에서 atrribute를 조절하기 힘들고 artifact들이 나타나는 현상을 해결하기 위해 제시되었다. <br> StyleGAN은 input space(input image)를 그대로 GAN 모델에 넣는 것이 아닌 latent space로 mapping 하여 model에 input으로 넣었다. Input image의 probability density를 따르는 latent space는 input space의 entanglement를 어느정도 해결하여 image attribute 조절에 도움이 된다.<br>

#### Style-based generator

![StyleGAN_Image1](https://user-images.githubusercontent.com/48177363/143530735-8c468fc7-6c96-4547-b94e-fbfd5f9497c1.jpg){: width="450"} <br>

전통적인 GAN은 latent space $$\mathcal{Z}$$(input image)를 그래도 GAN network의 첫번째 layer의 input으로 넣는 방식이 많았다. 하지만 StyleGAN에서는 $$\mathcal{Z}$$를 이미지 스타일 생성을 위한 latent space $$\mathcal{W}$$로 mapping 시킨 후 AdaIN을 사용하여 image에 style을 입힌다. 
<details>
  <summary>
    AdaIN
  </summary>  
  <div markdown="1">
  AdaIN은 Content의 평균, 분산을 통해 스타일을 일치시키는 방식이다. 보통 style transfar는 이미지 등의 probability distribution를 일치시킴으로서 일어난다.<br>
  <ul>
    <li>Batch Normalization </li>
    Batch Normalization은 feature map을 channel 별로 mean과 std를 정규화시키는 방식이다. Network에 input이 minibatch 단위로 들어갈 때, batch의 각 channel을 같이 normalization 시킨다.<br>
    \begin{align}
      &BN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta  ( \gamma, \beta 는 parameter 다) \\
      &\mu_{c}(x) = \frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{c}(x) = \sqrt{\frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{c}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Instance Normalization</li>
    Batch Normalization이 같은 위치의 channel이면 batch 단위로 normalization을 진행했던 것과 다르게 IN은 각 sample 마다 channel 별로 normalization을 진행한다. <br>
    \begin{align}
      &IN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta (\gamma, \beta는 parameter 다) \\
      &\mu_{nc}(x) = \frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{nc}(x) = \sqrt{\frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{nc}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Conditional Instance Normalization </li>
    Style s마다 IN를 따로 학습하여 같은 Convolution weight를 가지고 있더라도 다른 Style을 generate할 수 있다.
    \begin{align}
      &CIN(x) = \gamma^{s} (\frac{x - \mu (x)}{\sigma (x)}) + \beta^{s} (\gamma, \beta는 parameter 다) \\
    \end{align}
    <li>Interpreting IN</li>
    BN과 IN의 비교 그래프는 아래와 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143556856-595b1646-2cee-4b61-8e53-9fd75bbbc251.jpg" width="600"> <br>
    Training set을 Origin(a)과 luminance(b)로 normalize해서 training set으로 활용했을 때, BN보다 IN의 Style Loss convergence가 빠른 것을 확인할 수 있다. 하지만 Training set을 Target과는 다른 하나의 Style로 normalize하여 training을 하였을 시, BN과 IN의 Style Loss의 convergence gap이 줄어든 것을 확인할 수 있다. 따라서 IN이 training 시, Style을 normalize하는 효과를 가지고 있어, Style transfar에 더 효과적인 normalize 방식이라는 것을 확인할 수 있다. <br>
    <li>Adaptive Instance Normalization</li>
    따라서 Style transfar에는 BN보다 IN이 좀 더 효과적이라고 할 수 있다. Style Mixing을 위한 StyleGAN에서는 AdaIN을 사용한다. AdaIN은 content input x와 style input y를 받고 training parameter가 없다. 기존 IN의 parameter를 input으로 대체한다. 아래의 식과 같이 기존 content input의 probability distribution을 style input의 것으로 바꿔 스타일 변화를 일으킨다.
      - $$AdaIN(x) = \sigma (y) (\frac{x - \mu (x)}{\sigma (x)}) + \mu (y)$$ <br>
    </div>
    </details> 
  <br>

논문에서 latent z와 
