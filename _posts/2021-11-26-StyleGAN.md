---
title: StyleGAN Paper Review
tags:
    - Deep Learning
    - Paper
    - GAN
---

A brief paper review of StyleGAN <br>

<!--more-->

---

### StyleGAN

[StyleGAN](https://github.com/NVlabs/stylegan)은 이미지의 스타일을 바꿔주는 생성 모델로 2018년 NVidia에서 발표했다. <br>

##### Introduction

StyleGAN은 기존 GAN(Generative Adversarial Network)를 기반으로 한 이미지 생성 모델들이 이미지 생성, 합성 과정은 block box이고, 합성 과정에서 atrribute를 조절하기 힘들고 artifact들이 나타나는 현상을 해결하기 위해 제시되었다. <br> StyleGAN은 input space(input image)를 그대로 GAN 모델에 넣는 것이 아닌 latent space로 mapping 하여 model에 input으로 넣었다. Input image의 probability density를 따르는 latent space는 input space의 entanglement를 어느정도 해결하여 image attribute 조절에 도움이 된다.<br>

##### Style-based generator

![StyleGAN_Image1](https://user-images.githubusercontent.com/48177363/143530735-8c468fc7-6c96-4547-b94e-fbfd5f9497c1.jpg){: width="450"} <br>

전통적인 GAN은 latent space $$\mathcal{Z}$$(input image)를 그래도 GAN network의 첫번째 layer의 input으로 넣는 방식이 많았다. 하지만 StyleGAN에서는 $$\mathcal{Z}$$를 이미지 스타일 생성을 위한 latent space $$\mathcal{W}$$로 mapping 시킨 후 AdaIN을 사용하여 image에 style을 입히며 기본 구조는 PGGAN의 progressive 방식을 사용한다.

<details>
  <summary>
    AdaIN
  </summary>  
  <div markdown="1">
  AdaIN은 Content의 평균, 분산을 통해 스타일을 일치시키는 방식이다. 보통 style transfar는 이미지 등의 probability distribution를 일치시킴으로서 일어난다.<br>
  <ul>
    <li>Batch Normalization </li>
    Batch Normalization은 feature map을 channel 별로 mean과 std를 정규화시키는 방식이다. Network에 input이 minibatch 단위로 들어갈 때, batch의 각 channel을 같이 normalization 시킨다.<br>
    \begin{align}
      &BN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta  ( \gamma, \beta 는 parameter 다) \\
      &\mu_{c}(x) = \frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{c}(x) = \sqrt{\frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{c}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Instance Normalization</li>
    Batch Normalization이 같은 위치의 channel이면 batch 단위로 normalization을 진행했던 것과 다르게 IN은 각 sample 마다 channel 별로 normalization을 진행한다. <br>
    \begin{align}
      &IN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta (\gamma, \beta는 parameter 다) \\
      &\mu_{nc}(x) = \frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{nc}(x) = \sqrt{\frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{nc}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Conditional Instance Normalization </li>
    Style s마다 IN를 따로 학습하여 같은 Convolution weight를 가지고 있더라도 다른 Style을 generate할 수 있다.
    \begin{align}
      &CIN(x) = \gamma^{s} (\frac{x - \mu (x)}{\sigma (x)}) + \beta^{s} (\gamma, \beta는 parameter 다) \\
    \end{align}
    <li>Interpreting IN</li>
    BN과 IN의 비교 그래프는 아래와 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143556856-595b1646-2cee-4b61-8e53-9fd75bbbc251.jpg" width="600"> <br>
    Training set을 Origin(a)과 luminance(b)로 normalize해서 training set으로 활용했을 때, BN보다 IN의 Style Loss convergence가 빠른 것을 확인할 수 있다. 하지만 Training set을 Target과는 다른 하나의 Style로 normalize하여 training을 하였을 시, BN과 IN의 Style Loss의 convergence gap이 줄어든 것을 확인할 수 있다. 따라서 IN이 training 시, Style을 normalize하는 효과를 가지고 있어, Style transfar에 더 효과적인 normalize 방식이라는 것을 확인할 수 있다. <br><br>
    <li>Adaptive Instance Normalization</li>
    따라서 Style transfar에는 BN보다 IN이 좀 더 효과적이라고 할 수 있다. Style Mixing을 위한 StyleGAN에서는 AdaIN을 사용한다. AdaIN은 content input x와 style input y를 받고 training parameter가 없다. 기존 IN의 parameter를 input으로 대체한다. 아래의 식과 같이 기존 content input의 probability distribution을 style input의 것으로 바꿔 스타일 변화를 일으킨다.
    \begin{align}
      AdaIN(x) = \sigma (y) (\frac{x - \mu (x)}{\sigma (x)}) + \mu (y) \\
    \end{align}
    </ul>
  </div>
</details>

기존의 방법처럼 latent space z를 바로 network에 넣게 되면 임의의 latent z의 distribution은 고정되어 있기 때문에 training dataset의 distribution과 맞지 않아, image attribution과 latent z가 non-linear하게 mapping 되어있기 때문에 attribute 조절에 어려움이 있게 된다. 따라서 latent z를 latent w에 mapping시켜, 유동적인 분포를 갖는 latent w를 network의 input으로 넣게 되어 latent w와 attribute가 linear하게 될 수 있도록 한다. 이러한 특징을 disentanglement라 하고 disentangle한 정도를 측정하기 위해 논문에서 PPL(perceptual path length)와 linear saperability를 사용한다. <br><br>
latent z,w는 모두 1x512를 사용하며, mapping network $$\mathcal{f}$$는 8 layer MLP다. <br>
latent w는 ($$y_{s}, y_{b}$$)로 나뉘어 각 layer에 AdaIN의 입력으로 들어간다.

$$
\begin{align}
AdaIN(x*{i}, y) = y*{s,i}(\frac{x*{i} - \mu (x*{i})}{\sigma (x*{i})}) + y*{b,i} \\
\end{align}
$$

feature map x는 y(style)로 normalize 된다. <br><br>
(b)는 각 layer에 넣어주는 noise input이다. noise에 대한 설명은 뒤에 stocastic variation에서 자세하게 다룬다.<br>
generator에서 layer의 첫번째에 latent를 넣는 것은 성능에 큰 의미가 없다. 따라서 generator의 input으로 고정된 4x4x512 크기의 tensor를 넣는다.

##### Properties of the style-based generator

-   Style Mixing<br>
    Style Mixing은 style, attribute를 더 localize 하기 위해 사용된다. 훈련을 할 때, 설정된 퍼센티지의 이미지는 latent $$z_{1}, z_{2}$$를 latent $$w_{1}, w_{2}$$로 mapping 한 후, network의 앞부분은 $$w_{1}$$을, 뒷 부분은 $$w_{2}$$를 사용하여 image generate를 한다. 이 regularization은 network가 인접한 style이 서로 상관관계가 없다고 가정할 수 있도록 한다. (latent w에서 style하나를 변경했을 때 다른 부분까지 영향받지 않도록 한다는 것을 의미하는 것 같다. 두 latent가 들어가다보니 이미지에서 각 역할을 분리해주는 느낌...?) <br>
    ![StyleGAN_Image3](https://user-images.githubusercontent.com/48177363/143576232-e874dcf7-36a5-4d34-b105-9366a4655088.jpg 'Style Mixing'){: width="600"} <br>
    Style Mixing 그림을 보면 얕은 layer 부분의 AdaIN에 들어간 latent는 얼굴 윤곽, 피부톤, 머리색 등 이미지의 Coarse한 부분에 영향을 끼치고, 깊은 layer는 눈, 코, 입 모양 등 dense한 부분에 영ㅇ향을 끼치는 것을 확인할 수 있다. <br><br>
-   Stochastic variation <br>
    사람에게는 피부, 머리카락 위치 등 이미지에 확률적으로 나타난다고 간주할 수 있는 특징이 있다. 기존의 GAN은 activation에 pseudorandom numbers를 추가하여 구현하였기 때문에 반복적인 패턴이 나타나는 경향이 있었다. StyleGAN은 Generator에서 이런 stochastic variation을 구현하기 위해 각 layer에 noise를 넣는다. <br>
    ![StyleGAN_Image4](https://user-images.githubusercontent.com/48177363/143583877-31466559-d3bc-43ec-a02d-fa43e76d3b3a.jpg 'Examples of stochastic variation'){: width="300"} <br>
    위 그림에서 흰 부분은 noise에 의한 변화가 일어난 부분을 의미한다. 전체적인 실루엣 등은 유지되지만 상세한 부분이 변화된 것을 확인할 수 있다. 노이즈가 없이 generation을 수행한다면 generator는 전체적인 평균적 특징을 잡기 때문에 detail에 약간씩 번지는 느낌이 날 수 있다. <br>
    ![StyleGAN_Image5](https://user-images.githubusercontent.com/48177363/143584161-b2be9006-4bc0-4b32-b38c-9b0f3dd7352d.jpg 'Effect of noise inputs at different layers (a) Noise is applied to all layers. (b) No noise. (c) Noise in coarse layer (d) Noise in coarse layers'){: width="300"} <br>
    위와 같이 노이즈가 없으면 머리카락같이 세밀한 부분이 번지는 모습을 볼 수 있고, noise도 마찬가지로 layer의 깊이에 따라 이미지에 미치는 영향이 다름을 확인할 수 있다.

##### Disentanglement studies

Disentanglement은 linear한 subspace로 구성된 latent space로, 각 subspace는 variation을 control하는 것을 목표로 한다. 하지만 latent z는 sampling probability가 training dataset에 해당 subspace의 밀도와 일치하기 때문에 training dataset에 독립적일 수 없고, linear하게 되는 것을 방해한다. 하지만 $$\mathcal{f}$$로 mapping된 latent w는 각 subspace가 linear하게 되도록 trainable하기 때문에 attribute의 variation이 더 linear해진다. 이 disentanglement를 정량화하기 위해 PPL과 linear saperability가 쓰인다. <br>

-   PPL (Perceptual Path Length) <br>
    latent vector에서의 interpolation이 이미지 생성에서 non-linear하다는 것을 entangle하다고 한다. 따라서 PPL은 두 latent vector를 interpolation하는 경로를 일정하게 나누어 만들어진 이미지들의 image length를 더해서 계산한다.<br>

    $$
    \begin{align}
    \mathcal{l}_{z} &= \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(slerp(z_{1},z*{2};t)), G(slerp(z*{1},z*{2};t + \epsilon)))] \\
    \mathcal{l}*{w} &= \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(lerp(f(z_{1}),f(z_{2});t)), G(lerp(f(z_{1}),f(z_{2});t + \epsilon)))] \\
    \end{align}
    $$

    ![StyleGAN_Image6](https://user-images.githubusercontent.com/48177363/143680687-87124db5-199f-41b1-9499-a7e72cd00078.jpg 'Perceptual Path Length'){: width="750"}
    <br>

-   Linear Saperability <br>
    Latent space가 충분히 disentangle하다면 variation의 각 attribute를 찾을 수 있다. 따라서 SVM(Soft Vector Machine)이 각 attribute를 잘 구별하는지로 linear spaerability를 판단한다. Discriminator로 attribute에 대해 판별하고 labeling을 진행한다. 이에 대해 SVM이 latent vector에서 attribute를 잘 구별할 수 있는지 확인한다. <br>
    이에 대해 논문에서는 latent space $$\mathcal{Z}$$보다 $$\mathcal{W}$$에서 attribute를 더 잘 구별하는 것을 확인했다. $$\mathcal{Z}\to\mathcal{W}$$의 layer가 깊어질수록 $$\mathcal{Z}$$의 attribute 분리도는 떨어지지만 $$\mathcal{W}$$에 대한 분리도와 성능이 모두 좋아진다.<br>

-   FID(Frechet Inception Distance)<br>
    FID는 GAN으로 생성된 이미지의 품질 지표로 사용되는 방법이다.

---

### StyleGAN2 <br>

[StyleGANv2](https://github.com/NVlabs/stylegan2)는 NVidia에서 2020년 발표한 StyleGAN의 단점을 보완한 v2이다. <br>

##### Introduction <br>

기존 StyleGAN은 이미지 생성시, artifact가 생기는 문제가 있었다. StyleGANv2는 이러한 문제가 AdaIN과 progressive 방식에 있다고 생각하여 그에 대한 model architecture의 재구성과 latent space에 대한 연구를 진행했다. <br>

##### Removing normalization artifacts <br>

기존 StyleGAN에는 물방울 얼룩의 모양이 생성된 이미지에 생기는 것을 확인할 수 있다. 해당 현상은 64x64 resolution에서부터 관측이 되는데 resolution이 높아질수록 심해지는 경향이 있다. 논문에서는 해당 현상이 instance normalization에 의해 생긴 것이라 추정했다. Normalization은 각 feature map마다 평균, 분산으로 normalize하는데 이것이 feature들 사이의 상대적 크기 차이에서 발생하는 정보를 파괴하기 때문이라고 추정한다. 실제로 normalization을 없애면 물방울 모양의 얼룩이 사라졌다. 띠라서 StyleGAN의 세부 architecture를 개선한다. <br>

<ul>
<li>Generator architecture revisited</li>
    개선된 model architecture의 모양은 다음 그림과 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143733760-16d15c0b-6bf3-46ae-b860-a6ff92f9ba85.jpg" title='Model Architecture' width="750"><br>
    우선적으로 기존 StyleGAN은 noise와 bias를 style block 안에 배치하여, 해당 현상이 AdaIN에 의해 적용되는 normalization에 반대로 작용하게 하였다. (noise가 추가된 후 style 변경이 일어나기 때문에 noise도 style에 대해서 같이 normalize 됨) <br>
    하지만 이는 오히려 style 결과 예측에 어려움을 겪게 했고, 이를 style block 밖으로 뺌으로써 결과 예측이 좀 더 용이하도록 했다. <br>
    또한 mean과 std를 모두 이용해, normalize를 수행했지만 std만 normalize해도 style 변형에 큰 영향이 없다는 것을 알아냈고, (c)에 해당하는 architecture부터는 std에 대한 normalize만 진행했다. <br>

<li>Instance normalization revisited</li>
    StyleGAN은 style mixing이 가능하다는 점이 가장 큰 장점이다. 서로 다른 latent w를 다른 layer의 A에 적용하는 방식으로 동작한다. 하지만 style modulation은 그 자체가 feature map의 크기를 비약적으로 상승시킬 수 있다. 따라서 이러한 feature map magnitude의 발산을 조절해야 한다. 따라서 normalization을 대체할 다른 방식을 사용했다. <br>
    이전까지의 normalization은 feature map에 대해 수행된다. 앞서 (c)에 해당하는 architecture에서 mean에 대한 normalization을 없앴기 때문에 사실상 feature map의 scaling과 동일한 기능을 한다고 생각할 수 있다. 따라서 출력된 feature map에 대한 직접적인 scaling을 진행하는 것이 아니라 weight 자체를 scaling하여 간접적으로 normalization을 수행할 수 있다. <br>
    $$
    \begin{align}
    \mathcal{w}^\prime_{ijk} = \mathcal{s}_{i}\dot\mathcal{w}^\prime_{ijk} (\dot Scaling)\\
    \sigma_{j}=\sqrt{\sum_{i,k}\mathcal{w}^\prime_{ijk}^{2}} (supp. IID)(\dot Demodulate)\\
    mathcal{w}^\prime\prime_{ijk} = \mathcal{w}^\prime_{ijk} / \sqrt{\sum_{i,k}\mathcal{w}^\prime_{ijk}^{2} + \epsilon} (\dot Convolution weight)\\
    \end{align}
    $$
</ul>
$$\mathcal{w}^\prime\prime_{ijk}$$가 최종 Convolution weight가 된다. 하지만 feature map을 직접 쓰는 것이 아닌 신호에 대한 가정이 추가되기 때문에 normalization보단 약하다. <br>

##### Image quality and generator smoothness <br>

기존 이미지 품질 측정을 위한 FID와 P&R은 좋은 품질 평가 지표지만 실제 이미지의 품질과는 다른 경우가 존재한다. StyleGANv2에서는 latent space와 그 perturbation에 대해 generate된 이미지 사이의 PPL이 낮을수록 이미지의 품질이 좋다는 것을 발견했다. 해당 현상의 이유는 명확하게 밝히지 못했지만 논문에서는 discriminator가 깨진 이미지는 fake image라고 판단하기 때문에 latent w에서 깨진 부분이 없도록 학습이 되는 것이라 가정한다. 따라서 latent space $$\mathcal{W}$$에서 perturbation끼리의 PPL이 낮으면 이미지 품질이 좋은 것으로 판단된다. <br>
하지만 PPL이 너무 낮으면 단기적인 이미지 품질은 좋아도 왜곡이 누적되어 전체적인 이미지 품질은 낮아질 수 있다. ($$\mathcal{W}$$에 대해 원하는 attribute에 대한 확장에만 집중하다보니 나머지에 대한 품질의 왜곡이 누적된다는 말인듯.) 따라서 Recall이 망가지기 때문에 논문에서 왜곡이 없이 smoothness한 regularization을 제안한다. <br>

<ul>
<li>Lazy regularization</li>
sdf
<li> Path length regularization</li>
</ul>
