---
title: StyleGAN Paper Review
tags:
    - Deep Learning
    - Paper
    - GAN
---

A brief paper review of StyleGAN <br>

<!--more-->

---

### StyleGAN

[StyleGAN](https://github.com/NVlabs/stylegan)은 이미지의 스타일을 바꿔주는 생성 모델로 2018년 NVidia에서 발표했다. <br>

##### Introduction

StyleGAN은 기존 GAN(Generative Adversarial Network)를 기반으로 한 이미지 생성 모델들이 이미지 생성, 합성 과정은 block box이고, 합성 과정에서 atrribute를 조절하기 힘들고 artifact들이 나타나는 현상을 해결하기 위해 제시되었다. <br> StyleGAN은 input space(input image)를 그대로 GAN 모델에 넣는 것이 아닌 latent space로 mapping 하여 model에 input으로 넣었다. Input image의 probability density를 따르는 latent space는 input space의 entanglement를 어느정도 해결하여 image attribute 조절에 도움이 된다.<br>

##### Style-based generator

![StyleGAN_Image1](https://user-images.githubusercontent.com/48177363/143530735-8c468fc7-6c96-4547-b94e-fbfd5f9497c1.jpg){: width="450"} <br>

전통적인 GAN은 latent space $$\mathcal{Z}$$(input image)를 그래도 GAN network의 첫번째 layer의 input으로 넣는 방식이 많았다. 하지만 StyleGAN에서는 $$\mathcal{Z}$$를 이미지 스타일 생성을 위한 latent space $$\mathcal{W}$$로 mapping 시킨 후 AdaIN을 사용하여 image에 style을 입히며 기본 구조는 PGGAN의 progressive 방식을 사용한다.

<details>
  <summary>
    AdaIN
  </summary>  
  <div markdown="1">
  AdaIN은 Content의 평균, 분산을 통해 스타일을 일치시키는 방식이다. 보통 style transfar는 이미지 등의 probability distribution를 일치시킴으로서 일어난다.<br>
  <ul>
    <li>Batch Normalization </li>
    Batch Normalization은 feature map을 channel 별로 mean과 std를 정규화시키는 방식이다. Network에 input이 minibatch 단위로 들어갈 때, batch의 각 channel을 같이 normalization 시킨다.<br>
    \begin{align}
      &BN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta  ( \gamma, \beta 는 parameter 다) \\
      &\mu_{c}(x) = \frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{c}(x) = \sqrt{\frac{1}{NHW}\sum^{N}_{1}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{c}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Instance Normalization</li>
    Batch Normalization이 같은 위치의 channel이면 batch 단위로 normalization을 진행했던 것과 다르게 IN은 각 sample 마다 channel 별로 normalization을 진행한다. <br>
    \begin{align}
      &IN(x) = \gamma (\frac{x - \mu (x)}{\sigma (x)}) + \beta (\gamma, \beta는 parameter 다) \\
      &\mu_{nc}(x) = \frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}x_{nchw} \\
      &\sigma_{nc}(x) = \sqrt{\frac{1}{HW}\sum^{H}_{1}\sum^{W}_{1}(x_{nchw}-\mu_{nc}(x))^{2}+\epsilon} \\
    \end{align}
    <li>Conditional Instance Normalization </li>
    Style s마다 IN를 따로 학습하여 같은 Convolution weight를 가지고 있더라도 다른 Style을 generate할 수 있다.
    \begin{align}
      &CIN(x) = \gamma^{s} (\frac{x - \mu (x)}{\sigma (x)}) + \beta^{s} (\gamma, \beta는 parameter 다) \\
    \end{align}
    <li>Interpreting IN</li>
    BN과 IN의 비교 그래프는 아래와 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143556856-595b1646-2cee-4b61-8e53-9fd75bbbc251.jpg" width="600"> <br>
    Training set을 Origin(a)과 luminance(b)로 normalize해서 training set으로 활용했을 때, BN보다 IN의 Style Loss convergence가 빠른 것을 확인할 수 있다. 하지만 Training set을 Target과는 다른 하나의 Style로 normalize하여 training을 하였을 시, BN과 IN의 Style Loss의 convergence gap이 줄어든 것을 확인할 수 있다. 따라서 IN이 training 시, Style을 normalize하는 효과를 가지고 있어, Style transfar에 더 효과적인 normalize 방식이라는 것을 확인할 수 있다. <br><br>
    <li>Adaptive Instance Normalization</li>
    따라서 Style transfar에는 BN보다 IN이 좀 더 효과적이라고 할 수 있다. Style Mixing을 위한 StyleGAN에서는 AdaIN을 사용한다. AdaIN은 content input x와 style input y를 받고 training parameter가 없다. 기존 IN의 parameter를 input으로 대체한다. 아래의 식과 같이 기존 content input의 probability distribution을 style input의 것으로 바꿔 스타일 변화를 일으킨다.
    \begin{align}
      AdaIN(x) = \sigma (y) (\frac{x - \mu (x)}{\sigma (x)}) + \mu (y) \\
    \end{align}
    </ul>
  </div>
</details>

기존의 방법처럼 latent space z를 바로 network에 넣게 되면 임의의 latent z의 distribution은 고정되어 있기 때문에 training dataset의 distribution과 맞지 않아, image attribution과 latent z가 non-linear하게 mapping 되어있기 때문에 attribute 조절에 어려움이 있게 된다. 따라서 latent z를 latent w에 mapping시켜, 유동적인 분포를 갖는 latent w를 network의 input으로 넣게 되어 latent w와 attribute가 linear하게 될 수 있도록 한다. 이러한 특징을 disentanglement라 하고 disentangle한 정도를 측정하기 위해 논문에서 PPL(perceptual path length)와 linear saperability를 사용한다. <br><br>
latent z,w는 모두 1x512를 사용하며, mapping network $$\mathcal{f}$$는 8 layer MLP다. <br>
latent w는 ($$y_{s}, y_{b}$$)로 나뉘어 각 layer에 AdaIN의 입력으로 들어간다.

$$
\begin{align}
AdaIN(x_{i}, y) = y_{s,i}(\frac{x_{i} - \mu (x_{i})}{\sigma (x_{i})}) + y_{b,i} \\
\end{align}
$$

feature map x는 y(style)로 normalize 된다. <br><br>
(b)는 각 layer에 넣어주는 noise input이다. noise에 대한 설명은 뒤에 stocastic variation에서 자세하게 다룬다.<br>
generator에서 layer의 첫번째에 latent를 넣는 것은 성능에 큰 의미가 없다. 따라서 generator의 input으로 고정된 4x4x512 크기의 tensor를 넣는다.

##### Properties of the style-based generator

-   Style Mixing<br>
    Style Mixing은 style, attribute를 더 localize 하기 위해 사용된다. 훈련을 할 때, 설정된 퍼센티지의 이미지는 latent $$z_{1}, z_{2}$$를 latent $$w_{1}, w_{2}$$로 mapping 한 후, network의 앞부분은 $$w_{1}$$을, 뒷 부분은 $$w_{2}$$를 사용하여 image generate를 한다. 이 regularization은 network가 인접한 style이 서로 상관관계가 없다고 가정할 수 있도록 한다. (latent w에서 style하나를 변경했을 때 다른 부분까지 영향받지 않도록 한다는 것을 의미하는 것 같다. 두 latent가 들어가다보니 이미지에서 각 역할을 분리해주는 느낌...?) <br>
    ![StyleGAN_Image3](https://user-images.githubusercontent.com/48177363/143576232-e874dcf7-36a5-4d34-b105-9366a4655088.jpg 'Style Mixing'){: width="600"} <br>
    Style Mixing 그림을 보면 얕은 layer 부분의 AdaIN에 들어간 latent는 얼굴 윤곽, 피부톤, 머리색 등 이미지의 Coarse한 부분에 영향을 끼치고, 깊은 layer는 눈, 코, 입 모양 등 dense한 부분에 영ㅇ향을 끼치는 것을 확인할 수 있다. <br><br>
-   Stochastic variation <br>
    사람에게는 피부, 머리카락 위치 등 이미지에 확률적으로 나타난다고 간주할 수 있는 특징이 있다. 기존의 GAN은 activation에 pseudorandom numbers를 추가하여 구현하였기 때문에 반복적인 패턴이 나타나는 경향이 있었다. StyleGAN은 Generator에서 이런 stochastic variation을 구현하기 위해 각 layer에 noise를 넣는다. <br>
    ![StyleGAN_Image4](https://user-images.githubusercontent.com/48177363/143583877-31466559-d3bc-43ec-a02d-fa43e76d3b3a.jpg 'Examples of stochastic variation'){: width="300"} <br>
    위 그림에서 흰 부분은 noise에 의한 변화가 일어난 부분을 의미한다. 전체적인 실루엣 등은 유지되지만 상세한 부분이 변화된 것을 확인할 수 있다. 노이즈가 없이 generation을 수행한다면 generator는 전체적인 평균적 특징을 잡기 때문에 detail에 약간씩 번지는 느낌이 날 수 있다. <br>
    ![StyleGAN_Image5](https://user-images.githubusercontent.com/48177363/143584161-b2be9006-4bc0-4b32-b38c-9b0f3dd7352d.jpg 'Effect of noise inputs at different layers (a) Noise is applied to all layers. (b) No noise. (c) Noise in coarse layer (d) Noise in coarse layers'){: width="300"} <br>
    위와 같이 노이즈가 없으면 머리카락같이 세밀한 부분이 번지는 모습을 볼 수 있고, noise도 마찬가지로 layer의 깊이에 따라 이미지에 미치는 영향이 다름을 확인할 수 있다.

##### Disentanglement studies

Disentanglement은 linear한 subspace로 구성된 latent space로, 각 subspace는 variation을 control하는 것을 목표로 한다. 하지만 latent z는 sampling probability가 training dataset에 해당 subspace의 밀도와 일치하기 때문에 training dataset에 독립적일 수 없고, linear하게 되는 것을 방해한다. 하지만 $$\mathcal{f}$$로 mapping된 latent w는 각 subspace가 linear하게 되도록 trainable하기 때문에 attribute의 variation이 더 linear해진다. 이 disentanglement를 정량화하기 위해 PPL과 linear saperability가 쓰인다. <br>

-   PPL (Perceptual Path Length)<br>
    latent vector에서의 interpolation이 이미지 생성에서 non-linear하다는 것을 entangle하다고 한다. 따라서 PPL은 두 latent vector를 interpolation하는 경로를 일정하게 나누어 만들어진 이미지들의 image length를 더해서 계산한다.<br>

    $$
    \begin{align}
    \mathcal{l}_{z} &= \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(slerp(z_{1},z_{2};t)), G(slerp(z_{1},z_{2};t + \epsilon)))] \\
    \mathcal{l}_{w} &= \mathbb{E}[\frac{1}{\epsilon^{2}}d(G(lerp(f(z_{1}),f(z_{2});t)), G(lerp(f(z_{1}),f(z_{2});t + \epsilon)))] \\
    \end{align}
    $$

    ![StyleGAN_Image6](https://user-images.githubusercontent.com/48177363/143680687-87124db5-199f-41b1-9499-a7e72cd00078.jpg 'Perceptual Path Length'){: width="600"}
    <br>

-   Linear Saperability<br>
    Latent space가 충분히 disentangle하다면 variation의 각 attribute를 찾을 수 있다. 따라서 SVM(Soft Vector Machine)이 각 attribute를 잘 구별하는지로 linear spaerability를 판단한다. Discriminator로 attribute에 대해 판별하고 labeling을 진행한다. 이에 대해 SVM이 latent vector에서 attribute를 잘 구별할 수 있는지 확인한다. <br>
    이에 대해 논문에서는 latent space $$\mathcal{Z}$$보다 $$\mathcal{W}$$에서 attribute를 더 잘 구별하는 것을 확인했다. $$\mathcal{Z}\to\mathcal{W}$$의 layer가 깊어질수록 $$\mathcal{Z}$$의 attribute 분리도는 떨어지지만 $$\mathcal{W}$$에 대한 분리도와 성능이 모두 좋아진다.<br>
-   FID(Frechet Inception Distance)<br>
    FID는 GAN으로 생성된 이미지의 품질 지표로 사용되는 방법이다.

---

### StyleGAN2 <br>

[StyleGANv2](https://github.com/NVlabs/stylegan2)는 NVidia에서 2020년 발표한 StyleGAN의 단점을 보완한 v2이다. <br>

##### Introduction <br>

기존 StyleGAN은 이미지 생성시, artifact가 생기는 문제가 있었다. StyleGANv2는 이러한 문제가 AdaIN과 progressive 방식에 있다고 생각하여 그에 대한 model architecture의 재구성과 latent space에 대한 연구를 진행했다. <br>

##### Removing normalization artifacts <br>

기존 StyleGAN에는 물방울 얼룩의 모양이 생성된 이미지에 생기는 것을 확인할 수 있다. 해당 현상은 64x64 resolution에서부터 관측이 되는데 resolution이 높아질수록 심해지는 경향이 있다. 논문에서는 해당 현상이 instance normalization에 의해 생긴 것이라 추정했다. Normalization은 각 feature map마다 평균, 분산으로 normalize하는데 이것이 feature들 사이의 상대적 크기 차이에서 발생하는 정보를 파괴하기 때문이라고 추정한다. 실제로 normalization을 없애면 물방울 모양의 얼룩이 사라졌다. 띠라서 StyleGAN의 세부 architecture를 개선한다. <br>

-   Generator architecture revisited<br>
    개선된 model architecture의 모양은 다음 그림과 같다. <br>
    <img src="https://user-images.githubusercontent.com/48177363/143733760-16d15c0b-6bf3-46ae-b860-a6ff92f9ba85.jpg" title='Model Architecture' width="750"><br>
    우선적으로 기존 StyleGAN은 noise와 bias를 style block 안에 배치하여, 해당 현상이 AdaIN에 의해 적용되는 normalization에 반대로 작용하게 하였다. (noise가 추가된 후 style 변경이 일어나기 때문에 noise도 style에 대해서 같이 normalize 됨) <br>
    하지만 이는 오히려 style 결과 예측에 어려움을 겪게 했고, 이를 style block 밖으로 뺌으로써 결과 예측이 좀 더 용이하도록 했다. <br>
    또한 mean과 std를 모두 이용해, normalize를 수행했지만 std만 normalize해도 style 변형에 큰 영향이 없다는 것을 알아냈고, (c)에 해당하는 architecture부터는 std에 대한 normalize만 진행했다. <br>

-   Instance normalization revisited<br>
    StyleGAN은 style mixing이 가능하다는 점이 가장 큰 장점이다. 서로 다른 latent w를 다른 layer의 A에 적용하는 방식으로 동작한다. 하지만 style modulation은 그 자체가 feature map의 크기를 비약적으로 상승시킬 수 있다. 따라서 이러한 feature map magnitude의 발산을 조절해야 한다. 따라서 normalization을 대체할 다른 방식을 사용했다. <br>
    이전까지의 normalization은 feature map에 대해 수행된다. 앞서 (c)에 해당하는 architecture에서 mean에 대한 normalization을 없앴기 때문에 사실상 feature map의 scaling과 동일한 기능을 한다고 생각할 수 있다. 따라서 출력된 feature map에 대한 직접적인 scaling을 진행하는 것이 아니라 weight 자체를 scaling하여 간접적으로 normalization을 수행할 수 있다.

    $$
    \begin{align}
    \mathcal{w}^{\prime}_{i,j,k} &= \mathcal{s}_{i} \cdot \mathcal{w}^{\prime}_{i,j,k} \\
    \sigma_{j} &= \sqrt{\sum_{i,k} {\mathcal{w}^{\prime}_{i,j,k}}^{2}} \\
    \mathcal{w}^{\prime\prime}_{i,j,k} &= \frac{\mathcal{w}^{\prime}_{i,j,k}}{\sqrt{\sum_{i,k} {\mathcal{w}^{\prime}_{i,j,k}}^{2} + \epsilon}} \\
    \end{align}
    $$

    로 modulation과 demodulation이 수행된다. 따라서 $$\mathcal{w}^{\prime\prime}_{i,j,k}$$가 최종 Convolution weight가 된다. 하지만 feature map을 직접 쓰는 것이 아닌 신호에 대한 가정(I.I.D)이 추가되기 때문에 normalization보단 약하다.<br>

##### Image quality and generator smoothness <br>

기존 이미지 품질 측정을 위한 FID와 P&R은 좋은 품질 평가 지표지만 실제 이미지의 품질과는 다른 경우가 존재한다. StyleGANv2에서는 latent space와 그 perturbation에 대해 generate된 이미지 사이의 PPL이 낮을수록 이미지의 품질이 좋다는 것을 발견했다. 해당 현상의 이유는 명확하게 밝히지 못했지만 논문에서는 discriminator가 깨진 이미지는 fake image라고 판단하기 때문에 latent w에서 깨진 부분이 없도록 학습이 되는 것이라 가정한다. 따라서 latent space $$\mathcal{W}$$에서 perturbation끼리의 PPL이 낮으면 이미지 품질이 좋은 것으로 판단된다. <br>
하지만 PPL이 너무 낮으면 단기적인 이미지 품질은 좋아도 왜곡이 누적되어 전체적인 이미지 품질은 낮아질 수 있다. ( $$\mathcal{W}$$에 대해 원하는 attribute에 대한 확장에만 집중하다보니 나머지에 대한 품질의 왜곡이 누적된다는 말인듯.) 따라서 Recall이 망가지기 때문에 논문에서 왜곡이 없이 smoothness한 regularization을 제안한다. <br>

-   Lazy regularization<br>
    Main loss function(logistic loss)과 regularization( $$\mathcal{R}_{1}$$)은 원래 같이 optimize되자만 regularizer는 16 batch마다 optimize해도 문제가 없다. 따라서 해당하는 computational cost를 줄일 수 있다. <br>

-   Path length regularization<br>
    latent space $$\mathcal{W}$$에서 일정 크기의 step을 이동했을 떄 이미지에서 일정 PPL만큼 이동하길 원한다. (원활한 attribure control을 위해) 따라서 이미지에서 일정 PPL만큼 이동했을 때 그 방향에 관계없이 $$\mathcal{W}$$ space의 기울기가 일정한 것이 이상적인 $$\mathcal{Z}$$에서의 mapping으로 생각할 수 있다. <br>
    Image space $$\mathcal{Y}$$에서 $$g(\mathcal{W}): \mathcal{W}\to\mathcal{Y}$$일 때,

    $$
    \begin{align}
    \mathcal{J}_{\mathcal{w}}=\frac{\partial g(\mathcal{w})}{\partial\mathcal{w}} \\
    \end{align}
    $$

    위를 달성하기 위해 regularizer로

    $$
    \begin{align}
    \mathbb{E}_{w,y \sim \mathcal{N}(0,1)}(\lVert \mathcal{J}_{wy}^{T} \rVert_{2} - a)^{2} \\
    \end{align}
    $$

    Image generator같은 높은 차원에서는 $$\mathcal{J}_{w}$$가 $$\mathcal{W}$$에 orthogonal할 때 식이 minimized하다. (접선 느낌인듯) <br>
    위의 식에 사용되는 Jacobian matrix는 계산이 복잡하기 떄문에 backpropagation 시, 용이한 계산을 위해

    $$
    \begin{align}
    \mathcal{J}_{wy}^{T} = \nabla_{w}(g(w)\cdot y)\\
    \end{align}
    $$

##### Progressive growing revisited <br>

Progressive 방식은 고해상도 이미지를 만들기엔 좋은 방식이지만 artifact를 유발하는 경향이 있다. 낮은 해상도의 이미지는 거의 완성이 되는 상태로 다음 generator에 대한 학습이 이루어진다. 낮은 해상도는 위에 언급했던 대로 이미지의 전체적인 모양, 틀 등 큰 context에 관한 것을 조정하게 되는데, 이런 경우 detail한 부분을 조정함에 있어서 그 위치가 고정되어버리는 경우가 존재한다. (Image를 smooth하게 움직일 때 눈, 치아의 위치가 고정되는 경향이 있다.)<br>
![StyleGAN_Image8](https://user-images.githubusercontent.com/48177363/143799441-a3f2bb97-880c-42ee-8ed6-ce8219c72a95.jpg){: width="300"}<br>
이는 각 layer의 resolution이 output resolution으로 설정되어, detail한 부분에 대해 주파수가 최대가 되도록 학습이 되기 때문에 이미지가 network를 지나면서 과도하게 높은 주파수를 가지게 된다. (G layer 하나당 하나의 Discriminator로 연결이 되기 때문에 각 output이 D에서 image로 판별되어 높은 주파수, 즉, 과도하게 detail한 부분을 살리기 위해 학습된다는 것을 의미하는 듯) <br>
따라서 CNN의 shift-invariant가 파괴되어 detail한 부분의 위치가 고정되게 된다. <br>

-   Alternative network<br>
    ![StyleGAN_Image9](https://user-images.githubusercontent.com/48177363/143806427-6121e6f7-c47b-4580-9793-7d221c6b1421.jpg){:width="450"}<br>
    (a)MSG-GAN은 각 resolution에서 RGB scale의 image로 보낸 후 같은 해상도의 discriminator와 연결하는 방식이다. 각 resolution에서 이미지가 어떻게 작용하는지 확인하기에 용이하다. <br>
    (b)Input/output skips는 G와 D를 연결하는 대신 upsampling과 downsampling을 이용하여, RGB scale의 이미지를 각 resolution에 더한 후 Discriminator에 보내는 방식이다. <br>
    (c)Residual nets는 RGB scale로 보내는 것을 각 resolution이 아닌 최종 resolution에서만 적용하였고, residual을 추가한 architecture를 나타낸다. <br>
    위 세 architecture에 대해 논문에서는 G는 (b)를, D는 (c)를 썼을 때 성능이 가장 좋았다고 언급한다. <br>

-   Resolution usage <br>
    높은 해상도의 이미지가 잘 만들어진다는 Progressive 방식의 장점은 유지하면서 artifact는 제거하고 싶기 때문에 low-resolution부터 시작하여, high-resolution으로 attention을 이동시키는 방식이 필요하다. <br>
    따라서 논문에서는 각 resolution의 수치를 정규화했다. 1024개의 latent w 에 대해 각 tRGB layer에서 pixel에 대한 std를 계산하였다. (std가 클수록 sample마다 pixel 값의 변화량이 큰 것을 의미하기 때문에 논문에서는 그것을 generate되는 결과에 미치는 영향이 크다고 본 것 같다. std가 작으면 w가 변해도 값에 큰 차이가 없기 때문에 최종 기여도가 낮다고 여기는 것 같다.) <br>
    ![StyleGAN_Image10](https://user-images.githubusercontent.com/48177363/143807876-63176e72-3c6e-4452-a594-cfa4eec9183e.jpg){: width="450"}<br>
    실제로 Residual nets는 progressive처럼 후반에 갈수록 High-resolution으로 attention을 집중하는 경향을 보였다. 512x512 resolution의 비중이 컸고, 1024x1024로 나온 최종 결과도 512x512를 2배 upsampling 한 것처럼 나왔다. 이를 network의 capability 문제라 생각하고 network의 크기를 늘렸더니 (b)와 같이 attention 결과가 잘 나오는 것을 확인할 수 있었다. <br>

최종적으로 StyleGAN에서 StyleGANv2로 발전된 architecture의 과정은 아래 그림과 같다. <br>
![StyleGAN_Image11](https://user-images.githubusercontent.com/48177363/143808495-7307bf5c-a5eb-4d49-95a9-ff83fa9e2846.jpg){: width="750"} <br>

##### Projection of images to latent space <br>

-   Atrribution of Images <br>
    Generator의 성능이 빠르게 향상되고 있다. 그에 따라 Discriminator에 대한 성능도 보장되는 것이 필요하다. StyleGAN에서는 real image와 fake image의 구별을 이미지를 생성하는 latent w가 있는지 찾는 방식으로 사용할 수 있다. 아래는 real image와 fake image의 LPIPS 거리를 의미한다. 이미지의 품질이 좋아졌음에도 StyleGANv2는 StyleGAN보다 w mapping이 잘 되어 구별이 쉽다. 이는 이미지에 해당하는 latent를 더 찾기 쉽다는 것을 의미하며, image generate와 control이 더 용이하다는 것을 의미한다. <br>
    ![StyleGAN_Image12](https://user-images.githubusercontent.com/48177363/143809413-d59997f6-7ac7-404b-913f-9817032f0f78.jpg){: width="300"}<br>

##### Reference <br>

-   [StyleGAN github](https://github.com/NVlabs/stylegan)<br>
-   [StyleGAN paper](https://arxiv.org/abs/1812.04948)<br>
-   [StyleGANv2 github](https://github.com/NVlabs/stylegan2)<br>
-   [StyleGANv2 paper](https://arxiv.org/abs/1912.04958)<br>
-   [AdaIN paper](https://arxiv.org/abs/1703.06868)<br>
-   [PGGAN](https://arxiv.org/pdf/1710.10196.pdf)<br>
